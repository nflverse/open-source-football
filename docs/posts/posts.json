[
  {
    "path": "posts/2023-02-23-nfl-draft-value-chart/",
    "title": "NFL Draft Value Chart",
    "description": "Constructing surplus value from scratch.",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin/"
      }
    ],
    "date": "2023-04-17",
    "categories": [
      "NFL Draft",
      "nflreadr"
    ],
    "contents": "\n\nContents\nPart 1: Make a surplus value curve\nHow expensive is each draft pick’s rookie deal?\nHow productive has each pick been?\n\nPart 2: Show the draft curves\nOn-field value versus surplus value\nComparison with other draft curves\nDraft Value by Position\n\nPart 3: Some example calculations\nDraft day trade: example 1\nDraft day trade: example 2\nPlayer for picks trade\n\nPart 4: The full table\n\nDo we really need another draft values chart out there? No, not really.\nSo why does this exist? I haven’t seen a draft value chart that satisfies the following two conditions:\nIs made with publicly available code and data\nShows a draft value curve when excluding quarterbacks\nThe latter point is especially important. If one combines quarterbacks and non-quarterbacks when constructing a draft value chart, one ends up much too rosy on the value of the top picks in the draft when these picks are not used on a quarterback. This is due to quarterbacks being very valuable, and quarterbacks selected high in the draft tending to have higher hit rates.\nThe goal here is to construct a transparent (i.e., the code and methodology are public) and useful draft chart. Like the Fitzgerald-Spielberger (OTC) NFL Draft Trade Value Chart, we’re going to use second contracts as a measure of player value. There are some obvious limitations to this:\nWe assume that a player’s value on the first 4 years of his rookie contract is well represented by what he is able to earn on his 2nd deal. For players who improved a lot at the end of their rookie deals (or suffered serious injury), the measure is far from perfect\nWe aren’t capturing the value provided by 5th year option for 1st round picks. This somewhat underestimates 1st round pick value, but with the new fifth-round option values introduced in the 2020 CBA, this option isn’t as valuable as it used to be. For example, CeeDee Lamb would make about $18 million in his option year due to making one Pro Bowl and Justin Jefferson $20 million due to making multiple Pro Bowls. This isn’t typically what we think of when thinking about rookie deal prices\nWe aren’t capturing the value of being able to re-sign a player before he hits the open market\nAs mentioned above, we’re only going to do this for non-QBs. The value of QBs is so different than every other position that they should be considered separately in any draft analysis.\nBefore moving forward, a thank you to the invaluable resource that Over The Cap represents. All of the contract data used in this post is taken from their website and this analysis wouldn’t be possible without their hard work.\nUpdated on February 27, 2023: (a) added more picks to the table at the bottom so it now displays picks 1-256, (b) added on-field value to table at the bottom, and (c) re-scaled surplus value so it’s 0-to-100 (thank you to Luckym4n_ for the suggestion)/\nUpdated on April 11, 2023: Add 2018 draft class to on-field value calculations. Now covers 2011 through 2018. For Calvin Ridley, assume he’ll get an extension equivalent to tag value. In addition, use archieved version of Sportrac rookie contract value to avoid site updates breaking the scrape.\nUpdated on April 17, 2023: Added position-specific draft curves.\nPart 1: Make a surplus value curve\nIn order to calculate the surplus value of each pick. We need two pieces of information. First, how much each pick costs (the contract); and second, how much benefit each pick provides (on-field performance).\nHow expensive is each draft pick’s rookie deal?\nThe first thing we need is the actual cost of each draft pick’s contract. Some googling led me to this page.\n\n\n\nA couple notes on the calculations we’re be using (if you want to see the code, scroll down to the bottom of this post and click the “View source code on GitHub” button, or just click here). First, we’re assuming cap growth rate of 7% over the next 3 seasons (i.e., 2024, 2025, and 2026) before converting the total amount of the rookie deal into a percentage of the cap. For example, the 2023 cap is about $225 million, while the average of the 2023 through 2026 caps is about $250 million using this procedure. Taking the Bears’ No. 1 pick for example, we have total value of about $41 million, which is about $10.2 million per season, or 10.2/250 = 4.1 percent of the cap. If we assumed no growth in the salary cap rates, then we would divide 10.2 by 225 which is about 4.5 percent of the cap. In other words, we’d be counting the high draft picks as too expensive.\nHow productive has each pick been?\nThis is pretty simple: take the APY in terms of percent of the cap for each drafted player’s second deal (i.e., first deal after rookie contract). For players that did not receive a second contract, we’ll assign them a value of zero. We then smooth these values to get a smooth relationship between pick number and on-field performance.\n\n\n\nPart 2: Show the draft curves\nWe first plot our own draft curves and then afterwards compare to others.\nOn-field value versus surplus value\nUsing cost and on-field performance, we can now construct a surplus value curve:\n\n\n\nHow to read the chart: the first pick in the draft costs 4.1% of the salary cap (orange line). The on-field production is expected to be 6.4% of the salary cap (purple line). Thus, the surplus value associated with the first pick in the draft is 2.3% of the salary cap (green line).\nWe have re-produced the now-familiar result that for teams not drafting a quarterback, the surplus value of the very top picks is lower than later in the first round and even throughout most of the second round. This is because the dropoff in salary that teams have to pay to draft picks (orange line) is steeper than the dropoff in on-field value (purple line). This reflects the fact that teams are not very good at identifying the best players in the draft. We have a lot of evidence that this is the case.\nThis pattern of later first round picks having higher surplus value than early first round picks was originally referred to as the loser’s curse in the seminal paper by Cade Massey and Richard Thaler. Massey and Thaler wrote their paper during the pre-2011 CBA when rookie contracts were a lot more expensive; however, the pattern still holds up now. In addition, we have replicated earlier findings by PFF’s Timo Riske using a similar approach that took advantage of player contract data and yet another study by Riske that used PFF Wins Above Replacement (WAR).\nIn particular, our chart above looks remarkably similar to this picture below from Timo’s work, which is always a good sign that we’re on the right track:\nTimo Riske’s draft curveComparison with other draft curves\nThe three draft charts that I tend to see the most often are the OTC chart, the PFR chart made by Chase Stuart based on AV, and the original Jimmy Johnson chart. Here’s how they compare with the surplus and on-field values shown above.\nIn the figure below, we scale each draft curve so that the value of the first overall pick represents 100 draft points.\n\n\n\n\n\n\nBecause our surplus value chart doesn’t have the first pick as the most valuable pick, unsurprisingly, it looks very different than the other charts. Interestingly, the player value curve (i.e., on-field value of player without accounting for cost of rookie contract) is still flatter than the other three curves. This shouldn’t come as a surprise given that we have excluded quarterbacks and the other charts do not.\nUnsurprisingly, draft charts tend to overvalue the top picks in the case when these picks are not used on a quarterback. This adds to the mountain of evidence discussed above that NFL teams largely do not understand the relative values of draft picks and are dramatically overconfident in their ability to pick the right players at the top of the draft. The original Jimmy Johnson chart, for example, comes nowhere close to reflecting the true value of picks.\nDraft Value by Position\nInspired by Kevin Cole’s post here, we can also compute position-specific curves. Let’s show on-field value rather than surplus value because surplus value is computed using costs that are invariant to position. That is, to go from one to the other, we would subtract the pick-specific cost from each curve.\n\n\n\nLooking at the results, we see that RB (offense) and Safety (defense) are the low positions on the totem pole. At the top, the highest returns have been for pass rushers (both edge and inside) and offensive tackles.\nPart 3: Some example calculations\nAt the bottom of this post, I will leave a table that contains the same information presented in two ways. First, an estimated value of each pick on a familiar scale that is primarily useful for calculating trades involving draft picks only. In addition, the table contains the actual estimate of APY per year (over 4 years) in order to facilitate evaluations of pick-for-player trades. In this section, I will provide an example of both. Remember that this is for non-QBs: if a quarterback is involved, don’t use this table.\nFinally, the table also presents draft values using on-field value only (i.e., the purple curve above) for comparison.\nDraft day trade: example 1\nLet’s start with last year’s controversial Lions-Vikings trade, in which the Vikings were panned for the lack of return they got for moving back 20 picks in the first round.\nHere’s the trade with the estimated values obtained from this post (again, the full table is displayed below):\nLions receive: No. 12 (100), No. 46 (73). Total: 173\nVikings receive: No. 32 (82), No. 34 (86), No. 66 (55). Total: 223\nDifference: 50, which is the value of pick No. 72\nSo the surplus value the Vikings obtained in this trade was the equivalent of an early third-round pick. Not bad for a trade that popular commentary thought that they lost!\nWhat if we ignored surplus value and just looked at on-field value (i.e., player production ignoring cost)?\nLions receive: No. 12 (87), No. 46 (54). Total: 141\nVikings receive: No. 32 (66), No. 34 (64), No. 66 (41). Total: 171\nDifference: 30, which is the value of pick No. 91, or a pick towards the end of the 3rd round.\nSo even if you ignore the cost of players entirely (which is not a good idea in a league with a salary cap), there is no argument against the Vikings winning this trade in terms of the expected value of the picks.\nDraft day trade: example 2\nOne of the worst draft-day trades I can remember that didn’t involve players or future picks was the Jets trading up for Alijah Vera-Tucker in 2021. Let’s see how bad it looks using the chart below:\nJets receive: No. 14 (100), No. 143 (20). Total: 120\nVikings receive: No. 23 (91), No. 66 (55), No. 86 (41). Total: 187\nDifference: 67, which is more than the value of pick No. 3 (which is the same as the surplus value of pick 53)\nBeing extremely generous to the Jets, we could say they gave up the surplus value of a 2nd round pick to move up for a guard. Alternatively, we could say they gave up the surplus value of the No. 3 pick!\nPlayer for picks trade\nThis is somewhat complicated since some of the picks were in the future. Let’s be generous to the Seahawks and discount the 2022 picks by 10 percent. We will now use the APY version of the table in order to calculate the total cost of the Adams trade and extension:\nSeahawks receive: 2022 pick 109 (1.11 * .9)\nJets receive: 2021 pick 23 (3.28), 2021 pick 86 (1.50), 2022 pick 10 (3.42 * .9)\nDifference: 3.28 + 1.50 + (3.42 * .9) - (1.11 * .9) = 6.9 percent of the salary cap per year over 4 years\nSo we have arrived at 6.9% of the salary cap in APY per year over 4 years as the value of picks given up for the Seahawks. Adams’ 4-year deal extension with the Seahawks was $17.5 APY, or 9.6% of the salary cap at the time, per OTC. Dividing 6.9% (draft surplus given up) by 9.6% (contract APY), we need to inflate Adams’ APY by 72%, arriving at a true contract cost of, 30.1 million dollars per year over 4 years.\nThis is the same ballpark that Bill Barnwell landed on in his review of the trade, which was written before the extension was signed. Barnwell assumed an extension of $16 million APY:\n\n“Factor in the cost of acquiring Adams and that four-year, $64 million extension suddenly becomes a four-year, $108.5 million contract. Instead of paying Adams $16 million per year, now he has cost the Seahawks more than $27 million per season, which is more than anybody in the league who isn’t a quarterback. If their picks end up being juicier than they were expecting, that price goes up. Adams has to be the best non-quarterback in the league to make the math work.”\n\nCombine the Seahawks being worse than expected over this time period with Adams getting a somewhat larger deal than expected, and the estimates of the true cost of the trade and extension for Adams are remarkably consistent between Barnwell’s piece and this methodology.\nAnd hopefully now one can understand how surplus value calculations are formed.\nPart 4: The full table\nHere’s the full table:\n\n\nBB Draft Value Chart\n    Pick\n      Points1\n      APY2\n      OFV3\n      Pick\n      Points1\n      APY2\n      OFV3\n      Pick\n      Points1\n      APY2\n      OFV3\n      Pick\n      Points1\n      APY2\n      OFV3\n    1\n62\n2.25\n100\n65\n56\n2.05\n41\n129\n23\n0.84\n20\n193\n9\n0.33\n122\n65\n2.35\n99\n66\n55\n2.01\n41\n130\n23\n0.83\n20\n194\n9\n0.33\n113\n66\n2.39\n98\n67\n55\n1.97\n40\n131\n23\n0.82\n20\n195\n9\n0.32\n114\n68\n2.45\n96\n68\n54\n1.94\n39\n132\n22\n0.80\n20\n196\n9\n0.31\n115\n72\n2.60\n95\n69\n53\n1.91\n39\n133\n22\n0.79\n20\n197\n8\n0.31\n116\n81\n2.95\n94\n70\n52\n1.88\n38\n134\n22\n0.78\n19\n198\n8\n0.30\n117\n89\n3.20\n93\n71\n51\n1.85\n38\n135\n21\n0.77\n19\n199\n8\n0.29\n118\n96\n3.46\n91\n72\n50\n1.82\n37\n136\n21\n0.76\n19\n200\n8\n0.29\n119\n94\n3.41\n90\n73\n49\n1.79\n37\n137\n21\n0.77\n19\n201\n8\n0.28\n1110\n95\n3.42\n89\n74\n49\n1.76\n36\n138\n21\n0.76\n19\n202\n8\n0.28\n1111\n96\n3.49\n88\n75\n48\n1.74\n36\n139\n21\n0.77\n19\n203\n7\n0.27\n1112\n100\n3.62\n87\n76\n47\n1.72\n36\n140\n21\n0.76\n19\n204\n7\n0.26\n1013\n99\n3.60\n86\n77\n47\n1.69\n35\n141\n21\n0.75\n18\n205\n7\n0.26\n1014\n100\n3.61\n85\n78\n46\n1.67\n35\n142\n21\n0.74\n18\n206\n7\n0.25\n1015\n99\n3.57\n83\n79\n45\n1.64\n34\n143\n20\n0.74\n18\n207\n7\n0.25\n1016\n100\n3.60\n82\n80\n45\n1.62\n34\n144\n20\n0.73\n18\n208\n7\n0.24\n1017\n99\n3.57\n81\n81\n44\n1.60\n34\n145\n20\n0.72\n18\n209\n6\n0.23\n1018\n98\n3.54\n80\n82\n44\n1.58\n33\n146\n20\n0.72\n18\n210\n6\n0.23\n1019\n97\n3.50\n79\n83\n43\n1.56\n33\n147\n20\n0.71\n18\n211\n6\n0.22\n1020\n95\n3.44\n78\n84\n42\n1.54\n33\n148\n19\n0.70\n18\n212\n6\n0.22\n1021\n93\n3.38\n77\n85\n42\n1.52\n32\n149\n19\n0.69\n17\n213\n6\n0.21\n1022\n92\n3.33\n76\n86\n41\n1.50\n32\n150\n19\n0.69\n17\n214\n6\n0.21\n923\n91\n3.28\n75\n87\n41\n1.48\n32\n151\n19\n0.68\n17\n215\n6\n0.20\n924\n90\n3.25\n74\n88\n40\n1.46\n31\n152\n19\n0.67\n17\n216\n5\n0.20\n925\n88\n3.20\n73\n89\n40\n1.44\n31\n153\n18\n0.66\n17\n217\n5\n0.19\n926\n87\n3.15\n72\n90\n39\n1.42\n31\n154\n18\n0.66\n17\n218\n5\n0.19\n927\n86\n3.11\n71\n91\n39\n1.40\n30\n155\n18\n0.65\n17\n219\n5\n0.18\n928\n84\n3.05\n70\n92\n38\n1.38\n30\n156\n18\n0.64\n17\n220\n5\n0.18\n929\n85\n3.06\n69\n93\n38\n1.36\n30\n157\n17\n0.63\n16\n221\n5\n0.17\n930\n84\n3.03\n68\n94\n37\n1.34\n29\n158\n17\n0.62\n16\n222\n5\n0.17\n931\n83\n3.00\n67\n95\n37\n1.32\n29\n159\n17\n0.61\n16\n223\n5\n0.16\n932\n82\n2.96\n66\n96\n36\n1.30\n29\n160\n17\n0.60\n16\n224\n4\n0.16\n933\n87\n3.15\n65\n97\n35\n1.28\n28\n161\n16\n0.59\n16\n225\n4\n0.16\n934\n86\n3.10\n64\n98\n35\n1.26\n28\n162\n16\n0.58\n16\n226\n4\n0.15\n935\n84\n3.05\n63\n99\n34\n1.24\n28\n163\n16\n0.58\n16\n227\n4\n0.15\n836\n83\n3.01\n62\n100\n34\n1.22\n27\n164\n16\n0.57\n15\n228\n4\n0.14\n837\n82\n2.96\n62\n101\n33\n1.20\n27\n165\n15\n0.56\n15\n229\n4\n0.14\n838\n81\n2.92\n61\n102\n33\n1.18\n27\n166\n15\n0.55\n15\n230\n4\n0.13\n839\n80\n2.88\n60\n103\n32\n1.16\n26\n167\n15\n0.54\n15\n231\n4\n0.13\n840\n78\n2.84\n59\n104\n32\n1.15\n26\n168\n15\n0.53\n15\n232\n4\n0.13\n841\n77\n2.80\n58\n105\n32\n1.17\n26\n169\n14\n0.52\n15\n233\n3\n0.12\n842\n76\n2.76\n57\n106\n32\n1.16\n26\n170\n14\n0.52\n15\n234\n3\n0.12\n843\n75\n2.73\n57\n107\n32\n1.14\n25\n171\n14\n0.51\n14\n235\n3\n0.12\n844\n74\n2.69\n56\n108\n31\n1.13\n25\n172\n14\n0.50\n14\n236\n3\n0.11\n845\n73\n2.66\n55\n109\n31\n1.11\n25\n173\n14\n0.49\n14\n237\n3\n0.11\n846\n73\n2.62\n54\n110\n30\n1.10\n25\n174\n13\n0.48\n14\n238\n3\n0.11\n847\n72\n2.60\n53\n111\n30\n1.08\n24\n175\n13\n0.47\n14\n239\n3\n0.10\n848\n71\n2.57\n53\n112\n29\n1.07\n24\n176\n13\n0.46\n14\n240\n3\n0.10\n849\n70\n2.54\n52\n113\n29\n1.05\n24\n177\n13\n0.45\n14\n241\n3\n0.10\n850\n69\n2.51\n51\n114\n29\n1.04\n24\n178\n12\n0.44\n13\n242\n3\n0.09\n851\n68\n2.48\n50\n115\n28\n1.03\n23\n179\n12\n0.44\n13\n243\n3\n0.09\n852\n68\n2.45\n50\n116\n28\n1.01\n23\n180\n12\n0.43\n13\n244\n2\n0.09\n853\n67\n2.42\n49\n117\n28\n1.00\n23\n181\n12\n0.42\n13\n245\n2\n0.08\n754\n66\n2.39\n48\n118\n27\n0.98\n23\n182\n11\n0.42\n13\n246\n2\n0.08\n755\n65\n2.37\n48\n119\n27\n0.97\n23\n183\n11\n0.41\n13\n247\n2\n0.08\n756\n64\n2.33\n47\n120\n26\n0.96\n22\n184\n11\n0.40\n13\n248\n2\n0.08\n757\n63\n2.29\n46\n121\n26\n0.94\n22\n185\n11\n0.39\n13\n249\n2\n0.07\n758\n62\n2.26\n46\n122\n26\n0.93\n22\n186\n11\n0.39\n12\n250\n2\n0.07\n759\n62\n2.23\n45\n123\n25\n0.92\n22\n187\n10\n0.38\n12\n251\n2\n0.07\n760\n61\n2.19\n44\n124\n25\n0.90\n21\n188\n10\n0.37\n12\n252\n2\n0.07\n761\n60\n2.16\n44\n125\n25\n0.89\n21\n189\n10\n0.36\n12\n253\n2\n0.07\n762\n59\n2.12\n43\n126\n24\n0.88\n21\n190\n10\n0.36\n12\n254\n2\n0.06\n763\n58\n2.09\n42\n127\n24\n0.87\n21\n191\n10\n0.35\n12\n255\n2\n0.06\n764\n57\n2.05\n42\n128\n24\n0.85\n21\n192\n9\n0.34\n12\n256\n2\n0.06\n7Notes: @benbbaldwin | 2023-04-17\n    1 Surplus value: draft value points\n    2 Surplus value: APY as percent of salary cap\n    3 On-Field Value: draft value points IGNORING SALARY COST\n    \n\nHere is a link to a .csv of the table.\n\n\n\n\n\n\nView source code on GitHub\n\n\n\n\n\n\n",
    "preview": "posts/2023-02-23-nfl-draft-value-chart/nfl-draft-value-chart_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2023-04-17T16:10:48+00:00",
    "input_file": {},
    "preview_width": 7200,
    "preview_height": 4000
  },
  {
    "path": "posts/2021-08-15-QB-volatilty-gini-coefficients/",
    "title": "Exploring Quarterback Volatility with Gini Coefficients",
    "description": "Analyzing how volatile individual QB seasons are with gini coefficients and a volatility over expected (VOLoe) GAM model.",
    "author": [
      {
        "name": "Joseph Chernak",
        "url": "https://twitter.com/PatriotsStatsR/"
      }
    ],
    "date": "2021-08-15",
    "categories": [
      "Volatility",
      "nflfastR",
      "GAM"
    ],
    "contents": "\n\nContents\nIntro:\nData Prep and Packages\nGini Coefficient Calculation & Analysis\nVolatility Over Expected\n\n\n\n\nIntro:\nA few years ago, Bill Petti posted a great article on FanGraphs describing hitter volatility and how it can be quantified via Gini coefficients. Petti wrote that volatility is how a player distributes their overall season performance (as measured by wRC in his case) throughout the season on a game by game basis. This is opposed to streakiness which is related to clustering of good and bad performances over the course of a season. To quantify a hitters volatility, he proposed the usage of a Gini coefficient. Typically Gini coefficients are used to evaluate the wealth distribution in countries (0-1 scale where higher values indicate less equal distribution of wealth). In the case of football, a Gini coefficient can tell us how each individual quarterback distributes their cumulative season EPA (passes, rushes, penalties) on a game by game basis (includes playoffs).\nIn this post, I follow a similar methodology to Petti’s original work to calculate a quarterback’s volatility by season. In addition, I create a “Volatility Over Expected (VOLoe)” GAM model to evaluate a QB’s volatility within the context of their season EPA and total opportunities (passes, rushes, penalties).\nData Prep and Packages\nThis post uses NFLfastR quarterback data from 1999 to 2020. The full data prep process can be found on my GitHub (just a long loop that loads the data, adds rosters so we can filter for only QB’s, and adding team colors). Some of the notable assumptions in the data cleaning process are: 1. Games where QB x is involved in less than 5 plays (passes, rushes, penalties) are removed from that QB’s history. This is under the assumption that if QB x was involved in so few plays, they were injured or rested. Since the gini coefficient doesn’t consider the amount of plays in a game, this method protects the gini score from being inflated by very low opportunity, low EPA games. 2. Filtered for QB’s who threw greater than 250 passes in a season.\n\n\n#packages\nlibrary(dplyr)\nlibrary(ggcorrplot)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(mgcv)\nlibrary(nflfastR)\nlibrary(readr)\nlibrary(tidyverse)\n\n#load data\nDF <- read_rds(\"VOL_DF.rds\") %>%\n  filter(\n    plays >= 10,#remove any games where QB was involved in less than 10 plays\n    season_plays >= 250) #filter for greater than 250 attempts\n\n\n\nGini Coefficient Calculation & Analysis\nTo calculate each QB’s gini coefficient, we can use the same function created in Petti’s article. A typical gini calculation can’t be used with EPA because of negative EPA values, thus the modified gini method has to be used.\n\n\n#Gini coefficient function to handle negative EPA\nGini_Function <- function(Y) {\n  \n  Y <- sort(Y)\n  \n  N <- length(Y)\n  \n  u_Y <- mean(Y)\n  \n  top <- 2/N^2 * sum(seq(1,N)*Y) - (1/N)*sum(Y) - (1/N^2) * sum(Y)\n  \n  min_T <- function(x) {\n    \n    return(min(0,x))\n    \n  }\n  \n  max_T <- function(x) {\n    \n    return(max(0,x))\n    \n  }\n  \n  T_all <- sum(Y)\n  \n  T_min <- abs(sum(sapply(Y, FUN = min_T)))\n  \n  T_max <- sum(sapply(Y, FUN = max_T))\n  \n  u_P <- (N-1)/N^2*(T_max + T_min)\n  \n  return(top/u_P)\n  \n}\n\n#calculate gini scores for our QBs\npbp_QBs_gini <- DF %>% \n  select(id, season, Total_EPA) %>% \n  na.omit() %>% \n  aggregate(Total_EPA ~ id + season, data = ., FUN = \"Gini_Function\") %>%\n  rename(\"VOL\" = \"Total_EPA\")%>%\n  left_join(DF, by = c(\"id\", \"season\")) %>%\n  filter(VOL != \"NaN\")\n\n#remove extra data\nrm(DF)\nrm(Gini_Function)\n\n\n\nApplying the function to our data frame yields a gini score for each quarterback season since 1999 (minimum 250 throws in given season). We can then plot a QB’s volatility and EPA/Play in the 2020 season. Lower volatility is better, meaning up and to the right is best.\n\n\n#group up each QB & season\nQBs_Grouped <- pbp_QBs_gini %>%\n  group_by(\n    id,\n    season\n  ) %>%\n  #group by each QB to see their VOL on a season by season basis\n  summarize(\n    name  = unique(name),\n    color = unique(team_color),\n    VOL   = unique(VOL),\n    EPA_play = unique(Season_EPA),\n    Plays = sum(plays)\n  ) %>%\n  ungroup() \n\n#check who the least volatile, highest EPA/Play QB's were in 2020\nData_2020 <- QBs_Grouped %>%\n  filter(season == 2020) \n\nData_2020 %>%\n  ggplot(aes(x = EPA_play, y = VOL)) +\n  geom_point(colour = \"black\", shape=21, size = 3, \n             aes(x = EPA_play, y = VOL), fill = Data_2020$color, alpha = .75)+\n  theme_light() +\n  geom_abline(slope = -1.5, intercept = c(-1,-.8, -.6, -.4, -.2, 0), \n              alpha = .2)+\n  theme(plot.title = element_text(color=\"black\", size=8, face=\"bold\"))+\n  theme(plot.title = element_text(size = 10, face = \"bold\"),\n        plot.subtitle = element_text(size = 8))+\n  theme(plot.background = element_rect(fill = \"gray97\"))+\n  theme(panel.background = element_rect(fill = \"gray97\"))+\n  labs(title = \"Good Quarterbacks are Consistent While Bad Quarterbacks are Volatile\",\n       subtitle = \"Relationship Between Volatility and EPA/Play (2020) - Minimum 250 Plays\",\n       caption = \"Plot: PatriotsStatsR, Data: NFLFastR\")+\n  ylab(\"Volatility\")+\n  xlab(\"EPA/Play\")+\n  geom_hline(yintercept = mean(Data_2020$VOL) , linetype = \"dashed\") +\n  geom_vline(xintercept = mean(Data_2020$EPA_play), linetype = \"dashed\") +\n  ggrepel::geom_text_repel(aes(label=name), box.padding = 0.4) +\n  scale_y_reverse() \n\n\n\n\nThis isn’t too surprising, good quarterbacks (as measured by EPA/Play) are consistent while middle of the pack QB’s tend to be the most volatile. Once we reach the worst QB’s (Wentz, Haskins, Darnold) those players tend to be consistently bad. Essentially, the relationship between EPA/Play and volatility is a parabola shape.\nThis plot also provides perspective on QB’s who have a similar EPA/Play but have very different levels of volatility. For example, Wentz (-0.06 EPA/Play) and Foles (-0.08 EPA/Play). If a decision maker was in the unfortunate position of deciding between these two below average QB’s, they could possibly prefer the volatile QB (Foles, who has a chance of a great game) over a low volatility QB (Wentz, who is consistently mediocre).\nVolatility Over Expected\nVOL tells a simple story but it doesn’t tell us how much more volatile a QB was given their EPA. E.g. Ryan Tannehill had a 0.31 EPA/Play and a volatility of 0.45, was this more volatile than expected given his performance? We can built a Generalized Additive Model (GAM) with MGCV that includes variables that correlate with VOL. The two variables that most correlate with VOL are EPA/Play & Total Plays.\n\n\n#create melted data frame of plays & EPA/Play\nmodel_1_data_melted <- QBs_Grouped %>%\n  rename(\"EPA/Play\" = \"EPA_play\") %>%\n  gather(key = feature, value = value, -VOL) %>%\n  filter(feature %in% c(\"EPA/Play\", \"Plays\")) %>%\n  mutate(value = as.numeric(value), feature = factor(feature))\n\n#color palette from Petti\ngini_palette <- c('#3E0002', '#8e001c', '#D87F83', '#969696', '#636363', '#252525')\n\n#plot \nmodel_1_data_melted %>%\n  ggplot(aes(value, VOL)) + \n  geom_point(aes(color = feature), size = 2, alpha = .5) + \n  facet_wrap(~feature, scales = \"free_x\") +\n  xlab(\"\\nVariable Value\") +\n  ylab(\"\\nVolatility\\n\") +\n  theme_minimal(base_size = 12, base_family = \"cairo\") %+replace%\n  theme(\n    panel.border = element_blank(),\n    panel.background = element_blank(),\n    panel.grid.major = element_line(color='#BFBFBF', size=.25),\n    axis.title = element_text(face='bold', hjust=.5, vjust=0),\n    axis.text = element_text(color='black')\n  ) +\n  theme(plot.title = element_text(color=\"black\", size=8, face=\"bold\"))+\n  coord_cartesian(clip = \"off\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10))+\n  theme(plot.background = element_rect(fill = \"gray97\"))+\n  theme(panel.background = element_rect(fill = \"gray97\"))+\n  labs(title = \"Relationship Between Volatility, Plays, and EPA/Play\",\n       subtitle = \"Minimum 250 Plays (Passes, Rushes & Penalties): Seasons 1999 - 2020\",\n       caption = \"Plot: PatriotsStatsR, Data: NFLFastR\")+\n  scale_color_manual(values = gini_palette, \"Variables\") +\n  theme(title = element_text(face = \"bold\"), \n        axis.text = element_text(face = \"bold\"),\n        strip.text.x = element_text(face = \"bold\")) \n\n\n\n#remove extra data\nrm(gini_cor)\nrm(model_1_data_melted)\nrm(pbp_QBs_gini)\nrm(Data_2020)\n\n\n\nAnd then we can built the model, I tried a few variations but the model with a 9 knot cubic smoothing spline on EPA/Play performed best.\n\n\n#Build expected VOL model that controls for EPA & Plays\nModeling_Data <- QBs_Grouped %>%\n  select(\n    VOL,\n    EPA_play,\n    Plays\n  )\n\n#training test split\nset.seed(2018)\ndf    <- sample(nrow(Modeling_Data), nrow(Modeling_Data) * .7)\ntrain <- Modeling_Data[df,]\ntest  <- Modeling_Data[-df,]\n\n#tried a few models, GAM performed best\n#fit model after trying a few different combos\nmodel1 <- mgcv::gam(VOL ~ s(EPA_play, k = 9, bs=\"cr\") + Plays, data = train)\n\n#check again\nmgcv::gam.check(model1, k.rep=1000) #K & EDF are not close, p is no longer significant\n\n\n\n\nMethod: GCV   Optimizer: magic\nSmoothing parameter selection converged after 4 iterations.\nThe RMS GCV score gradient at convergence was 4.962146e-09 .\nThe Hessian was positive definite.\nModel rank =  10 / 10 \n\nBasis dimension (k) checking results. Low p-value (k-index<1) may\nindicate that k is too low, especially if edf is close to k'.\n\n             k' edf k-index p-value\ns(EPA_play) 8.0 4.7    1.02    0.69\n\nsummary(model1) #0.75 adjusted r squared\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nVOL ~ s(EPA_play, k = 9, bs = \"cr\") + Plays\n\nParametric coefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  6.892e-01  9.293e-03  74.170  < 2e-16 ***\nPlays       -6.668e-05  1.671e-05  -3.991 7.53e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n              edf Ref.df   F p-value    \ns(EPA_play) 4.704  5.597 238  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.748   Deviance explained = 75.1%\nGCV = 0.0027458  Scale est. = 0.0027109  n = 527\n\n#check rmse\nfit_1 <- predict(model1, test)\nmodel_1_res_act <- data.frame(actuals = test$VOL, predicted = fit_1) %>%\n  mutate(errors = predicted - actuals)\nrmse <- function(df) {\n  rmse <- sqrt(mean((df$actuals-df$predicted)^2))\n  rmse\n}\n\nrmse(model_1_res_act) #.05 rmse\n\n\n[1] 0.04983998\n\nThe model has an adjusted R squared of .75, an RMSE of .05, and the GAM check diagnostics pass the smell test. After fitting the model to our data, we can re-plot 2020 QB EPA/Play and VOL over expected. VOLoe values slightly off 0 (Allen, Mahomes, Brady) shouldn’t be cause for concern, instead they should be interpreted as about right where expected. The primary usage of VOLoe should be in detecting significantly higher or lower VOLoe values. Higher VOLoe is worse and represents a more volatile than expected season.\n\n\n#fit model to actual data\nfit_all <- data_frame(predicted = predict(model1, QBs_Grouped))\nfit_values <- cbind(QBs_Grouped, fit_all) %>%\n  mutate(VOLoe = VOL - predicted)\n\nrm(model1)\nrm(fit_all)\nrm(model_1_res_act)\nrm(test)\nrm(train)\nrm(Modeling_Data)\nrm(df)\nrm(fit_1)\nrm(model1)\nrm(QBs_Grouped)\nrm(rmse)\n\n#examine 2020 QBs after controlling for factors in model\nfit_2020 <- fit_values %>%\n  filter(season == 2020) \n\nfit_2020 %>%\n  ggplot(aes(y = VOLoe, x = EPA_play)) +\n  geom_point(colour = \"black\", shape=21, size = 3, \n             aes(y = VOLoe, x = EPA_play), fill = fit_2020$color, alpha = .75)+\n  theme_light() +\n  theme(plot.title = element_text(color=\"black\", size=8, face=\"bold\"))+\n  theme(plot.title = element_text(size = 10, face = \"bold\"),\n        plot.subtitle = element_text(size = 8))+\n  theme(plot.background = element_rect(fill = \"gray97\"))+\n  theme(panel.background = element_rect(fill = \"gray97\"))+\n  labs(title = \"Are Tannehill and Fitzpatricks's High VOLoe Cause for Concern?\",\n       subtitle = \"Relationship Between VOLoe and EPA/Play (2020) - Minimum 250 Dropbacks\",\n       caption = \"Plot: PatriotsStatsR, Data: NFLFastR\")+\n  ylab(\"Volatility Over Expected\")+\n  xlab(\"EPA/Play\")+\n  geom_hline(yintercept = 0 , linetype = \"dashed\") +\n  geom_vline(xintercept = mean(fit_2020$EPA_play), linetype = \"dashed\") +\n  ggrepel::geom_text_repel(aes(label=name), box.padding = 0.2)\n\n\n\nrm(fit_2020)\n\n\n\nMost quarterbacks were close to their predicted level of volatility. Two QB’s that particularly stick out are Fitzpatrick and Tannehill, both performed well but had high volatility over expected scores. A quick investigation of QB’s who had a similar high level of EPA/Play and VOL over expected could perhaps provide insight into if we should be skeptical of this profile.\n\n\n#find QBs with 85th percentile or greater EPA & VOL\nquantile(fit_values$EPA_play, probs = c(0.85)) #.18 \n\n\n      85% \n0.1781728 \n\nquantile(fit_values$VOLoe, probs = c(0.85)) #.05 or more\n\n\n       85% \n0.04765702 \n\n#filter for QBs that fit those conditions\noutlier_qbs <- fit_values %>%\n  filter(EPA_play >= .178, VOLoe >= .05) %>%\n  mutate(outlier = 1) %>%\n  select(id,\n         season,\n         outlier)\n\n#join data and create table\nfit_values %>%\n  left_join(outlier_qbs, by = c(\"id\", \"season\")) %>%\n  group_by(id) %>%\n  #create indicator that will allow us to filter out QBs without season of interest\n  mutate(outlier = mean(outlier, na.rm = TRUE)) %>%\n  ungroup() %>%\n  filter(outlier != \"NaN\") %>%\n  #create summary columns\n  mutate(\n    outlier = ifelse(EPA_play >= .178 & VOLoe >= .05, 1, 0), #mark outliers\n    VOLoe_prior = lag(VOLoe),\n    follower = lag(outlier),\n    prior_season = lag(season),\n    EPA_prior = round(lag(EPA_play), digits = 3),\n    EPA_diff = round(EPA_play - lag(EPA_play), digits = 3),\n    EPA_play = round(EPA_play, digits = 3)) %>%\n  filter(follower == 1) %>%\n  select(\n    name,\n    season,\n    prior_season,\n    VOLoe_prior,\n    EPA_prior,\n    EPA_play,\n    EPA_diff\n  ) %>%\n  rename(\"VOLoe Prior Season\" = \"VOLoe_prior\") %>%\n  rename(\"EPA/Play Prior Season\" = \"EPA_prior\") %>%\n  rename(\"EPA/Play Next Season\" = \"EPA_play\") %>%\n  rename(\"Change in EPA\" = \"EPA_diff\") %>%\n  arrange(desc(season)) %>%\n  filter(prior_season + 1 == season) %>%\n  arrange(desc(season)) %>%\n  gt()\n\n\n\nname\n      season\n      prior_season\n      VOLoe Prior Season\n      EPA/Play Prior Season\n      EPA/Play Next Season\n      Change in EPA\n    D.Prescott\n2020\n2019\n0.05301281\n0.197\n0.137\n-0.060L.Jackson\n2020\n2019\n0.05003333\n0.276\n0.172\n-0.103D.Brees\n2019\n2018\n0.10650383\n0.303\n0.184\n-0.119M.Trubisky\n2019\n2018\n0.05707277\n0.181\n-0.032\n-0.213D.Watson\n2018\n2017\n0.10174379\n0.220\n0.123\n-0.097R.Wilson\n2016\n2015\n0.12598953\n0.212\n0.093\n-0.119N.Foles\n2014\n2013\n0.06624689\n0.280\n0.012\n-0.268P.Rivers\n2012\n2011\n0.08539519\n0.183\n0.018\n-0.166M.Vick\n2011\n2010\n0.05385099\n0.189\n0.106\n-0.083D.Brees\n2010\n2009\n0.13467862\n0.298\n0.138\n-0.161P.Manning\n2009\n2008\n0.06238581\n0.225\n0.288\n0.063P.Rivers\n2009\n2008\n0.09014322\n0.241\n0.319\n0.078T.Romo\n2008\n2007\n0.05295818\n0.222\n0.051\n-0.172P.Manning\n2007\n2006\n0.07383574\n0.237\n0.259\n0.021T.Romo\n2007\n2006\n0.10809212\n0.202\n0.222\n0.020P.Manning\n2006\n2005\n0.07194718\n0.347\n0.237\n-0.109B.Roethlisberger\n2006\n2005\n0.08175888\n0.316\n0.023\n-0.293D.Brees\n2005\n2004\n0.07269145\n0.314\n0.136\n-0.178T.Green\n2004\n2003\n0.11238382\n0.187\n0.240\n0.053P.Manning\n2004\n2003\n0.08439443\n0.233\n0.416\n0.183R.Gannon\n2001\n2000\n0.07694551\n0.194\n0.183\n-0.011\n\nVery small sample size disclaimer but most QB’s with this profile experienced a decrease in their EPA/Play the following season (-.08 average decrease in EPA/Play). It’s difficult to know if this is because of a high VOLoe in the prior season or because it is difficult to maintain an elite level of EPA. This does align with some of skepticism people have about Tannehill and Fitzpatrick though.\nOther Questions to Answer\nThere are a lot of questions related to volatility that come to mind, but for the sake of brevity I will touch upon one that immediately comes to mind: does volatility of VOLoe change with a QB’s age? We can examine both metrics by age.\n\n\n#first, merge data with rosters and calculate age\ncalc_age <- function(birthDate, refDate = Sys.Date(), unit = \"year\") {\n  \n  require(lubridate)\n  \n  if(grepl(x = unit, pattern = \"year\")) {\n    as.period(interval(birthDate, refDate), unit = 'year')$year\n  } else if(grepl(x = unit, pattern = \"month\")) {\n    as.period(interval(birthDate, refDate), unit = 'month')$month\n  } else if(grepl(x = unit, pattern = \"week\")) {\n    floor(as.period(interval(birthDate, refDate), unit = 'day')$day / 7)\n  } else if(grepl(x = unit, pattern = \"day\")) {\n    as.period(interval(birthDate, refDate), unit = 'day')$day\n  } else {\n    print(\"Argument 'unit' must be one of 'year', 'month', 'week', or 'day'\")\n    NA\n  }\n  \n}\n\n#loop to get 1999 to 2020 rosters\nrosters <- data.frame()\n\nfor (x in 1999:2020) {\n  df <- nflfastR::fast_scraper_roster(x) %>%\n    mutate(birth_date = as.Date(birth_date)) %>%\n    select(\n      position,\n      birth_date,\n      gsis_id,\n      season,\n      full_name,\n      team\n    ) %>%\n    mutate(Age_Sep_1 = calc_age(birth_date, paste0(x,\"-09-01\")))\n  rosters <- rbind(df, rosters)\n}\n\nrosters <- rosters %>%\n  filter(position == \"QB\")\n\n#bind with VOL data & plot\nfit_values %>%\n  left_join(rosters, by = c(\"id\" = \"gsis_id\", \"season\" = \"season\")) %>%\n  group_by(Age_Sep_1) %>%\n  summarize(VOLoe   = mean(VOLoe, na.rm = TRUE),\n            VOL     = mean(VOL, na.rm = TRUE),\n            Sample = n()) %>%\n  gather(key = feature, value = value, -Age_Sep_1, - Sample) %>%\n  filter(feature %in% c(\"VOLoe\", \"VOL\")) %>%\n  ggplot(aes(Age_Sep_1, value)) + \n  geom_point(aes(color = feature, size = Sample),  alpha = .5) + \n  facet_wrap(~feature, scales = \"free_y\") +\n  xlab(\"Age (As of September 1st of Season)\") +\n  ylab(\"Feature Value\") +\n  theme_minimal(base_size = 12, base_family = \"cairo\") %+replace%\n  theme(\n    panel.border = element_blank(),\n    panel.background = element_blank(),\n    panel.grid.major = element_line(color='#BFBFBF', size=.25),\n    axis.title = element_text(face='bold', hjust=.5, vjust=0),\n    axis.text = element_text(color='black')\n  ) +\n  theme(plot.title = element_text(color=\"black\", size=8, face=\"bold\"))+\n  coord_cartesian(clip = \"off\") +\n  theme(plot.title = element_text(size = 12, face = \"bold\"),\n        plot.subtitle = element_text(size = 10))+\n  theme(plot.background = element_rect(fill = \"gray97\"))+\n  theme(panel.background = element_rect(fill = \"gray97\"))+\n  labs(title = \"No Relationship Between QB Age and Volatility or VOL Over Expectation\",\n       subtitle = \"Minimum 250 Plays (Passes, Rushes & Penalties): Seasons 1999 - 2020\",\n       caption = \"Plot: PatriotsStatsR, Data: NFLFastR\") +\n  scale_color_manual(values = gini_palette, \"Features\") +\n  theme(title = element_text(face = \"bold\"), \n        axis.text = element_text(face = \"bold\"),\n        strip.text.x = element_text(face = \"bold\"))+\n  geom_smooth(method = lm, formula = y ~ splines::bs(x, 7), se = FALSE, color = \"black\")\n\n\n\nrm(rosters)\nrm(df)\nrm(x)\nrm(calc_age)\n\n\n\nThere doesn’t appear to be very much of a relationship. This is also a difficult question to answer because of survivorship bias. Good QB’s play longer, good QB’s also tend to have low volatility. Thus, volatile QB’s tend to be average to below average QB’s and don’t play as long.\nWrapping Up\nThis post demonstrated that on a macro scale, good QB’s have low volatility while middle of the pack QB’s tend to have the highest level of volatility. Raw volatility could be helpful for deciding between backup QB’s with similar EPA/Play (prefer the volatile bad QB due to the chance of a good game). Volatility over expected (VOLoe) can be used to detect high level EPA/Play seasons that also have a high VOLoe (possible indication of future regression).\n\n\n\nView source code on GitHub \n\n\n\n\n\n",
    "preview": "posts/2021-08-15-QB-volatilty-gini-coefficients/QB_Volatility_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2023-04-17T16:10:48+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2100
  },
  {
    "path": "posts/2021-07-30-evauluating-defenses-by-how-well-they-play-the-offenses-wr-1/",
    "title": "Evauluating defenses by how well they play the offense's WR 1",
    "description": "Cornerback performance is hard to measure, but we can try.",
    "author": [
      {
        "name": "Jonas Trostle",
        "url": "https://twitter.com/JonasTrostle/"
      }
    ],
    "date": "2021-07-30",
    "categories": [
      "Figures",
      "nflfastR",
      "Defense"
    ],
    "contents": "\n\nContents\nData & transformations\nThe plot (thickens)\nCoda\n\n\n\n\nRoll the clocks back to 2009. Tom Brady is back from tearing his ACL and MCL, Brett Favre is a Viking, and the Colts and Saints are a combined 26-0 by week 13. Nothing was quite as beautiful as what Darrelle Revis was doing for the New York Jets. Week after week, Revis would eliminate the opponent’s best receiver.\nMeasuring this dominance is hard however. Can we do better than just adding up the EPA from his swats and tackles, or just summarizing the defense as a whole? Instead of that, what if we ranked every team’s wide receivers and compared how each defense did against the opponent’s number one playmaker?\nData & transformations\nFirst we need to load in the data, and to do that we follow Thomas Mock’s great guide on using Arrow to speed up the process.\n\n\nlibrary(arrow)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(nflfastR)\nlibrary(ggpmisc)\noptions(mc.cores = parallel::detectCores())\n\nset.seed(2009)\n\ndir.create(\"nflfastr\")\n\nget_data <- function(year){\n  dir.create(file.path(\"nflfastr\", year))\n\n  download.file(\n    glue::glue(\"https://github.com/guga31bb/nflfastR-data/blob/master/data/play_by_play_{year}.parquet?raw=true\"),\n    file.path(\"nflfastr\", year, \"data.parquet\"),\n    mode = 'wb'\n  )\n}\n\nwalk(1999:2019, get_data)\n\nds <- open_dataset(\"nflfastr\", partitioning = \"year\")\n# We're grabbing everything since 1999 so I only have to do it once,\n# but feel free to just do individual seasons\nds %>%\n  select(\n    desc,\n    posteam,\n    defteam,\n    receiver,\n    receiver_id,\n    epa,\n    cpoe,\n    pass,\n    qb_epa,\n    air_yards,\n    year,\n    yards_gained,\n    penalty\n  ) %>%\n  filter(year >= 1999, penalty == 0,!is.na(epa)) %>%\n  collect() -> pbp\n\n\n\nNext, we want to combine the play by play data with the roster data for receivers.\n\n\n# Again, if you're just doing an individual season you can change this\nnflfastR::fast_scraper_roster(1999:2020) -> roster\n\npbp %>%\n  select(\n    desc,\n    posteam,\n    defteam,\n    receiver,\n    receiver_id,\n    epa,\n    cpoe,\n    pass,\n    qb_epa,\n    air_yards,\n    year,\n    yards_gained\n  ) %>%\n  filter(!is.na(receiver_id)) -> pbp2\n\npbp2 %>%\n  left_join(roster, by = c(\"receiver_id\" = \"gsis_id\")) -> pbp3\n\n\n\nNow that we have the play by play merged with the roster data, we can go ahead and start transforming the data. We need to rank an offense’s wide receivers, and there are many ways to do so. For now, we’ll use targeted air yards, but we can change this decision later.\n\n\npbp3 %>%\n  # we keep only the wide receivers, no tight ends or running backs\n  filter(position == \"WR\") %>% \n  group_by(receiver_id, year) %>%\n  # we sum up all the receivers air yards when targeted\n  mutate(targeted_air_yards = sum(air_yards)) %>%\n  ungroup() %>%\n  # distinct allows us to keep only one row per player\n  distinct(receiver_id, year, .keep_all = T) %>%\n  # we want to rank receivers by team, not overall, so we group by offense\n  group_by(posteam, year) %>%\n  # since we want to rank the receivers, arrange allows us to order them by some\n  # criterion\n  arrange(-targeted_air_yards) %>%\n  # this index is now the within-team rank of each wide receiver that year\n  mutate(index = row_number()) %>%\n  ungroup() %>%\n  select(receiver_id, targeted_air_yards, index, year) -> pbp4\n\n\n\nNow that we have every receivers rank, we’ll add that back to the play by play data and remove any plays that didn’t have a receiver targeted.\n\n\npbp2 %>%\n  left_join(pbp4) -> pbp5\n\npbp5 %>%\n  filter(!is.na(index)) -> pbp6\n\n\n\nAt this point, I think the best way to graph this would be to compare how a defense does against WR 1, and against all other wide receivers. To do this, we’ll create a binary variable that says whether or not a receiver was number one. Uncreatively, I called this the smittywerbenjagermenjensen index, but you can rename it whatever you like.\n\n\npbp6 %>%\n  mutate(smittywerbenjagermenjensen = if_else(index == 1, 1,0)) -> pbp7\n\n\n\nAlmost done. We’re going to group by the new smittywerbenjagermenjensen index and by defense and calculate the average EPA given up to WR 1 vs all others. Like ranking the receivers, there are lots of ways to skin a cat: possible options include using qb_epa instead, or CPOE, or summing any of the above instead of averaging them. All will give a slightly different answer.\n\n\npbp7 %>%\n  group_by(defteam, smittywerbenjagermenjensen, year) %>%\n  summarise(epa = mean(epa)) %>%\n  ungroup() -> pbp8\n\n# I could live to be 100 and never learn how to pivot correctly on the first try\n\npbp8 %>%\n  pivot_wider(names_from = smittywerbenjagermenjensen,\n              values_from =  epa,\n              names_prefix = \"wr_\") -> pbp9\n\n# We'll add the team colors so the graph looks nice\n\npbp9 %>%\n  left_join(teams_colors_logos, by = c('defteam' = 'team_abbr')) -> pbp10\n\npbp10 %>%\n  filter(year == 2009) -> pbp11\n\n\n\nThe plot (thickens)\nIs it done? Yes! Now we can plot it and see which defenses could shutdown each opponent’s best receiver.\n\n\n\nWow! At a glance we can see that there are three teams that really shutdown the opponent’s best wide receiver. If you think back to ’09, even though Revis founded his island that year and solidified himself as the best cover corner in the league, it was Charles Woodson of the Packers who won defensive player of the year. In a very strange twist of fate however, the best performance was from New Orleans.\nThe coolest part about this, for me at least, is that we could recover all of these narratives from the data. Without measuring any cornerbacks directly, we can get a measurement that passes the eye test rather convincingly.\nOf course, this measure is flawed in that we still can’t directly measure a player’s contribution. For example, the corners in New Orleans almost always had Darren Sharper over top to help him, and there’s no guarantee that they were actually covering WR 1 every play. For a player like Revis, you could argue that because he could eliminate the opponent’s WR 1 the rest of the defense could roll towards the other receivers and play them better, which is why the Jets are so much more rightward on the graph than the Saints.\nCoda\nWe could argue endlessly about the best cover corners and seasons in NFL history, but as a coherence check I think we should take a look at how this new metric ranks every defense since 1999. To paraphrase Ben Baldwin, a coherence check for these models is “Do we find that Stephon Gilmore is in the top 5?”\nWith that in mind, let’s check out what the model thinks the best coverage season is. The code is mostly identical to the first section, but I put comments throughout the code to explain any changes. The biggest change is that we can’t use targeted air yards, since those don’t go far enough back, so I’ve elected to use total yards. Again, total EPA would be just as legitimate, as would targets or receptions. I invite you to play around with it.\n\n\n\n\n\n\nSure enough, Stephon Gilmore’s 2019 season is in the top right, and ranks #4 since 1999. Surprisingly, that 2009 season by the Saints ranks as the best of the millennium. Just as surprising is that the Legion of Boom is nowhere in sight.\nFor the astute viewer, you may notice that the coverage score against WR 1 and against all other WRs is correlated. This could lend credibility to the theory that a shutdown corner allows the defense to divert resources elsewhere, or it could just be that good coordinators get better performances from all corners.\nThis is by no means the be-all end-all of cornerback rankings. I’d love to see this extended to account for the quality of receivers faced and for different eras. A version looking at all receivers, including TEs and RBs, may be even more enlightening. Finally, like the team tiers, average EPA allowed to WR 1 is worth a different amount than the average EPA allowed to all other receivers, and it would be great to sort these defenses into tiers, as well as do a stability analysis to see if any of these measures are more stable than regular defense.\n\n\n\nView source code on GitHub \n\n\n\n\n\n",
    "preview": "posts/2021-07-30-evauluating-defenses-by-how-well-they-play-the-offenses-wr-1/evauluating-defenses-by-how-well-they-play-the-offenses-wr-1_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3600
  },
  {
    "path": "posts/2021-06-27-estimating-team-ability-from-epa/",
    "title": "Estimating Team Ability From EPA",
    "description": "We build a series of models to estimate team ability from EPA data",
    "author": [
      {
        "name": "Richard Anderson",
        "url": "http://richjand.rbind.io"
      }
    ],
    "date": "2021-06-27",
    "categories": [
      "nflfastR",
      "stan"
    ],
    "contents": "\n\nContents\nData\nThe Simplest Model\nAdding Defense\nUsing the Student’s T\nConclusions\nAppendix: Team Estimates\n\nWith 9:30 left in the third quarter of the fourth week of the 2019 season the Kansas city Chiefs had allowed .102 Expected Points Added (EPA) per play on defense for the season. Had they finished the season at .102 EPA/Play they would have been 30th in the NFL. On the next play, first and goal from the KC 1 yard line, the Chiefs returned a fumble 99 yards for a touchdown which was worth -13.6 EPA. 22 seconds after being at .102 the Chiefs were at .042 EPA/Play which would have been 22nd best at the end of the season.\nFollow NFL analytics Twitter for any amount of time and you’ll see EPA/Play used to evaluate NFL teams. The example above shows how doing so in limited samples can be dangerous. EPA is noisy and quality of competition differs sharply across teams. Given these complications we need to go beyond raw EPA/Play estimates. The fundamental questions we want to answer are what we should believe about a team given the information we have and how uncertain we should be about that belief and we are going to use models to answer those questions.\nWe can do better than the status quo using some fairly simple multilevel models that build in information that we know should affect our inferences, adjust for competition, measure the play-to-play uncertainty in EPA, and apply some regularization to our estimates of team ability using what we’ve observed from other teams. Using multilevel models to estimate team ability is becoming more common and the goal of this post is to show how you can build your own model of EPA that will return regularized, opponent-adjusted measures of offensive and defensive EPA/play for each team.\nData\nYou can see how the data was set up as well as the calculation of the Chiefs pre-and-post-interception EPA below.\n\n\nlibrary(rstan)\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(DT)\nlibrary(tidybayes)\nlibrary(ggrepel)\nlibrary(magick)\nlibrary(resample)\n\nset.seed(1234)\n\nseasons <- 2018:2020\ndat <- purrr::map_df(seasons, function(x) {\n  readRDS(\n    url(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\")\n    )\n  )\n})\n\npost16 <- filter(dat,\n                   season_type == 'REG' & \n                   !is.na(epa) &\n                   play_type %in% c('pass','run') &\n                   down %in% c(1,2,3,4)\n                 ) %>%\n  dplyr::select(season, week, posteam, defteam, epa, play_id, qtr, quarter_seconds_remaining) %>%\n  mutate(def_id = as.numeric(factor(str_c(defteam, '-', season))),\n         off_id = as.numeric(factor(str_c(posteam, '-', season))))\n\nepa_off_actual <- post16 %>%\n  group_by(posteam, season, off_id) %>%\n  summarise(offensive_epa = mean(epa),\n            n_offense = n())\n\nepa_def_actual <- post16 %>%\n  group_by(defteam, season, def_id) %>%\n  summarise(defensive_epa = mean(epa))\n\nepa_actual <- epa_off_actual %>%\n  inner_join(epa_def_actual, by = c('posteam' = 'defteam','season','off_id' = 'def_id'))\n\nchiefs_2019 <- filter(post16, week <= 4 & season == 2019 & (posteam == 'KC'|defteam == 'KC')) %>%\n  arrange(week, play_id) %>%\n  mutate(play_num = row_number(),\n         bp = ifelse(epa < -13, 1, 0))\n\nbig_play_id <- which(chiefs_2019$bp == 1)\n\npre <- chiefs_2019 %>%\n  filter(play_num < big_play_id,\n         defteam == 'KC') %>%\n  summarise(epa = mean(epa))\n\nwith <- chiefs_2019 %>%\n  filter(play_num <= big_play_id,\n         defteam == 'KC') %>%\n  summarise(epa = mean(epa))\n\nrm(dat)\n\n\n\nThe Simplest Model\nLet’s start by saying our goal is to get regularized estimates of offensive ability. The simplest multilevel model we could build is something like the equation below.\n\n\\(epa_{ij} \\sim Normal(\\alpha_{ij}, \\sigma)\\)\n\nThis says that the epa of play \\(i\\) when team \\(j\\) has possession is normally distributed with a mean of \\(alpha_{ij}\\) and standard deviation \\(\\sigma\\). We can think of \\(alpha_{ij}\\) as our estimate of team ability and to build the model we will put some hierarchical priors on this estimate. We’ll assume that EPA is centered at 0 and that team abilities follow a normal distribution with a standard deviation of \\(\\sigma_{off}\\). More formally,\n\n\\(\\alpha_{ij} \\sim Normal(0, \\sigma_{off})\\)\n\nThe model is going to include all offensive plays from 2018 through 2020. This excludes penalties and two point conversions.\nWe can estimate this model with the stan code below.\n\ndata{\n  int<lower = 0> N; //number of plays\n  int<lower = 1> I; //number of teams\n  int<lower = 0, upper = I> ii[N]; //indicator for offense\n  real<lower = -16, upper = 10> y[N]; //epa\n}\nparameters{\n  real<lower = 0> sigma_y; //error for normal distribution\n  real<lower = 0> sigma_off; //standard deviation of offensive ability\n  vector[I] alpha_off_raw;\n}\ntransformed parameters{ //using non-centered paramaterization\n  vector[I] alpha_off = alpha_off_raw * sigma_off;\n}\nmodel{\n  //priors\n  alpha_off_raw ~ normal(0,1);\n  sigma_off ~ normal(.05,.03);\n  sigma_y ~ normal(1,.2);\n  \n  //likelihood\n    y ~ normal(alpha_off[ii], sigma_y);\n}\n\nModels in stan require that the user specify the data as a list with an entry corresponding to each argument in the data block. The stan_model function compiles the model and sampling executes the sampling for the model.\n\n\nstanmod <- stan_model('stan-models/epa-per-play-offense-only.stan')\n\nstandat <- list(N = nrow(post16),\n                I = length(unique(post16$off_id)),\n                ii = post16$off_id,\n                y = post16$epa\n                )\n\nfit_off <- sampling(stanmod, data = standat, cores = 4, chains = 4, iter = 2000)\n\n\n\nBelow you can see what the model is doing. The blue points are our model estimate of team ability while the white points are the team’s actual EPA/Play. This model basically just pulls every team toward zero. The order of estimated team abilities is the same as the order of actual EPA/Play, though this doesn’t necessarily have to be the case given that sample sizes vary across teams.\nOne interesting part of these results is just how noisy the team estimate are. Even with a full season of data it’s very difficult to separate the bulk of teams. We are very confident that the Jets are bad and the Packers and Chiefs are good but we aren’t particularly confident in the order of any of these teams. We can become more confident in our estimates by building in additional information, but raw EPA/Play doesn’t tell us enough to confidently rank teams.\nAn important point to note is that the degree to which teams are being pulled back toward zero is determined by the model. If there were no value in pooling information across teams the model estimates would equal what we actually observed from teams. The fact that we see teams being pulled back toward zero suggests that there is value in assuming that we can learn about reasonable values for teams by observing what other teams have done. Regression to the mean is real!\n\n\nalpha_off <- rstan::extract(fit_off, pars = c('alpha_off'))$alpha_off\noffense <- colMeans(alpha_off)\noffense_se <- sqrt(colVars(alpha_off))\nepa_actual$offense_only_estimate <- offense \nepa_actual$offense_only_se <- offense_se\nepa_actual$offense_only_lower <- epa_actual$offense_only_estimate - epa_actual$offense_only_se\nepa_actual$offense_only_upper <- epa_actual$offense_only_estimate + epa_actual$offense_only_se\n\nfilter(epa_actual, season == 2020) %>%\n  ggplot(aes(y = reorder(posteam, offense_only_estimate), x = offensive_epa)) + \n  geom_point(shape = 1) +\n  geom_point(aes(x = offense_only_estimate, y = posteam), colour = 'blue') +\n  geom_linerange(aes(xmin = offense_only_lower, xmax = offense_only_upper)) +\n  theme_minimal() +\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(title = 'Estimated vs. Actual Offensive EPA/Play, 2020',\n       subtitle = 'Model Estimate in Blue, +/- 1 s.e.') +\n  ylab('Team') +\n  xlab('Offensive EPA')\n\n\n\n\nAt this point we could kick our heels up and enjoy a job well done knowing that we’ve applied some regularization and have estimates of offensive team ability that are going to provide a better projection than raw averages. However, we also want to know how good teams are at defense. While we could do exactly this same process for defensive plays, a better approach is to build both into one model. This will give us estimates of offensive and defensive team ability that are regularized and adjusted for quality of opponent.\nAdding Defense\nBuilding a model with offense and defense requires adding two additional parameters. We will use \\(\\alpha_{ik}\\) to represent defensive ability and we will need to estimate \\(\\sigma_{def}\\), the standard deviation in defensive ability. Our probability model is:\n\n\\(epa_{ijk} \\sim Normal(\\alpha_{ij} + \\alpha_{ik}, \\sigma)\\)\n\\(\\alpha_{ij} \\sim Normal(0, \\sigma_{off})\\)\n\\(\\alpha_{ik} \\sim Normal(0, \\sigma_{def})\\)\n\nThe stan code for the model is below. You’ll notice that we added some entries in the data block as well as a block called generated quantities. These are going to be used to simulate data from our model parameters so we’ll come back to those shortly.\nWe didn’t discuss priors for the first model but it’s worth doing now. In the code below you can see all of the choices made for priors as well as some restrictions on the data. The only relevant restriction on the data is setting EPA to be bound between -16 and 10. The thought here was that turning a sure-thing safety into a touchdown would be worth, at most, 10 points. Likewise, turning a sure-thing touchdown into a touchdown for the defense would be worth, at most, -16 points. We don’t want to put non-zero probability on values that we know can’t exist in our data and restrictions on parameters are just one way to do this.\nThe thought process for the priors on \\(\\sigma_{off}\\) and \\(\\sigma_{def}\\) was that there is more variation in team offensive ability than team defensive ability which we know from looking at measures like Football Outsiders’ DVOA. The priors are set up such that it would be surprising but not impossible for us to find that there is actually less variance in offensive ability. The values themselves are based on the assumption that teams run around 60 offensive plays per game (in 2018 it was 62.2 and in 2019 it was 62.8). A standard deviation of .06 in offensive EPA/Play would mean that the standard deviation in offensive EPA/Game is about 3.7 points. This would mean that 95% of teams should be within about 7.3 EPA/Game. The .03 standard deviation on the prior means that we think it’s most likely that 95% of teams are within 7.3 EPA/Game in true talent but that 10.9 would perfectly reasonable and that 14.6 is an unlikely but plausible value.\nWe’ll leave it to the reader to determine whether or not these are reasonable priors but it turns out that changing these priors by small amounts doesn’t make a big difference. The real value of setting more informative priors is in eliminating unrealistic parameter values where the variation in abilities is enormous or the variation in one skill is several times larger than the other. Doing so helps the model sample much more efficiently at the small cost of not considering that teams vary by +- 20 EPA/Game.\n\ndata{\n  int<lower = 0> N; //number of plays\n  int<lower = 1> I; //number of teams\n  int<lower = 0, upper = I> ii[N]; //indicator for offense\n  int<lower = 0, upper = I> jj[N]; //indicator for defense\n  real<lower = -16, upper = 10> y[N]; //epa\n  int<lower = 1> N_rep; //number of samples for posterior density check\n  int<lower = 1, upper = I> ii_rep[N_rep];\n  int<lower = 1, upper = I> jj_rep[N_rep];\n}\nparameters{\n  real<lower = 0> sigma_y; //error for t distribution\n  real<lower = 0> sigma_off; //variance in offensive ability\n  real<lower = 0> sigma_def; //variance in defensive ability\n  vector[I] alpha_off_raw;\n  vector[I] alpha_def_raw;\n}\ntransformed parameters{ //using non-centered paramaterization\n  vector[I] alpha_off = alpha_off_raw * sigma_off;\n  vector[I] alpha_def = alpha_def_raw * sigma_def;\n}\nmodel{\n  //priors\n  alpha_off_raw ~ normal(0,1);\n  alpha_def_raw ~ normal(0,1);\n  sigma_off ~ normal(.06,.03);\n  sigma_def ~ normal(.03,.03);\n  sigma_y ~ normal(1,.2);\n  \n  //likelihood\n    y ~ normal(alpha_off[ii] + alpha_def[jj], sigma_y);\n}\ngenerated quantities{\n  vector[N_rep] y_rep;\n  \n  for (n in 1:N_rep){\n    y_rep[n] = normal_rng(alpha_off[ii_rep[n]] + alpha_def[jj_rep[n]], sigma_y);\n  }\n}\n\nWe build the model in exactly the same way but add an indicator for the defense.\n\n\nstanmod_normal <- stan_model('stan-models/epa-per-play-normal.stan')\n\nstandat_normal <- list(N = nrow(post16),\n                I = length(unique(post16$off_id)),\n                ii = post16$off_id,\n                jj = post16$def_id,\n                y = post16$epa,\n                N_rep = nrow(filter(post16, season == 2020)),\n                ii_rep = filter(post16, season == 2020)$off_id,\n                jj_rep = filter(post16, season == 2020)$def_id\n                )\n\nfit_normal <- sampling(stanmod_normal, data = standat_normal, cores = 4, chains = 4, iter = 2000)\n\n\n\nBefore jumping into team estimates we can look at our variance parameters. The model shows that there is likely more spread in offensive ability than defensive ability, though the two are fairly close.\n\nInference for Stan model: epa-per-play-normal.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat\nsigma_off 0.08       0 0.01 0.07 0.07 0.08 0.08  0.09  1752    1\nsigma_def 0.06       0 0.01 0.05 0.06 0.06 0.07  0.08  2034    1\nsigma_y   1.39       0 0.00 1.39 1.39 1.39 1.40  1.40  4297    1\n\nSamples were drawn using NUTS(diag_e) at Tue Jul 06 21:11:02 2021.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\nThe plot below shows that this model is doing more than just pulling teams toward the middle which is good because it’s supposed to be doing more!\n\n\n\nWe can do the same thing for defense, keeping in mind that negative is good for defense. The Rams look like the clear best defense with Detroit and Houston bringing up the rear.\n\n\n\nThe next plot shows how team offensive estimates compare across the two models. Teams above the line are helped by opponent adjustments and teams below the line are hurt by opponent adjustments. The first thing to note is that these differences aren’t huge. While we want to adjust for opponent, doing so doesn’t completely change our understanding of who is and isn’t good. The team whose estimate improves the most is the Giants which makes sense given that they faced numbers 1, 2, 4(2x), 5, 6, 8, 10, 14(2x), 15, 20, 24(2x), 26, and 27 in the model’s estimated defensive ability. The team whose estimate drops the most is the Colts which also makes sense as they share a division with the 31st, 30th, and 28th ranked defenses and also got to face the 32nd, 29th, 27th, 26th, and 25th ranked defenses. Still, the vast majority of teams change very little which is consistent with findings that adding adjustments for defenses faced doesn’t do much to help predict out of sample.\n\n\nepa_actual %>%\n    dplyr::left_join(\n      nflfastR::teams_colors_logos %>% dplyr::select(team_abbr, team_logo_espn),\n      by = c(\"posteam\" = \"team_abbr\")\n    ) %>%\n    dplyr::mutate(\n      grob = purrr::map(seq_along(team_logo_espn), function(x) {\n        grid::rasterGrob(magick::image_read(team_logo_espn[[x]]))\n      })\n    ) %>%\n  filter(season == 2020) %>%\n  ggplot(aes(x = offense_only_estimate, y = offense_normal_estimate)) +\n      ggpmisc::geom_grob(aes(x = offense_only_estimate, y = offense_normal_estimate, label = grob), vp.width = 0.05) +\n    theme_minimal() +\n  geom_smooth(method = 'lm', se = F) +\n  xlab('Offense Only Model') +\n  ylab('Offense + Defense Model') +\n  labs(title = 'Estimated Offensive EPA/Play',\n       subtitle = '')\n\n\n\n\nOne way to evaluate the fit of our model is to ask if the model could plausibly have generated the data that we observe. Above we discussed the generated quantities block of the stan model. In this block we can generate simulated data from our model’s estimated parameters. If these simulated data sets look somewhat like what actually happened that’s a good start to build confidence in our model. We set up our stan data to simulate every play of the 2020 season and we are hoping that the dark line (the actual EPA values) is consistent with the light blue lines (the EPA values simulated from our model).\n\n\nyrep <- rstan::extract(fit_normal, pars = 'y_rep')$y_rep[1:100,]\n\nbayesplot::ppc_dens_overlay(post16$epa[post16$season == 2020], yrep) +\n  labs(title = 'Posterior Predictive Check') +\n  xlim(-16, 10)\n\n\n\n\nYikes! Our model is predicting too few plays near zero, too many in the -4 to -2 and 2 to 4 ranges, and too few on the tails. EPA is weirdly distributed. As we mentioned in the introduction there are a handful of extremely high leverage plays and clearly there are a lot of plays near zero. It also looks like there’s a second mode which is probably due to this being a mixture of distributions with EPA on runs, EPA on incomplete passes, and EPA on completed passes each having their own district shape.\nUsing the Student’s T\nUltimately there isn’t a “right” choice of distribution that will neatly produce what we see in the dark line above. What we’re seeing is probably a mixture of distributions with EPA on runs, EPA on completed passes, and EPA on incompletions all having their own distributions. Still, there are things we can do that are going to fit the data better while being easy to implement. The Student’s t distribution has more density near the mean and at the tails than the normal distribution which looks like what our model needs. Our new model is going to be:\n\n\\(epa_{ijk} \\sim StudentT(\\nu, \\alpha_{ij} + \\alpha_{ik}, \\sigma)\\)\n\\(\\nu = 6\\)\n\nOur new parameter, \\(\\nu\\), is the degrees of freedom parameter which controls how fat the tails will be with lower values corresponding to more extreme values with more density on the mean. We set this to 6 which is small enough to allow for the kinds of values that we actually see in our data without putting significant density on values way above or below what we would expect to find.\nThe stan code to fit the model is basically the same as above but we add an entry to the data block for degrees of freedom and and change the likelihood from normal to student_t.\n\ndata{\n  int<lower = 0> N; //number of plays\n  int<lower = 1> I; //number of teams\n  int<lower = 0, upper = I> ii[N]; //indicator for offense\n  int<lower = 0, upper = I> jj[N]; //indicator for defense\n  int df; //degrees of freedom for t distribution\n  real<lower = -16, upper = 10> y[N]; //epa\n  int<lower = 1> N_rep; //number of samples for posterior density check\n  int<lower = 1, upper = I> ii_rep[N_rep];\n  int<lower = 1, upper = I> jj_rep[N_rep];\n}\nparameters{\n  real<lower = 0> sigma_y; //error for t distribution\n  real<lower = 0> sigma_off; //variance in offensive ability\n  real<lower = 0> sigma_def; //variance in defensive ability\n  vector[I] alpha_off_raw;\n  vector[I] alpha_def_raw;\n}\ntransformed parameters{ //using non-centered paramaterization\n  vector[I] alpha_off = alpha_off_raw * sigma_off;\n  vector[I] alpha_def = alpha_def_raw * sigma_def;\n}\nmodel{\n  //priors\n  alpha_off_raw ~ normal(0,1);\n  alpha_def_raw ~ normal(0,1);\n  sigma_off ~ normal(.06,.03);\n  sigma_def ~ normal(.03,.03);\n  sigma_y ~ normal(1,.2);\n  \n  //likelihood\n    y ~ student_t(df,alpha_off[ii] + alpha_def[jj], sigma_y);\n}\ngenerated quantities{\n  vector[N_rep] y_rep;\n  \n  for (n in 1:N_rep){\n    y_rep[n] = student_t_rng(df, alpha_off[ii_rep[n]] + alpha_def[jj_rep[n]], sigma_y);\n  }\n}\n\nThe only difference in the code to fit the model is that we add a df object to our data.\n\n\nstanmod_t <- stan_model('stan-models/epa-per-play.stan')\n\nstandat_t <- list(N = nrow(post16),\n                I = length(unique(post16$off_id)),\n                ii = post16$off_id,\n                jj = post16$def_id,\n                df = 6,\n                y = post16$epa,\n                N_rep = nrow(filter(post16, season == 2020)),\n                ii_rep = filter(post16, season == 2020)$off_id,\n                jj_rep = filter(post16, season == 2020)$def_id\n                )\n\nfit_t <- sampling(stanmod_t, data = standat_t, cores = 4, chains = 4, iter = 2000)\n\n\n\nOur estimates of the variance in team abilities are both slightly lower but very similar to what we saw in the normal model. They \\(\\sigma_{y}\\) parameter is much smaller but being a different distribution it’s not a 1 to 1 comparison with the same parameter in the normal model.\n\nInference for Stan model: epa-per-play.\n4 chains, each with iter=2000; warmup=1000; thin=1; \npost-warmup draws per chain=1000, total post-warmup draws=4000.\n\n          mean se_mean   sd 2.5%  25%  50%  75% 97.5% n_eff Rhat\nsigma_off 0.07       0 0.01 0.06 0.07 0.07 0.08  0.08  1313    1\nsigma_def 0.05       0 0.01 0.04 0.05 0.05 0.06  0.07  1525    1\nsigma_y   1.02       0 0.00 1.02 1.02 1.02 1.03  1.03  5877    1\n\nSamples were drawn using NUTS(diag_e) at Tue Jul 06 22:17:55 2021.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\nSo is this a better model? Our posterior predictive check certainly looks better. We’re capturing a lot more of the density in the center of the distribution and doing a better job with the tails. It’s still not good but this model is clearly a more plausible candidate to have generated our data than the normal model.\n\n\n\nHere you can see the offensive team ability estimates and now we have some interesting things happening! Buffalo and Kansas City both jump over Green Bay despite Green Bay having a much higher EPA/Play.\n\n\n\nDetroit remains the worst defense but Pittsburgh passes the Rams to become the best defense in the league.\n\n\n\nBelow we can see how estimates compare across the normal and Student’s t models. Generally speaking the estimates are lower, but more importantly we see some big differences in our team estimates. The Bills, Vikings, Patriots, Rams, and Cowboys look a good deal better while the Packers, Steelers, and Chargers look worse.\n\n\nepa_actual %>%\n    dplyr::left_join(\n      nflfastR::teams_colors_logos %>% dplyr::select(team_abbr, team_logo_espn),\n      by = c(\"posteam\" = \"team_abbr\")\n    ) %>%\n    dplyr::mutate(\n      grob = purrr::map(seq_along(team_logo_espn), function(x) {\n        grid::rasterGrob(magick::image_read(team_logo_espn[[x]]))\n      })\n    ) %>%\n  filter(season == 2020) %>%\n  ggplot(aes(x = offense_normal_estimate, y = offense_t_estimate)) +\n      ggpmisc::geom_grob(aes(x = offense_normal_estimate, y = offense_t_estimate, label = grob), vp.width = 0.05) +\n    theme_minimal() +\n  geom_smooth(method = 'lm', se = F) +\n  xlab('Normal Model') +\n  ylab(\"Student's t Model\") +\n  labs(title = 'Estimated Offensive EPA/Play',\n       subtitle = '')\n\n\n\n\nThe same plot for defense shows some big moves. The Steelers, 49ers, Bears, and Eagles improve while the Colts, Dolphins, Bills, and Patriots drop.\n\n\n\nThe obvious question is why certain teams are moving so much. What makes Miami so much worse and Pittsburgh so much better when using the Student’s t? The Student’s t puts more density on extreme values which makes our mean estimates more robust to outliers. This results in teams that get to a good EPA/Play by racking up a few huge plays looking worse than if we use the normal. Likewise, teams whose EPA/Play are dragged down by a few big plays are going to look better. Going back to the example at the start of the paper, this approach would give the Chiefs a lot less credit for that big play than if we use the normal distribution.\nAnother way to look at how this model is arriving at its team strength estimates is to compare Miami and San Francisco who are very close in the normal model and diverge sharply with the Student’s t. The plot below shows the density of defensive EPA for each team. Miami has a good number of plays on the far left of the distribution, which makes sense given all of the pick 6’s they had in the early to middle part of the season, but San Francisco has had better results on plays near the center of the distribution.\n\n\nfilter(post16, season == 2020 & defteam %in% c('MIA','SF')) %>%\n  ggplot(aes(x = epa, colour = defteam)) + \n  scale_colour_manual(values = c('#008e97','#aa0000')) +\n  geom_density() +\n  theme_minimal() +\n  xlab(\"EPA\") +\n  ylab(\"\") +\n  labs(title = \"Defensive EPA, Miami and San Francisco\",\n       colour = 'Team')\n\n\n\n\nPittsburgh and the Rams tell a similar story. Pittsburgh allowed more big plays than the Rams and the Rams forced some very high leverage turnovers but Pittsburgh was better near the center of the distribution.\n\n\n\nWe can do the same for New England and the Steelers on offense. New England has performed better on plays near the bulk of the distribution while having more disastrous plays and fewer big positive plays which I would guess lines up with the experience of most Pats fans.\n\n\n\nConclusions\nThe goal of this post was to introduce how you can use publicly available information to build a model that estimates team abilities while incorporating information about quality of competition. In so doing we found that EPA/Play is an extremely noisy measure that requires a good deal of regularization. Big plays and differences in competition can greatly affect how we view teams so we need to make sure that these are accounted for. This has important implications for what kinds of conclusions we can draw from splits in the data. You can look at EPA/Play against tight ends or EPA/Play weeks 1-3 vs. weeks 4-6 but you should probably shouldn’t read too much into them. When you’re looking at EPA you need to bet heavily on regression to the mean.\nAll that said, this model needs a lot of work! This was a very simple model and there are all kinds of extensions that you could build into this framework to make the model better. You might want to build in information about the quarterback for a given play to account for situations like the Cowboys where you have a very good quarterback for 4.5 games and some less good quarterbacks for the remaining games (just make sure you have a model to estimate QB ability with the appropriate amount of uncertainty!). You could model team abilities as coming from a different distribution. You could build in an autoregressive structure to allow team ability to vary over time or to take advantage of the fact that we can probably learn something about the 2020 Chiefs from the 2019 Chiefs. You could include information about win probability or coaching staffs or weather or betting lines or whatever you can imagine to take advantage of all of the information that you think is important. This post provides a framework and hopefully the community will find it useful and can build from it.\nAppendix: Team Estimates\n\n\nepa_actual %>%\n  dplyr::select(posteam, season, offense_normal_estimate, offense_t_estimate, defense_normal_estimate, defense_t_estimate) %>%\n  mutate_if(is.numeric, round, 2) %>%\n  arrange(desc(offense_t_estimate)) %>%\n  rename('Team' = posteam,\n         'Season' = season,\n         'Offense, Normal' = offense_normal_estimate,\n         'Offense, T' = offense_t_estimate,\n         'Defense, Normal' = defense_normal_estimate,\n         'Defense, T' = defense_t_estimate) %>%\n  datatable(filter = 'top')\n\n\n\n\n\n\n\nView source code on GitHub \n\n\n\n\n\n",
    "preview": "posts/2021-06-27-estimating-team-ability-from-epa/estimating-team-ability-from-epa_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-05-31-computer-vision-in-r-using-torch/",
    "title": "Computer Vision with NFL Player Tracking Data using torch for R",
    "description": "Coverage classification Using CNNs.",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin"
      }
    ],
    "date": "2021-06-07",
    "categories": [
      "Torch",
      "Machine learning",
      "Computer vision"
    ],
    "contents": "\n\nContents\nComputer Vision Background\nThe Goals for this Post\nLoad the Data\nData Wrangling / Create Tensors\nDigression into Wrangling This Data\nFill in the Tensors\nSplit the data\nData Augmentation\nDatasets and Dataloaders\nThe Model\nTraining with Cross Validation\nTesting\nWrapping Up\n\nThe winners of the 2019 and 2020 Big Data Bowls each used Convolutional Neural Networks (CNNs) in their winning entries. This approach essentially treats player tracking data as an image recognition problem and then applies well established computer vision techniques. In this post, we’ll dive into what that actually entails.\nComputer Vision Background\nIf you don’t know anything about computer vision and would like to learn, I personally found this University of Michigan course taught by Justin Johnson incredibly helpful. Here is a link to the lecture videos, here is the syllabus and PDFs of the lecture slides, and see the first link for the homework assignments (which are in PyTorch; i.e. Python). To understand what’s happening in the course, some familiarity with how to manipulate matrices is very useful, especially if you try to tackle the homework assignments.\nAn aside: If you haven’t done anything like this before, a lot of this post will not make a lot of sense. What we will do here is a lot different than learning how to use, for example, xgboost, for a couple reasons. First, we’re departing from the comfort of the “rectangular” data structure where each row is an observation and each column is a feature. In computer vision problems, there are typically (at least) four dimensions: one for each observation, another for each feature dimension (e.g. 3 for a typical red-green-blue image), one for the height of the image, and one for the width. And second, there’s a lot more housekeeping that needs to be done in the code: keeping track of batches, telling it to compute gradients and update weights, and some other stuff along these lines. If none of this makes sense, that’s okay! If you watch the lectures linked above (and even more so, try the homeworks) and come back to this, everything will make a lot more sense.\nIt’s no accident that the course linked above uses PyTorch. Python is the dominant language for computer vision. However, there is recent good news for R users: torch in R has been developed and contains basically all of the same things as the Python version, which means that if you’re already comfortable with R, you don’t need to learn a whole new language just to do computer vision stuff (plus trying to clean and manipulate data in pandas is the worst).\nThe Goals for this Post\nYes: demonstrate how to use torch in R and what this looks like when using NFL player tracking data\nNo: create the most accurate model possible\nSince this is Open Source Football, I’m only going to use things that others can access to: i.e., the tracking data and coverage labels provided through Big Data Bowl 2021. In particular, this means that this post will only be working with coverage labels from week 1 of the 2018 season since that is what was provided to contestants. If you want to replicate this post, you’ll need to get the player tracking data here and the coverage labels here. If one had access to a full season of coverage labels, hypothetically speaking, one could train a much better model. In addition, since this designed to be an introductory post, I’m only going to use one frame per play, which limits the accuracy a bit.\nLet’s get to it!\nLoad the Data\nI’m going to mostly skip over the data cleaning stuff this since it isn’t the focus of the post, plus I wrote a package that gets all of the annoying data prep out of the way. If you’re interested, you can check out the code in the package, and maybe by the time you’re reading this I’ll have even documented the functions (ha ha). The function below takes week 1 of the 2021 Big Data Bowl, makes all plays go from left to right, adds some columns like the extent to which each defender’s orientation points him at the quarterback, and gets some information about each play (e.g. whether each player is on offense or defense and the location of the line of scrimmage).\n\n\nlibrary(tidyverse)\nlibrary(torch)\nlibrary(patchwork)\nlibrary(gt)\n\ndf <- ngscleanR::prepare_bdb_week(\n  week = 1,\n  dir = \"../../../nfl-big-data-bowl-2021/input\",\n  # any throw that happens before 1.5 seconds after snap is thrown away\n  trim_frame = 25,\n  # all frames coming more than 1 second after pass released are thrown away\n  frames_after_throw = 10,\n  # let's keep this frame for fun (1.8 seconds after snap)\n  keep_frames = c(31)\n)\n\n\n\nNow let’s get the labels from 2018 week 1 with thanks to Telemetry.\n\n\nlabels <- readr::read_csv(\"../../../nfl-big-data-bowl-2021/input/coverages_week1.csv\") %>%\n  mutate(\n    play = paste0(gameId, \"_\", playId)\n  ) %>%\n  filter(!is.na(coverage)) %>%\n  select(play, coverage)\n\ndf <- df %>%\n  inner_join(labels, by = \"play\")\n\n# check labels\nlabels %>%\n  group_by(coverage) %>%\n  summarize(n = n()) %>%\n  ngscleanR:::make_table()\n\n\ncoverage\n      n\n    Cover 0 Man\n      13\n    Cover 1 Man\n      296\n    Cover 2 Man\n      32\n    Cover 2 Zone\n      113\n    Cover 3 Zone\n      352\n    Cover 4 Zone\n      152\n    Cover 6 Zone\n      69\n    Prevent Zone\n      1\n    \n\nData Wrangling / Create Tensors\nFor reference, here’s what the location and orientation columns in Big Data Bowl mean:\nBig Data BowlAnd we want to create something along the lines of The Zoo’s solution but modified to predict coverages rather than the result of a rushing play. In particular, this section is creating the blue part of this on the left:\nThe Zoo’s winning 2020 Big Data Bowl EntryIn The Zoo’s entry, they had 10 feature dimensions ((a) defender position, (b) defender position relative to the rusher, (c) defender speed relative to the rusher, (d) defender position relative to offensive player, and (e) defender speed relative to offensive player, all in both the X and Y directions). But their problem was different because they were trying to predict the results of a run play. For coverage classification, things like speed relative to the rusher don’t make sense. There is no rusher! For this post, I’ve settled on 13 features, but haven’t done a lot of playing around with other features so there might be better ways to do this. Here are the features we’ll create:\n1: Distance from line of scrimmage (this serves as X location)\n2: Y location\n3 and 4: Speed in X and Y directions\n5 and 6: Acceleration in X and Y directions\n7: Orientation towards quarterback (measured 0 to 1, with 0 meaning facing directly at QB, 1 directly away)\n8 and 9: X and Y distance from each offensive player\n10 and 11: X and Y speed relative to each offensive player\n12 and 13: X and Y acceleration relative to each offensive player\nBecause the maximum number of non-QB offensive players provided in Big Data Bowl is 5 and defensive players 11, we will be creating a tensor – basically a higher-dimensional matrix used by torch – of size (number of plays) x (13) x (11) x (5), with 13 being the number of features (listed above), 11 the number of defenders, and 5 offensive players. Each of the 13 features will have a 11 x 5 matrix that gives a value for each defensive player relative to each offensive player. For example, the entry (1, 13, 1, 1) would pull out the difference in acceleration in the Y direction (i.e., the 13th feature) between the 1st defensive player and 1st offensive player on the 1st play, and the entry (1, 13, ..) would give an 11 by 5 matrix of the relative speed of each possible combination of defensive player and offensive player. If you’re wondering how to choose who the “1st defensive player” is, the beauty of The Zoo’s solution is that you can put the players in any order and the model will treat them the same; i.e., the order doesn’t matter.\nSince we need to get a data frame of defenders relative to offensive players, let’s do that using the pre-cleaned data by joining an offense dataframe to a defense dataframe by play, which joins each offensive player to each possible defensive player:\n\n\noffense_df <- df %>%\n  filter(defense == 0) %>%\n  select(play, frame_id, o_x = x, o_y = y, o_s_x = s_x, o_s_y = s_y, o_a_x = a_x, o_a_y = a_y)\n\ndefense_df <- df %>%\n  filter(defense == 1) %>%\n  select(play, frame_id, nfl_id, x, y, s_x, s_y, a_x, a_y, o_to_qb, dist_from_los)\n\nrel_df <- defense_df %>%\n  # since there are duplicates at each play & frame this creates 1 entry per offense-defense player combination\n  left_join(offense_df, by = c(\"play\", \"frame_id\")) %>%\n  mutate(diff_x = o_x - x, diff_y = o_y - y, diff_s_x = o_s_x - s_x, diff_s_y = o_s_y - s_y, diff_a_x = o_a_x - a_x, diff_a_y = o_a_y - a_y) %>%\n  select(play, frame_id, nfl_id, dist_from_los, y, s_x, s_y, a_x, a_y, o_to_qb, starts_with(\"diff_\"))\n\nrel_df %>%\n  select(nfl_id, dist_from_los, y, diff_x, diff_y) %>%\n  head(10) %>%\n  ngscleanR:::make_table()\n\n\nnfl_id\n      dist_from_los\n      y\n      diff_x\n      diff_y\n    2539334\n      4.03\n      15.46333\n      -0.37\n      -1.47\n    2539334\n      4.03\n      15.46333\n      -0.88\n      16.43\n    2539334\n      4.03\n      15.46333\n      -8.70\n      15.81\n    2539334\n      4.03\n      15.46333\n      0.93\n      22.14\n    2539334\n      4.03\n      15.46333\n      -6.18\n      6.53\n    2539653\n      5.28\n      38.06333\n      -1.62\n      -24.07\n    2539653\n      5.28\n      38.06333\n      -2.13\n      -6.17\n    2539653\n      5.28\n      38.06333\n      -9.95\n      -6.79\n    2539653\n      5.28\n      38.06333\n      -0.32\n      -0.46\n    2539653\n      5.28\n      38.06333\n      -7.43\n      -16.07\n    \n\nThe first 5 entries here represent the first defensive player matched to each of the 5 offensive players. Note that a lot of the columns are constant for the first 5 rows because each row represents a defense-offense match and many entries do not depend on the offensive player. For example, the defender’s distance from the line of scrimmage is the same for each matched offensive player (in The Zoo’s entry, this is what they mean by some features being “constant across ‘off’ dimension of the tensor”).\nFor some housekeeping, let’s save a master list of plays to refer to later.\n\n\nplay_indices <- df %>%\n  select(play, frame_id, play, week, coverage) %>%\n  unique() %>%\n  # get play index for 1 : n_plays\n  mutate(\n    i = as.integer(as.factor(play))\n  ) %>%\n  # get time step indices\n  # this is useless for this post bc we're only using one frame\n  # but useful when extending to more frames\n  group_by(play) %>%\n  mutate(f = 1:n()) %>%\n  ungroup()\n\nn_frames <- n_distinct(play_indices$f)\nn_plays <- n_distinct(play_indices$i)\nn_class <- n_distinct(play_indices$coverage)\n\ndef_only_features <- 7\noff_def_features <- 6\nn_features <- def_only_features + off_def_features\n\nplay_indices %>%\n  head(10) %>%\n  ngscleanR:::make_table()\n\n\nplay\n      frame_id\n      week\n      coverage\n      i\n      f\n    2018090600_1037\n      31\n      1\n      Cover 2 Man\n      1\n      1\n    2018090600_1061\n      31\n      1\n      Cover 1 Man\n      2\n      1\n    2018090600_1085\n      31\n      1\n      Cover 3 Zone\n      3\n      1\n    2018090600_1202\n      31\n      1\n      Cover 3 Zone\n      4\n      1\n    2018090600_1226\n      31\n      1\n      Cover 1 Man\n      5\n      1\n    2018090600_1295\n      31\n      1\n      Cover 1 Man\n      6\n      1\n    2018090600_1344\n      31\n      1\n      Cover 4 Zone\n      7\n      1\n    2018090600_1423\n      31\n      1\n      Cover 3 Zone\n      8\n      1\n    2018090600_146\n      31\n      1\n      Cover 3 Zone\n      9\n      1\n    2018090600_1473\n      31\n      1\n      Cover 3 Zone\n      10\n      1\n    \n\nNow we’re ready to start making our tensor, which is what torch knows how to deal with. Note that I’m throwing in an extra dimension for number of frames on each play, which is useless in this case (1 frame per play) but makes it easier to extend to using multiple frames in a given play (the real reason I’m doing this is that my existing code uses more frames and it’s easier to not re-write this part of it).\n\n\ndata_tensor <- torch_empty(n_plays, n_frames, n_features, 11, 5)\ndim(data_tensor)\n\n\n#> [1] 1018    1   13   11    5\n\nHere is the function that will help us populate our tensor, explained below.\n\n\nfill_row <- function(row) {\n\n  # indices for putting in tensor\n  i <- row$i # row\n  f <- row$f # frame\n\n  # play info for extracting from df\n  playid <- row$play\n  frameid <- row$frame_id\n\n  play_df <- rel_df %>%\n    filter(play == playid, frame_id == frameid) %>%\n    select(-play, -frame_id)\n\n  # how many defense and offense players are there on this play?\n  defenders <- n_distinct(play_df$nfl_id)\n  n_offense <- nrow(play_df) / defenders\n\n  # get rid of player ID since it's not in the model\n  play_df <- play_df %>% select(-nfl_id)\n\n  # where the magic happens\n  # explanation of this part below!\n  data_tensor[i, f, , 1:defenders, 1:n_offense] <-\n    torch_tensor(t(play_df))$view(c(-1, defenders, n_offense))\n}\n\n\n\nDigression into Wrangling This Data\nIf you haven’t worked with higher-dimensional data before, it can be kind of hard to wrap your head around. Let’s illustrate what is happening in the function above using the first play as an example. Here is what the raw data look like when running this function on the first play (showing the first 10 rows):\n\n\nrow <- play_indices %>% dplyr::slice(1)\n\ni <- row$i\nf <- row$f\n\nplayid <- row$play\nframeid <- row$frame_id\n\nplay_df <- rel_df %>%\n  filter(play == playid, frame_id == frameid) %>%\n  select(-play, -frame_id)\n\ndefenders <- n_distinct(play_df$nfl_id)\nn_offense <- nrow(play_df) / defenders\n\nplay_df <- play_df %>% select(-nfl_id)\n\ndefenders\n\n\n#> [1] 7\n\ndim(play_df)\n\n\n#> [1] 35 13\n\n\nplay_df %>%\n  head(10) %>%\n  ngscleanR:::make_table()\n\n\ndist_from_los\n      y\n      s_x\n      s_y\n      a_x\n      a_y\n      o_to_qb\n      diff_x\n      diff_y\n      diff_s_x\n      diff_s_y\n      diff_a_x\n      diff_a_y\n    4.03\n      15.46333\n      5.270580\n      -3.648162\n      2.442063\n      -1.6903340\n      0.9869417\n      -0.37\n      -1.47\n      0.6550943\n      -0.02161781\n      0.2784763\n      0.005499684\n    4.03\n      15.46333\n      5.270580\n      -3.648162\n      2.442063\n      -1.6903340\n      0.9869417\n      -0.88\n      16.43\n      -0.4223971\n      -0.15193297\n      -0.5216787\n      0.185101602\n    4.03\n      15.46333\n      5.270580\n      -3.648162\n      2.442063\n      -1.6903340\n      0.9869417\n      -8.70\n      15.81\n      -5.7954932\n      2.80877747\n      -3.7516945\n      -0.403887787\n    4.03\n      15.46333\n      5.270580\n      -3.648162\n      2.442063\n      -1.6903340\n      0.9869417\n      0.93\n      22.14\n      0.2337943\n      1.72702611\n      -0.6765088\n      1.074120621\n    4.03\n      15.46333\n      5.270580\n      -3.648162\n      2.442063\n      -1.6903340\n      0.9869417\n      -6.18\n      6.53\n      -4.6102605\n      -1.42907927\n      -1.9377953\n      -2.187012303\n    5.28\n      38.06333\n      4.597714\n      -1.605593\n      2.039232\n      -0.7121318\n      0.2054857\n      -1.62\n      -24.07\n      1.3279608\n      -2.06418628\n      0.6813069\n      -0.972702512\n    5.28\n      38.06333\n      4.597714\n      -1.605593\n      2.039232\n      -0.7121318\n      0.2054857\n      -2.13\n      -6.17\n      0.2504695\n      -2.19450144\n      -0.1188481\n      -0.793100594\n    5.28\n      38.06333\n      4.597714\n      -1.605593\n      2.039232\n      -0.7121318\n      0.2054857\n      -9.95\n      -6.79\n      -5.1226266\n      0.76620901\n      -3.3488640\n      -1.382089984\n    5.28\n      38.06333\n      4.597714\n      -1.605593\n      2.039232\n      -0.7121318\n      0.2054857\n      -0.32\n      -0.46\n      0.9066608\n      -0.31554235\n      -0.2736782\n      0.095918425\n    5.28\n      38.06333\n      4.597714\n      -1.605593\n      2.039232\n      -0.7121318\n      0.2054857\n      -7.43\n      -16.07\n      -3.9373939\n      -3.47164774\n      -1.5349648\n      -3.165214499\n    \n\nWe need to have this thing be 13 x 7 x 5 (7 defenders and 5 offensive players on this play), but right now it is (5 x 7) x 13 with each group of 5 rows a set of rows for each defensive player, and the features going across as columns rather than as rows like we need for the tensor shape.\nThe first step is to transpose (showing first 8 columns so it’s not huge):\n\n\n# showing first 8 columns so this isn't huge\nt(play_df)[, 1:8]\n\n\n#>                       [,1]       [,2]       [,3]       [,4]\n#> dist_from_los  4.030000000  4.0300000  4.0300000  4.0300000\n#> y             15.463333333 15.4633333 15.4633333 15.4633333\n#> s_x            5.270580108  5.2705801  5.2705801  5.2705801\n#> s_y           -3.648161911 -3.6481619 -3.6481619 -3.6481619\n#> a_x            2.442062858  2.4420629  2.4420629  2.4420629\n#> a_y           -1.690333990 -1.6903340 -1.6903340 -1.6903340\n#> o_to_qb        0.986941709  0.9869417  0.9869417  0.9869417\n#> diff_x        -0.370000000 -0.8800000 -8.7000000  0.9300000\n#> diff_y        -1.470000000 16.4300000 15.8100000 22.1400000\n#> diff_s_x       0.655094267 -0.4223971 -5.7954932  0.2337943\n#> diff_s_y      -0.021617811 -0.1519330  2.8087775  1.7270261\n#> diff_a_x       0.278476310 -0.5216787 -3.7516945 -0.6765088\n#> diff_a_y       0.005499684  0.1851016 -0.4038878  1.0741206\n#>                     [,5]        [,6]       [,7]       [,8]\n#> dist_from_los  4.0300000   5.2800000  5.2800000  5.2800000\n#> y             15.4633333  38.0633333 38.0633333 38.0633333\n#> s_x            5.2705801   4.5977135  4.5977135  4.5977135\n#> s_y           -3.6481619  -1.6055934 -1.6055934 -1.6055934\n#> a_x            2.4420629   2.0392323  2.0392323  2.0392323\n#> a_y           -1.6903340  -0.7121318 -0.7121318 -0.7121318\n#> o_to_qb        0.9869417   0.2054857  0.2054857  0.2054857\n#> diff_x        -6.1800000  -1.6200000 -2.1300000 -9.9500000\n#> diff_y         6.5300000 -24.0700000 -6.1700000 -6.7900000\n#> diff_s_x      -4.6102605   1.3279608  0.2504695 -5.1226266\n#> diff_s_y      -1.4290793  -2.0641863 -2.1945014  0.7662090\n#> diff_a_x      -1.9377953   0.6813069 -0.1188481 -3.3488640\n#> diff_a_y      -2.1870123  -0.9727025 -0.7931006 -1.3820900\n\ndim(t(play_df))\n\n\n#> [1] 13 35\n\nNow we’re closer, with the data shaped 13 x (5 X 7). That is, there are now 13 rows with each row a feature like we want, but columns 1-5 represent the first defender, 6-10 the second, etc (I’ve only shown the first 10 columns above so the output isn’t huge on the page). We need to take each of these and split up the defenders into different dimensions so that we end up with 13 sets of 7 x 5 matrices.\nThe below is the complete line for getting to the right shape (if you find this part hard to follow, don’t feel like it’s supposed to be easy; this took me a lot of trial and error while working with little toy examples).\n\n\ntorch_tensor(t(play_df))$view(c(-1, defenders, n_offense))\n\n\n#> torch_tensor\n#> (1,.,.) = \n#>    4.0300   4.0300   4.0300   4.0300   4.0300\n#>    5.2800   5.2800   5.2800   5.2800   5.2800\n#>   19.3800  19.3800  19.3800  19.3800  19.3800\n#>   -4.5100  -4.5100  -4.5100  -4.5100  -4.5100\n#>    3.3900   3.3900   3.3900   3.3900   3.3900\n#>   14.2700  14.2700  14.2700  14.2700  14.2700\n#>    1.7400   1.7400   1.7400   1.7400   1.7400\n#> \n#> (2,.,.) = \n#>   15.4633  15.4633  15.4633  15.4633  15.4633\n#>   38.0633  38.0633  38.0633  38.0633  38.0633\n#>   25.5533  25.5533  25.5533  25.5533  25.5533\n#>   30.8233  30.8233  30.8233  30.8233  30.8233\n#>   32.8033  32.8033  32.8033  32.8033  32.8033\n#>   31.6433  31.6433  31.6433  31.6433  31.6433\n#>   23.6133  23.6133  23.6133  23.6133  23.6133\n#> \n#> (3,.,.) = \n#>   5.2706  5.2706  5.2706  5.2706  5.2706\n#>   4.5977  4.5977  4.5977  4.5977  4.5977\n#>   2.5835  2.5835  2.5835  2.5835  2.5835\n#>  -3.1190 -3.1190 -3.1190 -3.1190 -3.1190\n#>   4.2293  4.2293  4.2293  4.2293  4.2293\n#>   5.3371  5.3371  5.3371  5.3371  5.3371\n#>   0.1803  0.1803  0.1803  0.1803  0.1803\n#> \n#> (4,.,.) = \n#>  -3.6482 -3.6482 -3.6482 -3.6482 -3.6482\n#>  -1.6056 -1.6056 -1.6056 -1.6056 -1.6056\n#> ... [the output was truncated (use n=-1 to disable)]\n#> [ CPUFloatType{13,7,5} ]\n\nHooray! It looks like it should. We have a 13x7x5 tensor with each of the 13 feature dimensions carrying a 7x5 matrix that matches each of the 7 defensive players to each of the 5 offensive players. Note that we’re starting to use torch stuff for the first time, where view is a function that reshapes tensors that will be familiar to anyone who has used PyTorch.\nVery helpful reference page for tensor operations\nHopefully this sheds some light on the above function. Now let’s use it.\nFill in the Tensors\nDigression over and back to work. This iterates over every play to fill in the tensor for each play.\n\n\n# build the tensor for train and test data\nwalk(1:nrow(play_indices), ~ {\n  if (.x %% 250 == 0) {\n    message(glue::glue(\"{.x} of {nrow(play_indices)}\"))\n  }\n  fill_row(play_indices %>% dplyr::slice(.x))\n})\n\n\n\nLet’s make sure it worked:\n\n\ndata_tensor[1, ..]\n\n\n#> torch_tensor\n#> (1,1,.,.) = \n#>    4.0300   4.0300   4.0300   4.0300   4.0300\n#>    5.2800   5.2800   5.2800   5.2800   5.2800\n#>   19.3800  19.3800  19.3800  19.3800  19.3800\n#>   -4.5100  -4.5100  -4.5100  -4.5100  -4.5100\n#>    3.3900   3.3900   3.3900   3.3900   3.3900\n#>   14.2700  14.2700  14.2700  14.2700  14.2700\n#>    1.7400   1.7400   1.7400   1.7400   1.7400\n#>    0.0000   0.0000   0.0000   0.0000   0.0000\n#>    0.0000   0.0000   0.0000   0.0000   0.0000\n#>    0.0000   0.0000   0.0000   0.0000   0.0000\n#>    0.0000   0.0000   0.0000   0.0000   0.0000\n#> \n#> (1,2,.,.) = \n#>   15.4633  15.4633  15.4633  15.4633  15.4633\n#>   38.0633  38.0633  38.0633  38.0633  38.0633\n#>   25.5533  25.5533  25.5533  25.5533  25.5533\n#>   30.8233  30.8233  30.8233  30.8233  30.8233\n#>   32.8033  32.8033  32.8033  32.8033  32.8033\n#>   31.6433  31.6433  31.6433  31.6433  31.6433\n#>   23.6133  23.6133  23.6133  23.6133  23.6133\n#>    0.0000   0.0000   0.0000   0.0000   0.0000\n#>    0.0000   0.0000   0.0000   0.0000   0.0000\n#>    0.0000   0.0000   0.0000   0.0000   0.0000\n#>    0.0000   0.0000   0.0000   0.0000   0.0000\n#> \n#> (1,3,.,.) = \n#>   5.2706  5.2706  5.2706  5.2706  5.2706\n#>   4.5977  4.5977  4.5977  4.5977  4.5977\n#>   2.5835  2.5835  2.5835  2.5835  2.5835\n#> ... [the output was truncated (use n=-1 to disable)]\n#> [ CPUFloatType{1,13,11,5} ]\n\nThis shows that the first play has been filled in. The extra zeroes in the final rows are because we initialized a tensor of zeroes for 11 defenders, but not all defenders are provided on every play, so a lot of plays will have zeroes like this. I think about it like a bunch of players standing in the corner of the field together not impacting the play since that’s basically what the computer sees (I tried sticking all the “missing” players on the line of scrimmage standing there looking at each other, but it didn’t seem to make much of a difference).\nNow let’s fill in the coverage labels. Note that the torch_long() part is required for labels and the labels have to be integers starting from 1.\n\n\nlabel_tensor <- torch_zeros(n_plays, dtype = torch_long())\n\nlabel_tensor[1:n_plays] <- play_indices %>%\n  mutate(coverage = as.factor(coverage) %>% as.integer()) %>%\n  pull(coverage)\n\n\n\n\n\n\nFinally, I need to get rid of the time dimension since I’m not using it here (torch_squeeze gets rid of any singleton dimensions).\n\n\ndata_tensor <- torch_squeeze(data_tensor)\ndim(data_tensor)\n\n\n#> [1] 1018   13   11    5\n\ndim(label_tensor)\n\n\n#> [1] 1018\n\nNow we have about 1,000 plays from week 1 shaped in the way we want.\nSplit the data\nLet’s hold out 200 plays for testing and split the remaining sample into 5 sets of 80% training and 20% validation splits.\n\n\ntest_size <- 200\nset.seed(2013) # gohawks\n\n# hold out\ntest_id <- sample(1:n_plays, size = test_size)\ntest_data <- data_tensor[test_id, ..]\ntest_label <- label_tensor[test_id]\n\n# full training set\ntrain_id <- setdiff(1:n_plays, test_id)\ntrain_data <- data_tensor[train_id, ..]\ntrain_label <- label_tensor[train_id]\n\n# helper thing that is just 1, ..., length train data\nall_train_idx <- 1:dim(train_data)[1]\n\n# create folds from the train indices\n# stratified by label\nfolds <- splitTools::create_folds(\n  y = as.integer(train_label),\n  k = 5,\n  type = \"stratified\",\n  invert = TRUE\n)\n\ndim(train_data)\n\n\n#> [1] 818  13  11   5\n\ndim(train_label)\n\n\n#> [1] 818\n\nstr(folds)\n\n\n#> List of 5\n#>  $ Fold1: int [1:164] 2 6 10 14 17 24 29 43 53 66 ...\n#>  $ Fold2: int [1:163] 5 11 15 18 21 23 34 35 44 48 ...\n#>  $ Fold3: int [1:162] 1 7 8 12 13 16 20 31 40 41 ...\n#>  $ Fold4: int [1:164] 4 9 19 26 28 32 33 37 42 52 ...\n#>  $ Fold5: int [1:165] 3 22 25 27 30 36 38 39 45 46 ...\n\nThe folds object created above is a list of 5 where each item in the list is the set of validation indices for a given fold.\nData Augmentation\nFrom The Zoo’s entry:\n\nWhat worked really well for us is to add augmentation and TTA for Y coordinates. We assume that in a mirrored world the runs would have had the same outcomes. For training, we apply 50% augmentation to flip the Y coordinates (and all respective relative features emerging from it)\n\nSo we need a function that flips any features related to Y vertically. That is below.\n\n\naugment_data <- function(df,\n                         # stuff that will be multiplied by -1 (eg Sy)\n                         flip_indices = c(4, 6, 9, 11, 13),\n                         # raw y location\n                         subtract_indices = c(2)) {\n\n\n  # indices of the elements that need to be flipped\n  t <- torch_ones_like(df)\n  t[, flip_indices, , ] <- -1\n\n  # first fix: multiply by -1 where needed (stuff like speed in Y direction)\n  flipped <- df * t\n\n  # for flipping Y itself, need to do 160/3 - y\n  t <- torch_zeros_like(df)\n  t[, subtract_indices, , ] <- 160 / 3\n\n  # second fix: flip around y\n  flipped[, subtract_indices, , ] <- t[, subtract_indices, , ] - flipped[, subtract_indices, , ]\n\n  return(flipped)\n}\n\n\n\nThis will be used below during training (when augmenting the data by adding flipped data) and testing (when getting the prediction as an average of the flipped and non-flipped data).\nDatasets and Dataloaders\nOkay, so we have tensors, and now we need to turn them into something that can be used in training. For this, we need torch’s dataset(), which is way to easily fetch of observations at once. This is mostly copy and pasting from the below link and not that interesting.\nUseful reference page on datasets and dataloaders.\n\n\n# define dataset\ntracking_dataset <- dataset(\n  name = \"tracking_dataset\",\n  initialize = function(x_tensor, y_tensor) {\n    self$data_x <- x_tensor\n    self$data_y <- y_tensor\n  },\n  .getitem = function(i) {\n    list(\"x\" = self$data_x[i, ], \"y\" = self$data_y[i])\n  },\n  .length = function() {\n    self$data_y$size()[[1]]\n  }\n)\n\n\n\nAnother possible workflow is to put all the data cleaning and preparation work done above into the dataset() function, which would probably make for a simpler predict stage when putting something like this into production, but I haven’t done this yet.\nNow let’s stick our data in the dataset:\n\n\ntrain_ds <- tracking_dataset(train_data, train_label)\n\n\n\nAlright, so what did that actually do? Now we can access the .getitem() and .length() things created in the dataset() definition above, which would show the first item in the training dataset (if we ran train_ds$.getitem(1)), or the total length of the dataset (train_ds$.length()).\nYou might have thought we’re done now, but not quite. Now we need to send our dataset to a dataloader .\n\n\n# Dataloaders\ntrain_dl <- train_ds %>%\n  dataloader(batch_size = 64, shuffle = TRUE)\n\n\n\nThe dataloaders allow for torch to access the data in batches, which as we’ll see below, is how the model is trained.\nThis shows how many batches we have:\n\n\ntrain_dl$.length()\n\n\n#> [1] 13\n\nAnd this shows one of the batches, and we can refer to the data and labels as x and y because of how we constructed the dataset above:\n\n\nbatch <- train_dl$.iter()$.next()\ndim(batch$x)\n\n\n#> [1] 64 13 11  5\n\ndim(batch$y)\n\n\n#> [1] 64\n\nBoth the data and labels have 64 rows as expected (the batch size).\nThe Model\nThis is a straight copy of The Zoo’s model so there’s not much to say here, but I will leave some comments to explain what is happening.\n\n\nnet <- nn_module(\n  \"Net\",\n  initialize = function() {\n    self$conv_block_1 <- nn_sequential(\n      nn_conv2d(\n        # 1x1 convolution taking in 13 (n_features) channels and outputting 128\n        # before: batch * 13 * 11 * 5\n        # after: batch * 128 * 11 * 5\n        in_channels = n_features,\n        out_channels = 128,\n        kernel_size = 1\n      ),\n      nn_relu(inplace = TRUE),\n      nn_conv2d(\n        in_channels = 128,\n        out_channels = 160,\n        kernel_size = 1\n      ),\n      nn_relu(inplace = TRUE),\n      nn_conv2d(\n        in_channels = 160,\n        out_channels = 128,\n        kernel_size = 1\n      ),\n      nn_relu(inplace = TRUE),\n    )\n\n    self$conv_block_2 <- nn_sequential(\n      nn_batch_norm1d(128),\n      nn_conv1d(\n        in_channels = 128,\n        out_channels = 160,\n        kernel_size = 1\n      ),\n      nn_relu(inplace = TRUE),\n      nn_batch_norm1d(160),\n      nn_conv1d(\n        in_channels = 160,\n        out_channels = 96,\n        kernel_size = 1\n      ),\n      nn_relu(inplace = TRUE),\n      nn_batch_norm1d(96),\n      nn_conv1d(\n        in_channels = 96,\n        out_channels = 96,\n        kernel_size = 1\n      ),\n      nn_relu(inplace = TRUE),\n      nn_batch_norm1d(96)\n    )\n\n    self$linear_block <- nn_sequential(\n      nn_linear(96, 96),\n      nn_relu(inplace = TRUE),\n      nn_batch_norm1d(96),\n      nn_linear(96, 256),\n      nn_relu(inplace = TRUE),\n\n      # note: breaks on current kaggle version\n      nn_batch_norm1d(256),\n      nn_layer_norm(256),\n      nn_dropout(p = 0.3),\n\n      # n_class is how many distinct labels there are\n      nn_linear(256, n_class)\n    )\n  },\n  forward = function(x) {\n\n    # first conv layer\n    # outputs batch * 128 * 11 * 5\n    x <- self$conv_block_1(x)\n\n    # first pool layer: average of mean and max pooling\n    # the 5 is number of offensive players\n    avg <- nn_avg_pool2d(kernel_size = c(1, 5))(x) %>%\n      torch_squeeze(-1)\n    max <- nn_max_pool2d(kernel_size = c(1, 5))(x) %>%\n      torch_squeeze(-1)\n\n    x <- 0.7 * avg + 0.3 * max\n\n    # x is now batch * 128 * 11\n\n    # second conv layer\n    x <- self$conv_block_2(x)\n\n    # second pool layer\n    avg <- nn_avg_pool1d(kernel_size = 11)(x) %>%\n      torch_squeeze(-1)\n    max <- nn_max_pool1d(kernel_size = 11)(x) %>%\n      torch_squeeze(-1)\n\n    x <- 0.7 * avg + 0.3 * max\n\n    # x is now batch * 96\n\n    x <- self$linear_block(x)\n\n    # x is now batch * # labels\n\n    x\n  }\n)\n\n\n\nTraining with Cross Validation\nWe’ve done a lot of setup to get to this point, and now we can actually train a model!\nI’m going to use what is known as k-fold validation (with k = 5 in this case). For each of the 5 folds, we estimate a separate model using 80% of the data as the training set and the remaining 20% as the validation set. This helps give a more realistic expectation of what to expect from final testing than using one fold, especially here since our data are so limited. Looking at the below, the accuracy among the 5 folds ranges from 73% - 82%, with an average of 78%.\nAt the predict stage to follow, we could either average over these 5 models (this is what I do) or re-train on the entire train set. Reading through the link above, it seems like the choice of which to do is not super obvious (and might not matter that much).\nTorch reference page explaining optimizer, loss$backward(), model$train(), model$eval(), optimizer$step(), etc:\nHere’s the big loop:\n\n\nset.seed(2013)\ntorch_manual_seed(2013)\n\naccuracies <- torch_zeros(length(folds))\nbest_epochs <- torch_zeros(length(folds))\n\nepochs <- 50\n\n# start iteration over folds\nfor (fold in 1:length(folds)) {\n  cat(sprintf(\"\\n------------- FOLD %d ---------\", fold))\n\n  model <- net()\n  optimizer <- optim_adam(model$parameters, lr = 0.001)\n  scheduler <- lr_step(optimizer, step_size = 1, 0.975)\n\n  # extract train and validation sets\n  val_i <- folds[[fold]]\n  train_i <- all_train_idx[-val_i]\n\n  .ds_train <- dataset_subset(train_ds, train_i)\n  .ds_val <- dataset_subset(train_ds, val_i)\n\n  .train_dl <- .ds_train %>%\n    dataloader(batch_size = 64, shuffle = TRUE)\n  .valid_dl <- .ds_val %>%\n    dataloader(batch_size = 64, shuffle = TRUE)\n\n  for (epoch in 1:epochs) {\n    train_losses <- c()\n    valid_losses <- c()\n    valid_accuracies <- c()\n\n    # train step: loop over batches\n    model$train()\n    for (b in enumerate(.train_dl)) {\n\n      # augment first\n      b_augmented <- augment_data(b$x)\n      x <- torch_cat(list(b$x, b_augmented))\n      # double the label list\n      y <- torch_cat(list(b$y, b$y))\n\n      optimizer$zero_grad()\n      loss <- nnf_cross_entropy(model(x), y)\n      loss$backward()\n      optimizer$step()\n      train_losses <- c(train_losses, loss$item())\n    }\n\n    # validation step: loop over batches\n    model$eval()\n    for (b in enumerate(.valid_dl)) {\n      output <- model(b$x)\n\n      # augment\n      valid_data_augmented <- augment_data(b$x)\n      output_augmented <- model(valid_data_augmented)\n      output <- (output + output_augmented) / 2\n\n      valid_losses <- c(valid_losses, nnf_cross_entropy(output, b$y)$item())\n\n      pred <- torch_max(output, dim = 2)[[2]]\n      correct <- (pred == b$y)$sum()$item()\n      valid_accuracies <- c(valid_accuracies, correct / length(b$y))\n    }\n\n    scheduler$step()\n\n    if (epoch %% 10 == 0) {\n      cat(sprintf(\"\\nLoss at epoch %d: training: %1.4f, validation: %1.4f // validation accuracy %1.4f\", epoch, mean(train_losses), mean(valid_losses), mean(valid_accuracies)))\n    }\n\n    if (mean(valid_accuracies) > as.numeric(accuracies[fold])) {\n      message(glue::glue(\"Fold {fold}: New best at epoch {epoch} ({round(mean(valid_accuracies), 3)}). Saving model\"))\n\n      torch_save(model, glue::glue(\"best_model_{fold}.pt\"))\n\n      # save new best loss\n      accuracies[fold] <- mean(valid_accuracies)\n      best_epochs[fold] <- epoch\n    }\n  }\n}\n\n\n#> \n#> ------------- FOLD 1 ---------\n#> Loss at epoch 10: training: 0.5016, validation: 0.8785 // validation accuracy 0.6956\n#> Loss at epoch 20: training: 0.3003, validation: 0.8284 // validation accuracy 0.7425\n#> Loss at epoch 30: training: 0.2528, validation: 1.0263 // validation accuracy 0.7367\n#> Loss at epoch 40: training: 0.0898, validation: 1.0639 // validation accuracy 0.7436\n#> Loss at epoch 50: training: 0.0738, validation: 1.1549 // validation accuracy 0.7407\n#> ------------- FOLD 2 ---------\n#> Loss at epoch 10: training: 0.6065, validation: 0.7676 // validation accuracy 0.7216\n#> Loss at epoch 20: training: 0.2548, validation: 0.8657 // validation accuracy 0.7112\n#> Loss at epoch 30: training: 0.1632, validation: 1.0050 // validation accuracy 0.6896\n#> Loss at epoch 40: training: 0.1457, validation: 1.0056 // validation accuracy 0.6966\n#> Loss at epoch 50: training: 0.0670, validation: 1.0053 // validation accuracy 0.7372\n#> ------------- FOLD 3 ---------\n#> Loss at epoch 10: training: 0.5593, validation: 1.2702 // validation accuracy 0.6238\n#> Loss at epoch 20: training: 0.2961, validation: 0.7408 // validation accuracy 0.7381\n#> Loss at epoch 30: training: 0.1939, validation: 0.9673 // validation accuracy 0.7086\n#> Loss at epoch 40: training: 0.1380, validation: 0.7870 // validation accuracy 0.7307\n#> Loss at epoch 50: training: 0.0719, validation: 1.0105 // validation accuracy 0.7439\n#> ------------- FOLD 4 ---------\n#> Loss at epoch 10: training: 0.5566, validation: 0.8309 // validation accuracy 0.6939\n#> Loss at epoch 20: training: 0.3361, validation: 1.0483 // validation accuracy 0.6487\n#> Loss at epoch 30: training: 0.2409, validation: 1.0290 // validation accuracy 0.6667\n#> Loss at epoch 40: training: 0.1300, validation: 1.1014 // validation accuracy 0.6817\n#> Loss at epoch 50: training: 0.1303, validation: 1.0972 // validation accuracy 0.7043\n#> ------------- FOLD 5 ---------\n#> Loss at epoch 10: training: 0.5872, validation: 0.7824 // validation accuracy 0.7494\n#> Loss at epoch 20: training: 0.3780, validation: 0.7256 // validation accuracy 0.7603\n#> Loss at epoch 30: training: 0.1491, validation: 0.7148 // validation accuracy 0.7807\n#> Loss at epoch 40: training: 0.1557, validation: 0.8051 // validation accuracy 0.7845\n#> Loss at epoch 50: training: 0.0439, validation: 0.8338 // validation accuracy 0.8001\n\n\naccuracies\n\n\n#> torch_tensor\n#>  0.7807\n#>  0.7632\n#>  0.7947\n#>  0.7303\n#>  0.8167\n#> [ CPUFloatType{5} ]\n\n\nmean(accuracies)\n\n\n#> torch_tensor\n#> 0.777139\n#> [ CPUFloatType{} ]\n\nAn epoch is one full time through the training data. For each batch that makes up the full data, the model takes the batch and uses it to update the model. I have chosen 50 epochs because the best model seems to have emerged by then, and the best model of the 50 epochs is saved. This process happens 5 times: once for each cross-validation fold.\nTesting\nNow we can test the model on the 200 plays that we held out earlier. We’ll take the average of the 5 models generated above (one for each cross-validation fold) and in addition, for each model, have the prediction be the average of the actual and flipped (across Y direction) prediction, as in The Zoo’s entry (this is what they refer to as TTA, or Test Time Augmentation).\n\n\n# get the labels\nlabels <- test_label %>%\n  as.matrix() %>%\n  as_tibble() %>%\n  set_names(\"label\")\n\n# load all the models\nmodels <- map(1:length(folds), ~ {\n  torch_load(glue::glue(\"best_model_{.x}.pt\"))\n})\n\n# augment test data\ntest_data_augmented <- augment_data(test_data)\n\n# initialize empty output\noutput <- torch_zeros(length(folds), dim(test_data)[1], n_class)\n\n# get augmented prediction for each fold\nwalk(1:length(folds), ~ {\n  output[.x, ..] <- (models[[.x]](test_data) + models[[.x]](test_data_augmented)) / 2\n})\n\n# average prediction over folds\npredictions <- (1 / length(folds)) * torch_sum(output, 1) %>%\n  as.matrix()\n\n# join prediction to label\npredictions <- predictions %>%\n  as_tibble() %>%\n  mutate(row = 1:n()) %>%\n  transform(prediction = max.col(predictions)) %>%\n  bind_cols(labels) %>%\n  mutate(correct = ifelse(prediction == label, 1, 0)) %>%\n  as_tibble() %>%\n  mutate(\n    label = as.factor(label),\n    prediction = as.factor(prediction)\n  )\n\n# the magic correct number\ncat(sprintf(\"Week 1 test: %1.0f percent correct\", round(100 * mean(predictions$correct), 1), mean(train_losses), mean(valid_losses), mean(valid_accuracies)))\n\n\n#> Week 1 test: 78 percent correct\n\nSo we hit 78% accuracy, which was about the mean of the cross validation accuracies. Good! This 78% is an under-estimate of the true accuracy because some plays are mislabeled (this also puts an upper limit on how good the model can be, although only having 1 week of data is the bigger hurdle for this post).\nLet’s take a look at which types of plays the model is having problems with:\n\n\n# confusion matrix\ntab <- predictions %>%\n  mutate(\n    label = as.factor(as.integer(label)),\n    prediction = as.factor(as.integer(prediction))\n  )\n\nlevels(tab$label) <-\n  c(\"C0m\", \"C1m\", \"C2m\", \"C2z\", \"C3z\", \"C4z\", \"C6z\")\nlevels(tab$prediction) <-\n  c(\"C0m\", \"C1m\", \"C2m\", \"C2z\", \"C3z\", \"C4z\", \"C6z\")\n\nconf_mat <- caret::confusionMatrix(tab$prediction, tab$label)\nconf_mat$table %>%\n  broom::tidy() %>%\n  dplyr::rename(\n    Target = Reference,\n    N = n\n  ) %>%\n  cvms::plot_confusion_matrix(\n    add_sums = TRUE, place_x_axis_above = FALSE,\n    add_normalized = FALSE\n  )\n\n\n\n\nUnsurprisingly, there biggest source of confusion is distinguishing Cover 1 Man from Cover 3 Zone. Let’s take a look at some play that are labeled as Cover 1 Man but are really Cover 3 Zone (with thanks to the great package sportyR for making it easy to plot a field).\n\n\ntracking <- readr::read_csv(\"../../../nfl-big-data-bowl-2021/input/week1.csv\") %>%\n  ngscleanR::clean_and_rotate() %>%\n  filter(frame_id == 31)\n\nplot_plays <- play_indices[test_id, ] %>%\n  bind_cols(predictions) %>%\n  filter(prediction == 2, label == 5) %>%\n  select(play) %>%\n  dplyr::slice(1:4) %>%\n  pull(play)\n\n\n\n\n\nplots <- map(plot_plays, ~ {\n  plot <- tracking %>%\n    filter(play == .x) %>%\n    ngscleanR::plot_play(\n      animated = FALSE,\n      segment_length = 6,\n      segment_size = 3,\n      dot_size = 4\n    )\n\n  plot +\n    theme(\n      plot.title = element_blank(),\n      plot.caption = element_blank(),\n      plot.margin = unit(c(0, 0, 0, 0), \"cm\")\n    )\n})\n\ncombined <- (plots[[1]] + plots[[2]]) / (plots[[3]] + plots[[4]])\n\ncombined + plot_annotation(\n  title = \"Sample Cover 3 plays that model thinks are Cover 1\",\n  theme = theme(plot.title = element_text(size = 16, hjust = 0.5))\n)\n\n\n\n\nIt’s kind of hard to tell from a still image exactly why the model got them wrong. The bottom right one, in particular, should be easy for a model to tell that it’s Cover 3, since so many defenders are watching the quarterback. However, the glass half full view is that it’s impressive that the model can get nearly 80% of plays right with only 1 week of data and 1 frame per play.\nWrapping Up\nHopefully this has been at least somewhat helpful to someone. We’ve covered how to get data into tensors, deal with datasets and dataloaders, augmentation, and how to train a model with k-fold cross-validation.\nOne thing I didn’t cover is how to deal with multiple frames per play. One option would be to feed the CNN predictions in each frame into a LSTM. Another would be to send every frame in a given play into the CNN and then perform some sort of pooling by play afterwards. I didn’t explore this in the post because (a) it’s already long enough and (b) my laptop can barely handle knitting this post at its given size and more frames would kill it.\nSome things that I learned:\nIf you get inexplicable error messages, make very sure that you don’t have any missing data anywhere (including in the labels)\nBe very careful with using torch_squeeze() without any arguments (i.e., the index to squeeze) because if you happen to have a batch size of 1 at the end of an epoch, the batch size dimension will get squeezed out and break things\nKaggle supports R and comes with torch pre-installed, BUT it’s a very old version of torch so upgrade to the latest version in your kaggle notebook to get access to all the new things\nIf you have limited memory or don’t have access to a GPU on your own computer, use Kaggle to train models!\nThe documentation is very helpful!\nThis post wouldn’t have been possible without a lot of help from a lot of people. Thank you in particular to Daniel Falbel, both for all of his work on torch and for answering a million of my questions, and to Sean Clement and Lau Sze Yui for allowing me to pester them with a million questions about neural nets. Additional thanks to helpful discussions with Timo Riske, Suvansh Sanjeev, Udit Ranasaria, Rishav Dutta, and Zach Feldman.\n\n\n\n\nView source code on GitHub \n\n\n\n\n",
    "preview": "posts/2021-05-31-computer-vision-in-r-using-torch/computer-vision-in-r-using-torch_files/figure-html5/unnamed-chunk-24-1.png",
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {},
    "preview_width": 7200,
    "preview_height": 4000
  },
  {
    "path": "posts/2021-04-13-creating-a-model-from-scratch-using-xgboost-in-r/",
    "title": "NFL win probability from scratch using xgboost in R",
    "description": "xgboost and R",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin"
      }
    ],
    "date": "2021-04-17",
    "categories": [
      "xgboost",
      "nflfastR"
    ],
    "contents": "\n\nContents\nIntro\nBoring preliminary stuff\nGet the data\nMake some splits\nTuning\nCollect best parameters\nTrain the model\nPrediction\nModel evaluation\nWrapping up\n\n\n\n\nIntro\nI get a lot of questions about win probability / expected points models and xgboost. As demonstrated here, tree-based models like xgboost can offer an improvement over simpler methods such as logistic regression. This post is designed to show how to tune and train a win probability model.\nBefore we go on, a couple links:\nIntroduction to boosted trees\nWhat the xgboost parameters do\nIn general, the xgboost documentation is very, very good so I recommend checking it out if you plan to use it. Parts of this post are drawn from the extremely helplful Richard Anderson OSF post, which in turn is based on this excellent post from Julia Silge.\nThis overview skips over some important steps like exploratory data analysis and feature engineering. This also doesn’t cover handling NAs (which I just drop), categorical variables, and probably a lot of other stuff. All of those are important steps, but covering them would probably worth a separate post on its own. For this one, we’re taking a starting point of knowing which features will be included and wanting to properly tune and train a model.\nBoring preliminary stuff\nLet’s load the necessary packages and set some variables needed later.\n\n\n# packages\nlibrary(nflfastR)\nlibrary(splitTools)\nlibrary(dials)\nlibrary(xgboost)\nlibrary(ggplot2)\nlibrary(tidyverse)\n\nset.seed(2013) # gohawks\n\n# this should be the default u y do this R\noptions(scipen = 999999)\n\n# size of hyperparameter grid to search over\n# if you don't have a potato computer, can set this to a bigger number\ngrid_size <- 40\n\n\n\nGet the data\nFirst let’s load the data from nflfastR, label it (1 = win, 0 = loss), and keep the necessary columns.\n\n\nfuture::plan(\"multisession\")\npbp_data <- nflfastR::load_pbp(2001:2020) %>%\n  dplyr::mutate(\n    # label data with whether possession team ended up winning\n    # note that NA result and ties dealt with later\n    label = dplyr::case_when(\n      result > 0 & posteam == home_team ~ 1,\n      result < 0 & posteam == away_team ~ 1,\n      TRUE ~ 0\n    ),\n    # create home indicator used in model\n    home = ifelse(posteam == home_team, 1, 0)\n  ) %>%\n  # creates Diff_Time_Ratio and spread_time\n  nflfastR:::prepare_wp_data() %>%\n  # don't deal with NA, just drop\n  dplyr::filter(\n    !is.na(down),\n    !is.na(game_seconds_remaining),\n    !is.na(yardline_100),\n    !is.na(score_differential),\n    # overtime is hard\n    qtr <= 4,\n    !is.na(result),\n    !is.na(posteam),\n    # throw out ties\n    result != 0\n  ) %>%\n  dplyr::select(\n    # label and identifying info\n    label,\n    game_id,\n    season,\n\n    # features\n    receive_2h_ko,\n    spread_time,\n    home,\n    half_seconds_remaining,\n    game_seconds_remaining,\n    Diff_Time_Ratio,\n    score_differential,\n    down,\n    ydstogo,\n    yardline_100,\n    posteam_timeouts_remaining,\n    defteam_timeouts_remaining\n  )\n\n\n\nMake some splits\nLet’s hold out 2019 and 2020 for the final test and chop the remainder into folds for cross validation. This is somewhat different than a typical tutorial because we can’t just take random samples of the data since that would split up plays from a given game across the training and test sets, which would be bad (more on this below).\n\n\ntest_data <- pbp_data %>%\n  dplyr::filter(season >= 2019)\n\ntrain_data <- pbp_data %>%\n  dplyr::filter(season < 2019)\n\n# explanation of this step below\nfolds <- splitTools::create_folds(\n  y = train_data$game_id,\n  k = 5,\n  type = \"grouped\",\n  invert = TRUE\n)\n\ntrain_labels <- train_data %>%\n  dplyr::select(label)\n\n# get rid of extra columns\ntrain_data <- train_data %>%\n  dplyr::select(-season, -game_id, -label)\n\nstr(folds)\n\n\nList of 5\n $ Fold1: int [1:142027] 141 142 143 144 145 146 147 148 149 150 ...\n $ Fold2: int [1:141614] 1891 1892 1893 1894 1895 1896 1897 1898 1899 1900 ...\n $ Fold3: int [1:141321] 1 2 3 4 5 6 7 8 9 10 ...\n $ Fold4: int [1:141883] 579 580 581 582 583 584 585 586 587 588 ...\n $ Fold5: int [1:141960] 718 719 720 721 722 723 724 725 726 727 ...\n\nAbove, we create the folds object that will be passed to xgb.cv later. From the xgboost documentation: “folds (list) provides a possibility to use a list of pre-defined CV folds (each element must be a vector of test fold’s indices).”\nWe are thus using splitTools to create such folds. From the splitTools docs, what the above is doing:\ny: “Either the variable used for stratification or grouped splits.” Here, we’re going to “group” based on game_id, which means that “groups specified by y are kept together when splitting” as wanted.\nk: number of folds. Typically 5 or 10\ntype: As noted above, “grouped” means that rows with the same game_id are kept together when splitting\ninvert = TRUE: Docs: “Set to TRUE if the row numbers not in the fold are to be returned”. This is confusingly worded, but basically this means that when TRUE, the hold-out indices for a given fold are returned (rather than the train indices). Because xgboost requires test fold indices (see bolded part above), we want this to be the case.\nThe very important note here is that since labels are shared across observations – i.e., for a given team in a given game, each of their rows will be labeled either 1 or 0 since they either win or they don’t – we must be very careful to make sure that when a game is in a validation fold, there are no plays from that game in other folds. This is what the “grouped” part does above. If we failed to do this, we would experience what is referred to as “leakage” across the train and validation sets, and would encounter a very bad surprise when trying to predict out of sample because the model would overfit badly.\nAn alternative to what I’m doing here would be to create folds based on seasons, instead of games. This is what I did for nflfastR since I hadn’t discovered how to easily split based on games (thanks to Andrew Patton for the tip), but it shouldn’t make much of a difference regardless. Finally, if you’re modeling something without interdependence across observations (e.g., an expected completion model where each observation is a one-off), you can skip manually creating folds and just let xgb.cv handle the folds by using the nfold parameter.\nTuning\nLet’s use dials to create a grid of hyperparameters to search over. This is one thing from tidymodels that I have found very useful, though note that it has different names for the hyperparameters than xgboost so there’s some renaming things at the end.\n\n\ngrid <- dials::grid_latin_hypercube(\n  # this finalize thing is because mtry depends on # of columns in data\n  dials::finalize(dials::mtry(), train_data),\n  dials::min_n(),\n  dials::tree_depth(),\n  # to force learn_rate to not be crazy small like dials defaults to\n  # because my computer is slow\n  # if you're trying this for a different problem, expand the range here\n  # by using more negative values\n  dials::learn_rate(range = c(-1.5, -0.5), trans = scales::log10_trans()),\n  dials::loss_reduction(),\n  sample_size = dials::sample_prop(),\n  size = grid_size\n) %>%\n  dplyr::mutate(\n    # has to be between 0 and 1 for xgb\n    # for some reason mtry gives the number of columns rather than proportion\n    mtry = mtry / length(train_data),\n    # see note below\n    monotone_constraints = \"(0, 0, 0, 0, 0, 1, 1, -1, -1, -1, 1, -1)\"\n\n    # for the monotone constraints\n    # these are notes to myself to make sure the constraints are in the right order\n    # the order of the constraints needs to match up with the columns in the df\n\n    # receive_2h_ko, 0\n    # spread_time, 0\n    # home, 0\n\n    # half_seconds_remaining, 0\n    # game_seconds_remaining, 0\n    # Diff_Time_Ratio, 1\n\n    # score_differential, 1\n    # down, -1\n    # ydstogo, -1\n\n    # yardline_100, -1\n    # posteam_timeouts_remaining, 1\n    # defteam_timeouts_remaining, -1\n  ) %>%\n  # make these the right names for xgb\n  dplyr::rename(\n    eta = learn_rate,\n    gamma = loss_reduction,\n    subsample = sample_size,\n    colsample_bytree = mtry,\n    max_depth = tree_depth,\n    min_child_weight = min_n\n  )\n\ngrid\n\n\n# A tibble: 40 x 7\n   colsample_bytree min_child_weight max_depth    eta    gamma\n              <dbl>            <int>     <int>  <dbl>    <dbl>\n 1           0.583                33         5 0.0600 1.91e+ 1\n 2           0.167                26        12 0.237  5.98e- 7\n 3           0.417                36        13 0.155  6.46e- 1\n 4           0.5                  20        12 0.0925 7.95e- 8\n 5           0.833                34         3 0.167  2.55e- 9\n 6           0.333                31         4 0.254  7.40e+ 0\n 7           0.0833                6         3 0.186  2.21e- 4\n 8           0.25                 26         8 0.228  3.22e- 5\n 9           0.0833               19         2 0.0383 2.23e-10\n10           0.667                38        11 0.0783 5.85e-10\n# ... with 30 more rows, and 2 more variables: subsample <dbl>,\n#   monotone_constraints <chr>\n\nA lot of notes here:\nSee the docs for what monotone constraints do. In short, we want to impose constraints on certain columns such that larger (or smaller) values for those columns can only increase (or decrease) the prediction. For example, all else equal, we want a team’s win probability to be higher if they are ahead by more points; or stated another way, we want it to be impossible for increasing the size of a team’s lead to result in a decrease in win probability\nThe size parameter tells it how many rows you want to fill out to search over. We defined grid_size above (normally I would just put the size directly into here but I have it defined at the top to make testing the knitting of this file quicker)\nI do some renaming so that the things are called what xgboost expects\nAnd now we need a function to take a row from our grid and trains the model using those hyperparameters using xgb.cv. This is the sort of thing that tidymodels would handle but I find it easier to just write a function myself. For example, passing monotone constraints to xgboost using tidymodels requires writing a custom function anyway.\n\n\n# function to perform xgb.cv for a given row in a hyperparameter grid\nget_row <- function(row) {\n  params <-\n    list(\n      booster = \"gbtree\",\n      objective = \"binary:logistic\",\n      eval_metric = c(\"logloss\"),\n      eta = row$eta,\n      gamma = row$gamma,\n      subsample = row$subsample,\n      colsample_bytree = row$colsample_bytree,\n      max_depth = row$max_depth,\n      min_child_weight = row$min_child_weight,\n      monotone_constraints = row$monotone_constraints\n    )\n\n  # do the cross validation\n  wp_cv_model <- xgboost::xgb.cv(\n    data = as.matrix(train_data),\n    label = train_labels$label,\n    params = params,\n    # this doesn't matter with early stopping in xgb.cv, just set a big number\n    # the actual optimal rounds will be found in this tuning process\n    nrounds = 15000,\n    # created above\n    folds = folds,\n    metrics = list(\"logloss\"),\n    early_stopping_rounds = 50,\n    print_every_n = 50\n  )\n\n  # bundle up the results together for returning\n  output <- params\n  output$iter <- wp_cv_model$best_iteration\n  output$logloss <- wp_cv_model$evaluation_log[output$iter]$test_logloss_mean\n\n  row_result <- bind_rows(output)\n\n  return(row_result)\n}\n\n\n\nNow we’re ready to run the function above on each row of our hyperparameter grid.\n\n\n# get results\nresults <- purrr::map_df(1:nrow(grid), function(x) {\n  get_row(grid %>% dplyr::slice(x))\n})\n\n\n\nLet’s take a look at what we’ve got with thanks to the code from Julia Silge’s post mentioned above:\n\n\nresults %>%\n  dplyr::select(logloss, eta, gamma, subsample, colsample_bytree, max_depth, min_child_weight) %>%\n  tidyr::pivot_longer(\n    eta:min_child_weight,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, logloss, color = parameter)) +\n  geom_point(alpha = 0.8, show.legend = FALSE, size = 3) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"logloss\") +\n  theme_minimal()\n\n\n\n\nLooking at the results, it appears that we want max_depth somewhere in the 4-8 range, colsample_bytree definitely greater than 1/4, and learn_rate no greater than 0.10. So let’s hone in on that and re-tune. This is partially done to show an example of giving dials some ranges to work with rather than letting it pull numbers from anywhere:\n\n\ngrid <- dials::grid_latin_hypercube(\n  # don't need the finalize business since we're using length in here\n  dials::mtry(range = c(length(train_data) / 4, length(train_data))),\n  dials::min_n(),\n  # force tree depth to be between 3 and 5\n  dials::tree_depth(range = c(4L, 8L)),\n  # to force learn_rate to not be crazy small like dials defaults to\n  dials::learn_rate(range = c(-1.5, -1), trans = scales::log10_trans()),\n  dials::loss_reduction(),\n  sample_size = dials::sample_prop(),\n  size = grid_size\n) %>%\n  dplyr::mutate(\n    # has to be between 0 and 1 for xgb\n    # for some reason mtry gives the number of columns rather than proportion\n    mtry = mtry / length(train_data),\n    monotone_constraints = \"(0, 0, 0, 0, 0, 1, 1, -1, -1, -1, 1, -1)\"\n  ) %>%\n  # make these the right names for xgb\n  dplyr::rename(\n    eta = learn_rate,\n    gamma = loss_reduction,\n    subsample = sample_size,\n    colsample_bytree = mtry,\n    max_depth = tree_depth,\n    min_child_weight = min_n\n  )\n\ngrid\n\n\n# A tibble: 40 x 7\n   colsample_bytree min_child_weight max_depth    eta   gamma\n              <dbl>            <int>     <int>  <dbl>   <dbl>\n 1            0.917                8         8 0.0426 5.30e-3\n 2            0.75                18         6 0.0897 8.48e-3\n 3            0.417               27         6 0.0973 2.38e-4\n 4            0.5                 38         5 0.0604 1.05e+1\n 5            0.75                31         6 0.0337 1.79e-2\n 6            0.75                39         7 0.0674 1.45e+0\n 7            0.25                 8         8 0.0495 1.13e-5\n 8            0.417                3         4 0.0459 1.36e-9\n 9            0.417               10         7 0.0820 6.86e-5\n10            0.667               19         6 0.0562 2.72e-6\n# ... with 30 more rows, and 2 more variables: subsample <dbl>,\n#   monotone_constraints <chr>\n\nAnd now we can re-run our function with the new grid:\n\n\n# get results\nresults <- purrr::map_df(1:nrow(grid), function(x) {\n  get_row(grid %>% dplyr::slice(x))\n})\n\n\n\nAnd plot it again:\n\n\nresults %>%\n  dplyr::select(logloss, eta, gamma, subsample, colsample_bytree, max_depth, min_child_weight) %>%\n  tidyr::pivot_longer(eta:min_child_weight,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, logloss, color = parameter)) +\n  geom_point(alpha = 0.8, show.legend = FALSE, size = 3) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"logloss\") +\n  theme_minimal()\n\n\n\n\nOne thing I’m curious is whether we can add a monotone constraint on spread_time, which is the time-decaying value for point spread from the perspective of the possession team. To do this, let’s modify the tuning grid with a different monotone_constraints, re-tune, and see how the best value compares. Again, this might be something that would be simpler to implement in tidymodels but I haven’t figured out how to use it well.\n\n\ngrid2 <- grid %>%\n  dplyr::mutate(\n    # old\n    # monotone_constraints = \"(0, 0, 0, 0, 0, 1, 1, -1, -1, -1, 1, -1)\"\n\n    # new\n    monotone_constraints = \"(0, 1, 0, 0, 0, 1, 1, -1, -1, -1, 1, -1)\"\n  )\n\nresults2 <- purrr::map_df(1:nrow(grid2), function(x) {\n  get_row(grid2 %>% dplyr::slice(x))\n})\n\n\n\n\n\nglue::glue(\n  \"--BEST LOGLOSS--\n\nNo monotone constraint on spread_time:\n{round(results %>% arrange(logloss) %>% dplyr::slice(1) %>% pull(logloss), 5)}\n\nMonotone constraint on spread_time:\n{round(results2 %>% arrange(logloss) %>% dplyr::slice(1) %>% pull(logloss), 5)}\"\n)\n\n\n--BEST LOGLOSS--\n\nNo monotone constraint on spread_time:\n0.44787\n\nMonotone constraint on spread_time:\n0.44826\n\nIt doesn’t seem to make much of a difference but since it should be monotonic (teams favored by more should have higher win prob, and the strength of point spread should decrease as the game goes on), let’s use that model. I find it a bit puzzling that adding the constraint doesn’t actually help the logloss, but oh well.\n\n\nresults2 %>%\n  dplyr::arrange(logloss) %>%\n  dplyr::select(eta, subsample, colsample_bytree, max_depth, logloss, min_child_weight, iter)\n\n\n# A tibble: 40 x 7\n      eta subsample colsample_bytree max_depth logloss\n    <dbl>     <dbl>            <dbl>     <int>   <dbl>\n 1 0.0334     0.528            0.5           7   0.448\n 2 0.0604     0.476            0.5           5   0.448\n 3 0.0543     0.837            0.583         8   0.448\n 4 0.0587     0.801            0.583         7   0.448\n 5 0.0718     0.452            0.5           6   0.448\n 6 0.0765     0.789            0.5           8   0.448\n 7 0.0378     0.751            0.75          7   0.448\n 8 0.0918     0.404            0.667         7   0.449\n 9 0.0319     0.708            0.75          6   0.449\n10 0.0562     0.689            0.667         6   0.449\n# ... with 30 more rows, and 2 more variables:\n#   min_child_weight <int>, iter <int>\n\nbest_model <- results2 %>%\n  dplyr::arrange(logloss) %>%\n  dplyr::slice(1)\n\n\n\nCollect best parameters\nNow we’ve finally picked everything that we need to train the model with. We can set our params with the results from best_model, including how many rounds to train for.\n\n\nparams <-\n  list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    eval_metric = c(\"logloss\"),\n    eta = best_model$eta,\n    gamma = best_model$gamma,\n    subsample = best_model$subsample,\n    colsample_bytree = best_model$colsample_bytree,\n    max_depth = best_model$max_depth,\n    min_child_weight = best_model$min_child_weight,\n    monotone_constraints = best_model$monotone_constraints\n  )\n\nnrounds <- best_model$iter\n\n\n\n\n\nparams\n\n\n$booster\n[1] \"gbtree\"\n\n$objective\n[1] \"binary:logistic\"\n\n$eval_metric\n[1] \"logloss\"\n\n$eta\n[1] 0.03336242\n\n$gamma\n[1] 0.001755908\n\n$subsample\n[1] 0.5283268\n\n$colsample_bytree\n[1] 0.5\n\n$max_depth\n[1] 7\n\n$min_child_weight\n[1] 4\n\n$monotone_constraints\n[1] \"(0, 1, 0, 0, 0, 1, 1, -1, -1, -1, 1, -1)\"\n\nglue::glue(\"nrounds: {nrounds}\")\n\n\nnrounds: 588\n\nTrain the model\nAnd finally train using xgboost and the best parameters saved above:\n\n\nwp_model <- xgboost::xgboost(\n  params = params,\n  data = as.matrix(train_data),\n  label = train_labels$label,\n  nrounds = nrounds,\n  verbose = 2\n)\n\n\n\nLet’s check the variable importance.\n\n\nimportance <- xgboost::xgb.importance(\n  feature_names = colnames(wp_model),\n  model = wp_model\n)\nxgboost::xgb.ggplot.importance(importance_matrix = importance)\n\n\n\n\nUnsurprisingly, score differential and the ratio of score differential to time left are the most important features. In addition, pregame point spread matters a lot.\nA couple other notes:\nIf you want to plot the trees, see this post here for example\nIf you want to get the actual trees as data, look up xgb.dump\nPrediction\nNow that we have our model, we can predict with the hold out set, which in this case is the 2019 and 2020 seasons. There might be a better way to do this, but I just join the predictions to the test dataframe.\n\n\npreds <- stats::predict(\n  wp_model,\n  # get rid of the things not needed for prediction here\n  as.matrix(test_data %>% select(-label, -game_id, -season))\n) %>%\n  tibble::as_tibble() %>%\n  dplyr::rename(wp = value) %>%\n  dplyr::bind_cols(test_data)\n\npreds\n\n\n# A tibble: 79,054 x 16\n      wp label game_id         season receive_2h_ko spread_time  home\n   <dbl> <dbl> <chr>            <int>         <dbl>       <dbl> <dbl>\n 1 0.328     0 2019_01_ATL_MIN   2019             0       -3.5      0\n 2 0.260     0 2019_01_ATL_MIN   2019             0       -3.35     0\n 3 0.260     0 2019_01_ATL_MIN   2019             0       -3.21     0\n 4 0.323     0 2019_01_ATL_MIN   2019             0       -3.06     0\n 5 0.793     1 2019_01_ATL_MIN   2019             1        3.04     1\n 6 0.731     1 2019_01_ATL_MIN   2019             1        2.97     1\n 7 0.756     1 2019_01_ATL_MIN   2019             1        2.86     1\n 8 0.191     0 2019_01_ATL_MIN   2019             0       -2.82     0\n 9 0.201     0 2019_01_ATL_MIN   2019             0       -2.69     0\n10 0.184     0 2019_01_ATL_MIN   2019             0       -2.57     0\n# ... with 79,044 more rows, and 9 more variables:\n#   half_seconds_remaining <dbl>, game_seconds_remaining <dbl>,\n#   Diff_Time_Ratio <dbl>, score_differential <dbl>, down <dbl>,\n#   ydstogo <dbl>, yardline_100 <dbl>,\n#   posteam_timeouts_remaining <dbl>,\n#   defteam_timeouts_remaining <dbl>\n\nFor fun, let’s see what the model thinks Green Bay’s win probability was after they elected to kick a field goal at the end of the 2020 NFC Championship Game.\n\n\npreds %>%\n  dplyr::filter(\n    game_id == \"2020_20_TB_GB\",\n    between(game_seconds_remaining, 120, 140)\n  ) %>%\n  dplyr::select(wp, label, game_seconds_remaining, score_differential)\n\n\n# A tibble: 4 x 4\n     wp label game_seconds_remaining score_differential\n  <dbl> <dbl>                  <dbl>              <dbl>\n1 0.213     0                    139                 -8\n2 0.185     0                    135                 -8\n3 0.114     0                    129                 -8\n4 0.910     1                    122                  5\n\nSo the Bucs had an estimated win probability of about 90% to start their next possession (final row).\nModel evaluation\nCalculating logloss on the 2019 and 2020 test data:\n\n\nMLmetrics::LogLoss(preds$wp, preds$label)\n\n\n[1] 0.4270486\n\nThis is better than the logloss in tuning with xgb.cv. I don’t think this is normal but it’s possible there are some data errors in older seasons that make predictions easier in 2019 and 2020. This raises the question of whether the model would be better if I didn’t use some of the oldest seasons when training, but this thing already takes too long to run on my computer.\nIf we wanted the test error instead of test logloss:\n\n\nMLmetrics::Accuracy(\n  # say a team is predicted to win if they have win prob > .5\n  preds %>%\n    dplyr::mutate(pred = ifelse(wp > .5, 1, 0)) %>%\n    dplyr::pull(pred),\n  # compare to whether they actually won\n  preds$label\n)\n\n\n[1] 0.792724\n\nSo about 80 percent of the time, the team expected to win on a given play ends up winning.\nFinally, let’s make sure a calibration plot looks okay, with this plot inspired by the Yurko et al. paper:\n\n\nplot <- preds %>%\n  # Create BINS for wp:\n  dplyr::mutate(bin_pred_prob = round(wp / 0.05) * .05) %>%\n  dplyr::group_by(bin_pred_prob) %>%\n  # Calculate the calibration results:\n  dplyr::summarize(\n    n_plays = n(),\n    n_wins = length(which(label == 1)),\n    bin_actual_prob = n_wins / n_plays\n  ) %>%\n  dplyr::ungroup()\n\nann_text <- data.frame(\n  x = c(.25, 0.75), y = c(0.75, 0.25),\n  lab = c(\"More times\\nthan expected\", \"Fewer times\\nthan expected\")\n)\n\n\n\n\n\nplot %>%\n  ggplot() +\n  geom_point(aes(x = bin_pred_prob, y = bin_actual_prob, size = n_plays)) +\n  geom_smooth(aes(x = bin_pred_prob, y = bin_actual_prob), method = \"loess\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", lty = 2) +\n  coord_equal() +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    size = \"Number of plays\",\n    x = \"Estimated win probability\",\n    y = \"Observed win probability\",\n    title = \"Win prob calibration plot\"\n  ) +\n  geom_text(data = ann_text, aes(x = x, y = y, label = lab), size = 2) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_text(size = 10, angle = 90),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    legend.position = \"bottom\"\n  )\n\n\n\n\nLooks pretty good, and remember that this is entirely out of sample!\nWrapping up\nHopefully this has given you the tools to apply xgboost to your own problems. I’ve found xgboost to be very powerful for a variety of applications, but not all (some examples of it failing are modeling overtime win probability and whether a team will go for it on 4th down).\nThanks for reading! If you have questions or suggestions, the best place to discuss this post is in the nflfastR discord.\n\n\n\nView source code on GitHub \n\n\n\n\n\n",
    "preview": "posts/2021-04-13-creating-a-model-from-scratch-using-xgboost-in-r/creating-a-model-from-scratch-using-xgboost-in-r_files/figure-html5/importance-1.png",
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2100
  },
  {
    "path": "posts/2020-09-28-nflfastr-ep-wp-and-cp-models/",
    "title": "nflfastR EP, WP, CP xYAC, and xPass models",
    "description": "A description of the nflfastR Expected Points (EP), Win Probability (WP), Completion Probability (CP) Expected Yards after Catch (xYAC), and Expected Pass (xPass) models.",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin"
      }
    ],
    "date": "2021-02-05",
    "categories": [
      "Figures",
      "nflfastR",
      "Decision Trees",
      "xgboost",
      "Model Calibration"
    ],
    "contents": "\n\nContents\nAbout\nModel features\nEP model features\nWP model features\nCP and expected yards after the catch model features\nExpected dropback model features\n\nEP Model Calibration Results\nWP Model Calibration Results\nWP Model Calibration Results: with point spread\nCP Model Calibration Results\nxYAC Model Calibration Results\nExpected Pass Model Calibration Results\n\n\n\n\nAbout\nThis page describes the nflfastR models before showing that they are well calibrated using the procedure introduced by Yurko, Ventura, and Horowitz. Because the 2020 season will mark 22 seasons of nflfastR data, the main purpose behind creating new models for EP and WP was to build in era adjustments to fascilitate better cross-era comparisons. However, we also discovered that switching to tree-based methods could improve model calibration, especially for end-of-half situations with complicated nonlinear interactions between variables. Because we are introducing new models, we compare our calibration results to nflscrapR to show that these new models are somewhat better calibrated. If they weren’t, there would be no point in updating the models!\nnflfastR switching from the nflscrapR EP and WP models to its own model should not be thought of as a criticism of nflscrapR: the improvements are relatively minor and nflscrapR provided the code base to perform much of this analysis, breaking new ground in the process.\nModel features\nThe models are trained using xgboost, which uses training data to create decision trees.\nEP model features\nSeconds remaining in half\nYard line\nWhether possession team is at home\nRoof type: retractable, dome, or outdoors\nDown\nYards to go\nEra: 1999-2001 (pre-expansion), 2002-2005 (pre-CPOE), 2006-2013 (pre-LOB rules change), 2014-2017, 2018 and beyond\nTimeouts remaining for each team\nWP model features\nSeconds remaining in half\nSeconds remaining in game\nYard line\nScore differential\nRatio of point differential: \\[\\mathrm{diff\\_time\\_ratio}=\\mathrm{point\\_differential}\\cdot e^{4\\cdot\\frac{3600-\\mathrm{game\\_seconds\\_remaining}}{3600}}\\]\nDown\nYards to go\nTimeouts remaining for each team\nWhether team will receive 2nd half kickoff\nWhether possession team is at home\nModel with Vegas line only: \\[\\mathrm{spread\\_time}=\\mathrm{posteam\\_spread}\\cdot e^{-4\\cdot\\frac{3600-\\mathrm{game\\_seconds\\_remaining}}{3600}}\\]\nCP and expected yards after the catch model features\nYard line\nWhether possession team is at home\nRoof type: retractable, dome, or outdoors\nDown\nYards to go\nDistance to sticks (air yards - yards to go)\nEra: 2006-2013, 2014-2017, 2018 and beyond (note that air yards data only go back to 2006, so there is no CP for earlier years)\nAir yards\nWhether air yards is 0 (probably unnecessary with tree-based method and a relic from earlier models where it was included because completion percentage is much lower for 0 air yard passes)\nPass location (binary: middle or not middle)\nWhether quarterback was hit on the play\nFor xyac model only: how far away the goal line is when the ball is caught\nExpected dropback model features\nYard line\nWhether possession team is at home\nRoof type: retractable, dome, or outdoors\nDown\nQuarter\nHalf seconds remaining\nYards to go\nScore differential\nTimeouts remaining for each team\nWin probability (both with spread and non-spread adjusted)\nEra: 2006-2013, 2014-2017, 2018 and beyond (note that scramble data only go back to 2006, so there is no xpass for earlier years)\nEP Model Calibration Results\nThe goal of this section is to show that the nflfastR EP model is well calibrated. To measure calibration, we follow Yurko et al. and perform leave-one-season-out (LOSO) calibration. In particular, for each of the 20 available seasons (2000-2019), we exclude one season, train the EP model on the other 19 seasons, and then compare the model’s predictions in the holdout season to what actually happened in that season. If the model is well calibrated, we would expect that, for example, 50 percent of plays with a touchdown probability of 50 percent prior to the play would have the next score be a touchdown for the possession team.\nLet’s start with some setup. The file used here isn’t pushed because it’s large, but its creation can be seen here and the file can be accessed here.\n\n\nset.seed(2013) # GoHawks\nlibrary(tidyverse)\nlibrary(xgboost)\n\n# some helper files are in these\nsource(\"https://raw.githubusercontent.com/mrcaseb/nflfastR/master/R/helper_add_nflscrapr_mutations.R\")\nsource(\"https://raw.githubusercontent.com/mrcaseb/nflfastR/master/R/helper_add_ep_wp.R\")\nsource(\"https://raw.githubusercontent.com/mrcaseb/nflfastR/master/R/helper_add_cp_cpoe.R\")\n\n# from remote\npbp_data <- readRDS(url(\"https://github.com/guga31bb/nflfastR-data/blob/master/models/cal_data.rds?raw=true\"))\n\n# from local\n# pbp_data <- readRDS('../../nflfastR-data/models/cal_data.rds')\n\nmodel_data <- pbp_data %>%\n  # in 'R/helper_add_nflscrapr_mutations.R'\n  make_model_mutations() %>%\n  mutate(\n    label = case_when(\n      Next_Score_Half == \"Touchdown\" ~ 0,\n      Next_Score_Half == \"Opp_Touchdown\" ~ 1,\n      Next_Score_Half == \"Field_Goal\" ~ 2,\n      Next_Score_Half == \"Opp_Field_Goal\" ~ 3,\n      Next_Score_Half == \"Safety\" ~ 4,\n      Next_Score_Half == \"Opp_Safety\" ~ 5,\n      Next_Score_Half == \"No_Score\" ~ 6\n    ),\n    label = as.factor(label),\n    # use nflscrapR weights\n    Drive_Score_Dist = Drive_Score_Half - drive,\n    Drive_Score_Dist_W = (max(Drive_Score_Dist) - Drive_Score_Dist) /\n      (max(Drive_Score_Dist) - min(Drive_Score_Dist)),\n    ScoreDiff_W = (max(abs(score_differential), na.rm = T) - abs(score_differential)) /\n      (max(abs(score_differential), na.rm = T) - min(abs(score_differential), na.rm = T)),\n    Total_W = Drive_Score_Dist_W + ScoreDiff_W,\n    Total_W_Scaled = (Total_W - min(Total_W, na.rm = T)) /\n      (max(Total_W, na.rm = T) - min(Total_W, na.rm = T))\n  ) %>%\n  filter(\n    !is.na(defteam_timeouts_remaining), !is.na(posteam_timeouts_remaining),\n    !is.na(yardline_100)\n  ) %>%\n  select(\n    label,\n    season,\n    half_seconds_remaining,\n    yardline_100,\n    home,\n    retractable,\n    dome,\n    outdoors,\n    ydstogo,\n    era0, era1, era2, era3, era4,\n    down1, down2, down3, down4,\n    posteam_timeouts_remaining,\n    defteam_timeouts_remaining,\n    Total_W_Scaled\n  )\n\n# idk why this is all necessary for xgb but it is\nmodel_data <- model_data %>%\n  mutate(\n    label = as.numeric(label),\n    label = label - 1\n  )\n\nrm(pbp_data)\n\nseasons <- unique(model_data$season)\n\n\n\nInput the stuff we’ll need to fit the model. The parameters were obtained from cross-validation, where each season was forced to be entirely contained in a given CV fold to prevent leakage in labels from one fold to another (for example, if a given drive were split up between folds).\n\n\nnrounds <- 525\nparams <-\n  list(\n    booster = \"gbtree\",\n    objective = \"multi:softprob\",\n    eval_metric = c(\"mlogloss\"),\n    num_class = 7,\n    eta = 0.025,\n    gamma = 1,\n    subsample = 0.8,\n    colsample_bytree = 0.8,\n    max_depth = 5,\n    min_child_weight = 1\n  )\n\n\n\nNow do the LOSO model fitting.\n\n\ncv_results <- map_dfr(seasons, function(x) {\n  test_data <- model_data %>%\n    filter(season == x) %>%\n    select(-season)\n  train_data <- model_data %>%\n    filter(season != x) %>%\n    select(-season)\n\n  full_train <- xgboost::xgb.DMatrix(model.matrix(~ . + 0, data = train_data %>% select(-label, -Total_W_Scaled)),\n    label = train_data$label, weight = train_data$Total_W_Scaled\n  )\n  ep_model <- xgboost::xgboost(params = params, data = full_train, nrounds = nrounds, verbose = 2)\n\n  preds <- as.data.frame(\n    matrix(predict(ep_model, as.matrix(test_data %>% select(-label, -Total_W_Scaled))), ncol = 7, byrow = TRUE)\n  )\n  colnames(preds) <- c(\n    \"Touchdown\", \"Opp_Touchdown\", \"Field_Goal\", \"Opp_Field_Goal\",\n    \"Safety\", \"Opp_Safety\", \"No_Score\"\n  )\n\n  cv_data <- bind_cols(test_data, preds) %>% mutate(season = x)\n  return(cv_data)\n})\n\n# get the BINS for the calibration plot\nplot <- cv_results %>%\n  select(Touchdown, Opp_Touchdown, Field_Goal, Opp_Field_Goal, Safety, Opp_Safety, No_Score, label) %>%\n  pivot_longer(-label, names_to = \"type\", values_to = \"pred_prob\") %>%\n  mutate(bin_pred_prob = round(pred_prob / 0.05) * .05) %>%\n  mutate(outcome = case_when(\n    label == 0 ~ \"Touchdown\",\n    label == 1 ~ \"Opp_Touchdown\",\n    label == 2 ~ \"Field_Goal\",\n    label == 3 ~ \"Opp_Field_Goal\",\n    label == 4 ~ \"Safety\",\n    label == 5 ~ \"Opp_Safety\",\n    label == 6 ~ \"No_Score\"\n  )) %>%\n  group_by(type, bin_pred_prob) %>%\n  mutate(correct = if_else(outcome == type, 1, 0)) %>%\n  summarize(\n    n_plays = n(),\n    n_outcome = sum(correct),\n    bin_actual_prob = n_outcome / n_plays\n  )\n\n\n\nHere is the EP calibration plot. Points close to the diagonal dotted line are consistent with a well-calibrated model:\n\n\nann_text <- data.frame(\n  x = c(.25, 0.75), y = c(0.75, 0.25),\n  lab = c(\"More times\\nthan expected\", \"Fewer times\\nthan expected\"),\n  next_score_type = factor(\"No Score (0)\")\n)\nplot %>%\n  # about .75M plays in total\n  # filter(n_plays >= 50) %>%\n  ungroup() %>%\n  mutate(\n    type = fct_relevel(\n      type,\n      \"Opp_Safety\", \"Opp_Field_Goal\",\n      \"Opp_Touchdown\", \"No_Score\", \"Safety\",\n      \"Field_Goal\", \"Touchdown\"\n    ),\n    type = fct_recode(type,\n      \"-Field Goal (-3)\" = \"Opp_Field_Goal\",\n      \"-Safety (-2)\" = \"Opp_Safety\",\n      \"-Touchdown (-7)\" = \"Opp_Touchdown\",\n      \"Field Goal (3)\" = \"Field_Goal\",\n      \"No Score (0)\" = \"No_Score\",\n      \"Touchdown (7)\" = \"Touchdown\",\n      \"Safety (2)\" = \"Safety\"\n    )\n  ) %>%\n  ggplot() +\n  geom_point(aes(x = bin_pred_prob, y = bin_actual_prob, size = n_plays)) +\n  geom_smooth(aes(x = bin_pred_prob, y = bin_actual_prob), method = \"loess\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", lty = 2) +\n  coord_equal() +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    size = \"Number of plays\",\n    x = \"Estimated next score probability\",\n    y = \"Observed next score probability\"\n  ) +\n  geom_text(data = ann_text, aes(x = x, y = y, label = lab), size = 2) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_text(size = 10, angle = 90),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    legend.position = c(1, .05), legend.justification = c(1, 0)\n  ) +\n  facet_wrap(~type, ncol = 4)\n\n\n\n\nThere is some weirdness with the opponent safety predictions, but these dots represent an extremely small number of plays (10-50 plays out of about 750,000).\nNow let’s get the calibration error using the measure developed in Yurko et al., and compare it to nflscrapR. First we need to get the nflscrapR predictions, which we have saved from the previous version of nflfastR which applied the nflscrapR models.\n\n\n# calibration error\ncv_cal_error <- plot %>%\n  ungroup() %>%\n  mutate(cal_diff = abs(bin_pred_prob - bin_actual_prob)) %>%\n  group_by(type) %>%\n  summarize(\n    weight_cal_error = weighted.mean(cal_diff, n_plays, na.rm = TRUE),\n    n_scoring_event = sum(n_outcome, na.rm = TRUE)\n  )\n\npbp_data <- readRDS(url(\"https://github.com/guga31bb/nflfastR-data/blob/master/models/cal_data_nflscrapr.rds?raw=true\"))\n# nflscrapr calibration error\nnflscrapr <- pbp_data %>%\n  select(td_prob, opp_td_prob, fg_prob, opp_fg_prob, safety_prob, opp_safety_prob, no_score_prob, Next_Score_Half) %>%\n  pivot_longer(-Next_Score_Half, names_to = \"type\", values_to = \"pred_prob\") %>%\n  mutate(bin_pred_prob = round(pred_prob / 0.05) * .05) %>%\n  mutate(\n    outcome = Next_Score_Half,\n    type = case_when(\n      type == \"td_prob\" ~ \"Touchdown\",\n      type == \"fg_prob\" ~ \"Field_Goal\",\n      type == \"opp_td_prob\" ~ \"Opp_Touchdown\",\n      type == \"opp_fg_prob\" ~ \"Opp_Field_Goal\",\n      type == \"safety_prob\" ~ \"Safety\",\n      type == \"opp_safety_prob\" ~ \"Opp_Safety\",\n      type == \"no_score_prob\" ~ \"No_Score\"\n    )\n  ) %>%\n  group_by(type, bin_pred_prob) %>%\n  mutate(correct = if_else(outcome == type, 1, 0)) %>%\n  summarize(\n    n_plays = n(),\n    n_outcome = sum(correct),\n    bin_actual_prob = n_outcome / n_plays\n  ) %>%\n  ungroup() %>%\n  mutate(cal_diff = abs(bin_pred_prob - bin_actual_prob)) %>%\n  group_by(type) %>%\n  summarize(\n    weight_cal_error = weighted.mean(cal_diff, n_plays, na.rm = TRUE),\n    n_scoring_event = sum(n_outcome, na.rm = TRUE)\n  )\nrm(pbp_data)\n\nglue::glue(\n  \"\n--CALIBRATION ERROR--\n\nnflfastR:\n{round(with(cv_cal_error, weighted.mean(weight_cal_error, n_scoring_event)), 4)}\n\nnflscrapR:\n{round(with(nflscrapr, weighted.mean(weight_cal_error, n_scoring_event)), 4)}\n\"\n)\n\n\n--CALIBRATION ERROR--\n\nnflfastR:\n0.006\n\nnflscrapR:\n0.017\n\nWe see that the new EP model is better calibrated. Note that nflscrapR reports a calibration error of 0.01309723. The number is higher here because of the additional seasons included outside of the time period nflscrapR was trained on, and the lack of era adjustment in nflscrapR.\nWP Model Calibration Results\nAs with EP, do some initial setup to get the data ready for fitting.\n\n\nmodel_data <-\n  readRDS(url(\"https://github.com/guga31bb/metrics/blob/master/wp_tuning/cal_data.rds?raw=true\")) %>%\n  filter(Winner != \"TIE\") %>%\n  make_model_mutations() %>%\n  prepare_wp_data() %>%\n  mutate(label = ifelse(posteam == Winner, 1, 0)) %>%\n  filter(\n    !is.na(ep), !is.na(score_differential), !is.na(play_type), !is.na(label),\n    !is.na(defteam_timeouts_remaining), !is.na(posteam_timeouts_remaining),\n    !is.na(yardline_100), qtr <= 4\n  ) %>%\n  select(\n    label,\n    receive_2h_ko,\n    spread_time,\n    home,\n    half_seconds_remaining,\n    game_seconds_remaining,\n    Diff_Time_Ratio,\n    score_differential,\n    down,\n    ydstogo,\n    yardline_100,\n    posteam_timeouts_remaining,\n    defteam_timeouts_remaining,\n    season,\n    # only needed for the plots here, not used in model\n    qtr\n  )\n\nnrounds <- 65\nparams <-\n  list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    eval_metric = c(\"logloss\"),\n    eta = 0.2,\n    gamma = 0,\n    subsample = 0.8,\n    colsample_bytree = 0.8,\n    max_depth = 4,\n    min_child_weight = 1\n  )\n\nseasons <- unique(model_data$season)\n\n\n\nDo the LOSO fitting:\n\n\ncv_results <- map_dfr(seasons, function(x) {\n  test_data <- model_data %>%\n    filter(season == x) %>%\n    select(-season)\n  train_data <- model_data %>%\n    filter(season != x) %>%\n    select(-season)\n\n  full_train <- xgboost::xgb.DMatrix(model.matrix(~ . + 0, data = train_data %>% select(-label, -qtr, -spread_time)),\n    label = train_data$label\n  )\n  wp_model <- xgboost::xgboost(params = params, data = full_train, nrounds = nrounds, verbose = 2)\n\n  preds <- as.data.frame(\n    matrix(predict(wp_model, as.matrix(test_data %>% select(-label, -qtr, -spread_time))))\n  ) %>%\n    dplyr::rename(wp = V1)\n\n  cv_data <- bind_cols(test_data, preds) %>% mutate(season = x)\n  return(cv_data)\n})\n\n# TIME FOR BINNING\nwp_cv_loso_calibration_results <- cv_results %>%\n  # Create BINS for wp:\n  mutate(bin_pred_prob = round(wp / 0.05) * .05) %>%\n  # Group by both the qtr and bin_pred_prob:\n  group_by(qtr, bin_pred_prob) %>%\n  # Calculate the calibration results:\n  summarize(\n    n_plays = n(),\n    n_wins = length(which(label == 1)),\n    bin_actual_prob = n_wins / n_plays\n  )\n\n\n\nThe WP plot. Looks good!\n\n\n# Create a label data frame for the chart:\nann_text <- data.frame(\n  x = c(.25, 0.75), y = c(0.75, 0.25),\n  lab = c(\"More times\\nthan expected\", \"Fewer times\\nthan expected\"),\n  qtr = factor(\"1st Quarter\")\n)\n\n# Create the calibration chart:\nwp_cv_loso_calibration_results %>%\n  ungroup() %>%\n  mutate(qtr = fct_recode(factor(qtr),\n    \"1st Quarter\" = \"1\", \"2nd Quarter\" = \"2\",\n    \"3rd Quarter\" = \"3\", \"4th Quarter\" = \"4\"\n  )) %>%\n  ggplot() +\n  geom_point(aes(x = bin_pred_prob, y = bin_actual_prob, size = n_plays)) +\n  geom_smooth(aes(x = bin_pred_prob, y = bin_actual_prob), method = \"loess\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", lty = 2) +\n  coord_equal() +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    size = \"Number of plays\",\n    x = \"Estimated win probability\",\n    y = \"Observed win probability\"\n  ) +\n  geom_text(data = ann_text, aes(x = x, y = y, label = lab), size = 2) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_text(size = 10, angle = 90),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    legend.position = \"bottom\"\n  ) +\n  facet_wrap(~qtr, ncol = 4)\n\n\n\n\nAnd get the WP calibration error:\n\n\n# Calculate the calibration error values:\nwp_cv_cal_error <- wp_cv_loso_calibration_results %>%\n  ungroup() %>%\n  mutate(cal_diff = abs(bin_pred_prob - bin_actual_prob)) %>%\n  group_by(qtr) %>%\n  summarize(\n    weight_cal_error = weighted.mean(cal_diff, n_plays, na.rm = TRUE),\n    n_wins = sum(n_wins, na.rm = TRUE)\n  )\n\n# get nflscrapR to compare\npbp_data <- readRDS(url(\"https://github.com/guga31bb/nflfastR-data/blob/master/models/cal_data_nflscrapr.rds?raw=true\")) %>%\n  mutate(label = ifelse(posteam == Winner, 1, 0)) %>%\n  filter(qtr <= 4, !is.na(label), !is.na(posteam), !is.na(wp))\n\nnflscrapR <- pbp_data %>%\n  # Create binned probability column:\n  mutate(bin_pred_prob = round(wp / 0.05) * .05) %>%\n  # Group by both the qtr and bin_pred_prob:\n  group_by(qtr, bin_pred_prob) %>%\n  # Calculate the calibration results:\n  summarize(\n    n_plays = n(),\n    n_wins = length(which(label == 1)),\n    bin_actual_prob = n_wins / n_plays\n  ) %>%\n  ungroup() %>%\n  mutate(cal_diff = abs(bin_pred_prob - bin_actual_prob)) %>%\n  group_by(qtr) %>%\n  summarize(\n    weight_cal_error = weighted.mean(cal_diff, n_plays, na.rm = TRUE),\n    n_wins = sum(n_wins, na.rm = TRUE)\n  )\n\nglue::glue(\n  \"--CALIBRATION ERROR--\n\nnflfastR:\n{round(with(wp_cv_cal_error, weighted.mean(weight_cal_error, n_wins)), 4)}\n\nnflscrapR:\n{round(with(nflscrapR, weighted.mean(weight_cal_error, n_wins)), 4)}\"\n)\n\n\n--CALIBRATION ERROR--\n\nnflfastR:\n0.0055\n\nnflscrapR:\n0.0397\n\nAgain, the new WP model represents an improvement.\nWP Model Calibration Results: with point spread\nnflfastR has a secondary win probability model that also incorporates the pregame spread to more accurately reflect a team’s chances of winning. Below are calibration results for this model.\n\n\nnrounds <- 534\nparams <-\n  list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    eval_metric = c(\"logloss\"),\n    eta = 0.05,\n    gamma = .79012017,\n    subsample = 0.9224245,\n    colsample_bytree = 5 / 12,\n    max_depth = 5,\n    min_child_weight = 7,\n    monotone_constraints =\n      \"(0, 0, 0, 0, 0, 1, 1, -1, -1, -1, 1, -1)\"\n  )\n\n\n\nDo the LOSO fitting:\n\n\ncv_results <- map_dfr(seasons, function(x) {\n  test_data <- model_data %>%\n    filter(season == x) %>%\n    select(-season)\n  train_data <- model_data %>%\n    filter(season != x) %>%\n    select(-season)\n\n  full_train <- xgboost::xgb.DMatrix(model.matrix(~ . + 0, data = train_data %>% select(-label, -qtr)),\n    label = train_data$label\n  )\n  wp_model <- xgboost::xgboost(params = params, data = full_train, nrounds = nrounds, verbose = 2)\n\n  preds <- as.data.frame(\n    matrix(predict(wp_model, as.matrix(test_data %>% select(-label, -qtr))))\n  ) %>%\n    dplyr::rename(wp = V1)\n\n  cv_data <- bind_cols(test_data, preds) %>% mutate(season = x)\n  return(cv_data)\n})\n\n# TIME FOR BINNING\nwp_cv_loso_calibration_results <- cv_results %>%\n  # Create BINS for wp:\n  mutate(bin_pred_prob = round(wp / 0.05) * .05) %>%\n  # Group by both the qtr and bin_pred_prob:\n  group_by(qtr, bin_pred_prob) %>%\n  # Calculate the calibration results:\n  summarize(\n    n_plays = n(),\n    n_wins = length(which(label == 1)),\n    bin_actual_prob = n_wins / n_plays\n  )\n\n\n\nThe WP plot.\n\n\n# Create a label data frame for the chart:\nann_text <- data.frame(\n  x = c(.25, 0.75), y = c(0.75, 0.25),\n  lab = c(\"More times\\nthan expected\", \"Fewer times\\nthan expected\"),\n  qtr = factor(\"1st Quarter\")\n)\n\n# Create the calibration chart:\nwp_cv_loso_calibration_results %>%\n  ungroup() %>%\n  mutate(qtr = fct_recode(factor(qtr),\n    \"1st Quarter\" = \"1\", \"2nd Quarter\" = \"2\",\n    \"3rd Quarter\" = \"3\", \"4th Quarter\" = \"4\"\n  )) %>%\n  ggplot() +\n  geom_point(aes(x = bin_pred_prob, y = bin_actual_prob, size = n_plays)) +\n  geom_smooth(aes(x = bin_pred_prob, y = bin_actual_prob), method = \"loess\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", lty = 2) +\n  coord_equal() +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    size = \"Number of plays\",\n    x = \"Estimated win probability\",\n    y = \"Observed win probability\"\n  ) +\n  geom_text(data = ann_text, aes(x = x, y = y, label = lab), size = 2) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_text(size = 10, angle = 90),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    legend.position = \"bottom\"\n  ) +\n  facet_wrap(~qtr, ncol = 4)\n\n\n\n\nAnd get the WP calibration error:\n\n\n# Calculate the calibration error values:\nwp_cv_cal_error <- wp_cv_loso_calibration_results %>%\n  ungroup() %>%\n  mutate(cal_diff = abs(bin_pred_prob - bin_actual_prob)) %>%\n  group_by(qtr) %>%\n  summarize(\n    weight_cal_error = weighted.mean(cal_diff, n_plays, na.rm = TRUE),\n    n_wins = sum(n_wins, na.rm = TRUE)\n  )\nglue::glue(\n  \"--CALIBRATION ERROR--\n\nnflfastR with Vegas line:\n{round(with(wp_cv_cal_error, weighted.mean(weight_cal_error, n_wins)), 4)}\n\nnflscrapR:\n{round(with(nflscrapR, weighted.mean(weight_cal_error, n_wins)), 4)}\"\n)\n\n\n--CALIBRATION ERROR--\n\nnflfastR with Vegas line:\n0.0066\n\nnflscrapR:\n0.0397\n\nAgain, the new WP model is better calibrated than nflscrapR. In our testing, incorporating the spread substantially improved the performance of the model as measured by cross-validation classification accuracy (reduced error rate from 27% to 23%) and log loss (reduced from .52 to .44). We include a time-decaying function of spread on its own as including spread on its own increases the LOSO calibration error, especially in the fourth quarter. We also tried removing the home indicator in the spread model, but this worsened the calibration results. Adding the over/under also worsened calibration results.\nCP Model Calibration Results\nBy now, the process should be familiar.\n\n\npbp <- readRDS(url(\"https://github.com/guga31bb/nflfastR-data/blob/master/models/cal_data.rds?raw=true\"))\n\nmodel_data <- pbp %>%\n  filter(season >= 2006) %>%\n  make_model_mutations() %>%\n  dplyr::mutate(\n    receiver_player_name =\n      stringr::str_extract(desc, \"(?<=((to)|(for))\\\\s[:digit:]{0,2}\\\\-{0,1})[A-Z][A-z]*\\\\.\\\\s?[A-Z][A-z]+(\\\\s(I{2,3})|(IV))?\"),\n    pass_middle = dplyr::if_else(pass_location == \"middle\", 1, 0),\n    air_is_zero = dplyr::if_else(air_yards == 0, 1, 0),\n    distance_to_sticks = air_yards - ydstogo\n  ) %>%\n  dplyr::filter(complete_pass == 1 | incomplete_pass == 1 | interception == 1) %>%\n  dplyr::filter(!is.na(air_yards) & air_yards >= -15 & air_yards < 70 & !is.na(receiver_player_name) & !is.na(pass_location)) %>%\n  dplyr::select(\n    season, complete_pass, air_yards, yardline_100, ydstogo,\n    down1, down2, down3, down4, air_is_zero, pass_middle,\n    era2, era3, era4, qb_hit, home,\n    outdoors, retractable, dome, distance_to_sticks\n  )\nrm(pbp)\n\nseasons <- unique(model_data$season)\n\n\nnrounds <- 560\nparams <-\n  list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    eval_metric = c(\"logloss\"),\n    eta = 0.025,\n    gamma = 5,\n    subsample = 0.8,\n    colsample_bytree = 0.8,\n    max_depth = 4,\n    min_child_weight = 6,\n    base_score = mean(model_data$complete_pass)\n  )\n\ncv_results <- map_dfr(2006:2019, function(x) {\n  test_data <- model_data %>%\n    filter(season == x) %>%\n    select(-season)\n  train_data <- model_data %>%\n    filter(season != x) %>%\n    select(-season)\n\n  full_train <- xgboost::xgb.DMatrix(model.matrix(~ . + 0, data = train_data %>% select(-complete_pass)),\n    label = train_data$complete_pass\n  )\n  cp_model <- xgboost::xgboost(params = params, data = full_train, nrounds = nrounds, verbose = 2)\n\n  preds <- as.data.frame(\n    matrix(predict(cp_model, as.matrix(test_data %>% select(-complete_pass))))\n  ) %>%\n    dplyr::rename(cp = V1)\n\n  cv_data <- bind_cols(test_data, preds) %>% mutate(season = x)\n  return(cv_data)\n})\n\n# TIME FOR BINNING\ncp_cv_loso_calibration_results <- cv_results %>%\n  # Create BINS for wp:\n  mutate(\n    bin_pred_prob = round(cp / 0.05) * .05,\n    distance = case_when(\n      air_yards < 5 ~ \"Short\",\n      air_yards >= 5 & air_yards < 15 ~ \"Intermediate\",\n      air_yards >= 15 ~ \"Deep\"\n    )\n  ) %>%\n  # Group by both the qtr and bin_pred_prob:\n  group_by(distance, bin_pred_prob) %>%\n  # Calculate the calibration results:\n  summarize(\n    n_plays = n(),\n    n_complete = length(which(complete_pass == 1)),\n    bin_actual_prob = n_complete / n_plays\n  )\n\nann_text <- data.frame(\n  x = c(.25, 0.75), y = c(0.75, 0.25),\n  lab = c(\"More times\\nthan expected\", \"Fewer times\\nthan expected\")\n)\n\n\n\nPlot the results:\n\n\ncp_cv_loso_calibration_results %>%\n  ungroup() %>%\n  mutate(distance = fct_relevel(\n    distance,\n    \"Short\", \"Intermediate\", \"Deep\"\n  )) %>%\n  filter(n_plays > 10) %>%\n  ggplot() +\n  geom_point(aes(x = bin_pred_prob, y = bin_actual_prob, size = n_plays)) +\n  geom_smooth(aes(x = bin_pred_prob, y = bin_actual_prob), method = \"loess\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", lty = 2) +\n  coord_equal() +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    size = \"Number of plays\",\n    x = \"Estimated completion percentage\",\n    y = \"Observed completion percentage\"\n  ) +\n  geom_text(data = ann_text, aes(x = x, y = y, label = lab), size = 3) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_text(size = 10, angle = 90),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    legend.position = \"bottom\"\n  ) +\n  facet_wrap(~distance, ncol = 3)\n\n\n\n\nAnd get the calibration error:\n\n\ncp_cv_cal_error <- cp_cv_loso_calibration_results %>%\n  ungroup() %>%\n  mutate(cal_diff = abs(bin_pred_prob - bin_actual_prob)) %>%\n  group_by(distance) %>%\n  summarize(\n    weight_cal_error = weighted.mean(cal_diff, n_plays, na.rm = TRUE),\n    n_complete = sum(n_complete, na.rm = TRUE)\n  )\n\nround(with(cp_cv_cal_error, weighted.mean(weight_cal_error, n_complete)), 4)\n\n\n[1] 0.005\n\nxYAC Model Calibration Results\nBy now, the process should be familiar.\n\n\npbp_data <- readRDS(url(\"https://github.com/guga31bb/nflfastR-data/blob/master/models/cal_data.rds?raw=true\"))\n# pbp_data <- readRDS('../../nflfastR-data/models/cal_data.rds')\n\nmodel_data <- pbp_data %>%\n  make_model_mutations() %>%\n  filter(\n    season >= 2006, complete_pass == 1, !is.na(yards_after_catch),\n    yards_after_catch >= -20, air_yards < yardline_100\n  ) %>%\n  dplyr::mutate(\n    distance_to_goal = yardline_100 - air_yards,\n    pass_middle = dplyr::if_else(pass_location == \"middle\", 1, 0),\n    air_is_zero = dplyr::if_else(air_yards == 0, 1, 0),\n    distance_to_sticks = air_yards - ydstogo,\n    yards_after_catch = dplyr::case_when(\n      yards_after_catch < -5 ~ -5,\n      yards_after_catch > 70 ~ 70,\n      TRUE ~ yards_after_catch\n    ),\n    label = yards_after_catch + 5\n  ) %>%\n  dplyr::filter(!is.na(air_yards) & air_yards >= -15 & air_yards < 70 & !is.na(pass_location)) %>%\n  dplyr::select(\n    season, label, air_yards, yardline_100, ydstogo, distance_to_goal,\n    down1, down2, down3, down4, air_is_zero, pass_middle,\n    era2, era3, era4, qb_hit, home,\n    outdoors, retractable, dome, distance_to_sticks\n  )\n\n\n# nrounds = 500\nnrounds <- 500\nparams <-\n  list(\n    booster = \"gbtree\",\n    objective = \"multi:softprob\",\n    eval_metric = c(\"mlogloss\"),\n    num_class = 76,\n    eta = .025,\n    gamma = 2,\n    subsample = 0.8,\n    colsample_bytree = 0.8,\n    max_depth = 4,\n    min_child_weight = 1\n  )\n\n\ncv_results <- map_dfr(2006:2019, function(x) {\n  test_data <- model_data %>%\n    filter(season == x) %>%\n    select(-season)\n  train_data <- model_data %>%\n    filter(season != x) %>%\n    select(-season)\n\n  full_train <- xgboost::xgb.DMatrix(model.matrix(~ . + 0, data = train_data %>% select(-label)), label = train_data$label)\n  xyac_model <- xgboost::xgboost(params = params, data = full_train, nrounds = nrounds, verbose = 2)\n\n  preds <- as.data.frame(\n    matrix(predict(xyac_model, as.matrix(test_data %>% select(-label))), ncol = 76, byrow = TRUE)\n  )\n\n  cv_data <- bind_cols(test_data, preds) %>%\n    mutate(season = x)\n  return(cv_data)\n})\n\n\n\n\n\nplot <- cv_results %>%\n  select(label, air_yards, starts_with(\"V\")) %>%\n  mutate(\n    loss = V1 + V2 + V3 + V4 + V5 + V6,\n    short_gain = V7 + V8 + V9 + V10 + V11,\n    med_gain = V12 + V13 + V14 + V15 + V16,\n    long_gain = select(., V17:V76) %>% rowSums(),\n    outcome = case_when(\n      label <= 5 ~ \"loss\",\n      between(label, 6, 10) ~ \"short_gain\",\n      between(label, 11, 15) ~ \"med_gain\",\n      label > 15 ~ \"long_gain\"\n    ),\n    distance = case_when(\n      air_yards < 5 ~ \"1: Short\",\n      air_yards >= 5 ~ \"2: Long\"\n    )\n  ) %>%\n  select(outcome, distance, loss, short_gain, med_gain, long_gain) %>%\n  pivot_longer(-c(outcome, distance), names_to = \"type\", values_to = \"pred_prob\") %>%\n  mutate(bin_pred_prob = round(pred_prob / 0.05) * .05) %>%\n  group_by(type, distance, bin_pred_prob) %>%\n  mutate(correct = if_else(outcome == type, 1, 0)) %>%\n  summarize(\n    n_plays = n(),\n    n_outcome = sum(correct),\n    bin_actual_prob = n_outcome / n_plays\n  )\n\nann_text <- data.frame(\n  x = c(.25, 0.75), y = c(0.75, 0.25),\n  lab = c(\"More times\\nthan expected\", \"Fewer times\\nthan expected\")\n)\n\n\n\n\n\nplot %>%\n  ungroup() %>%\n  mutate(\n    type = fct_relevel(\n      type,\n      \"loss\", \"short_gain\",\n      \"med_gain\", \"long_gain\"\n    ),\n    type = fct_recode(type,\n      \"Loss/ no gain\" = \"loss\",\n      \"1-5 yards\" = \"short_gain\",\n      \"6-10 yards\" = \"med_gain\",\n      \"11+ yards\" = \"long_gain\"\n    )\n  ) %>%\n  filter(n_plays > 15) %>%\n  ggplot() +\n  geom_point(aes(x = bin_pred_prob, y = bin_actual_prob, size = n_plays)) +\n  geom_smooth(aes(x = bin_pred_prob, y = bin_actual_prob), method = \"loess\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", lty = 2) +\n  coord_equal() +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    size = \"Number of plays\",\n    x = \"Estimated yards after catch\",\n    y = \"Observed yards after catch\"\n  ) +\n  geom_text(data = ann_text, aes(x = x, y = y, label = lab), size = 2) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_text(size = 10, angle = 90),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    legend.position = c(1.5, .05), legend.justification = c(1, 0)\n  ) +\n  facet_wrap(~ distance + type, ncol = 4)\n\n\n\n\nCalibration error:\n\n\nxyac_cv_cal_error <- plot %>%\n  ungroup() %>%\n  mutate(cal_diff = abs(bin_pred_prob - bin_actual_prob)) %>%\n  group_by(distance) %>%\n  summarize(\n    weight_cal_error = weighted.mean(cal_diff, n_plays, na.rm = TRUE),\n    n_outcome = sum(n_outcome, na.rm = TRUE)\n  )\n\nround(with(xyac_cv_cal_error, weighted.mean(weight_cal_error, n_outcome)), 4)\n\n\n[1] 0.0058\n\nExpected Pass Model Calibration Results\nBy now, the process should be super familiar.\n\n\n# model_data <- readRDS(url('https://github.com/guga31bb/nflfastR-data/blob/master/models/_dropback_model_data.rds?raw=true'))\n\nmodel_data <- readRDS(\"_dropback_model_data.rds\")\n\nnrounds <- 1121\nparams <-\n  list(\n    booster = \"gbtree\",\n    objective = \"binary:logistic\",\n    eval_metric = c(\"error\", \"logloss\"),\n    eta = .015,\n    gamma = 2,\n    subsample = 0.8,\n    colsample_bytree = 0.8,\n    max_depth = 7,\n    min_child_weight = 0.9,\n    base_score = mean(model_data$label)\n  )\n\ncv_results <- map_dfr(2006:2019, function(x) {\n  test_data <- model_data %>%\n    filter(season == x) %>%\n    select(-season)\n  train_data <- model_data %>%\n    filter(season != x) %>%\n    select(-season)\n\n  full_train <- xgboost::xgb.DMatrix(model.matrix(~ . + 0, data = train_data %>% select(-label)),\n    label = train_data$label\n  )\n  xp_model <- xgboost::xgboost(params = params, data = full_train, nrounds = nrounds, verbose = 2)\n\n  preds <- as.data.frame(\n    matrix(predict(xp_model, as.matrix(test_data %>% select(-label))))\n  ) %>%\n    dplyr::rename(xp = V1)\n\n  cv_data <- bind_cols(test_data, preds) %>% mutate(season = x)\n  return(cv_data)\n})\n\n# TIME FOR BINNING\nxp_cv_loso_calibration_results <- cv_results %>%\n  # Create BINS for wp:\n  mutate(\n    bin_pred_prob = round(xp / 0.05) * .05,\n    situation = case_when(\n      down == 1 & ydstogo == 10 ~ \"1st & 10\",\n      down == 2 ~ \"2nd down\",\n      down == 3 ~ \"3rd down\",\n      TRUE ~ \"Other\"\n    )\n  ) %>%\n  # Group by both the qtr and bin_pred_prob:\n  group_by(situation, bin_pred_prob) %>%\n  # Calculate the calibration results:\n  summarize(\n    n_plays = n(),\n    n_complete = length(which(label == 1)),\n    bin_actual_prob = n_complete / n_plays\n  )\n\nann_text <- data.frame(\n  x = c(.25, 0.75), y = c(0.75, 0.25),\n  lab = c(\"More times\\nthan expected\", \"Fewer times\\nthan expected\")\n)\n\n\n\nPlot the results:\n\n\nxp_cv_loso_calibration_results %>%\n  ungroup() %>%\n  mutate(situation = fct_relevel(\n    situation,\n    \"1st & 10\", \"2nd down\", \"3rd down\", \"Other\"\n  )) %>%\n  filter(n_plays > 10) %>%\n  ggplot() +\n  geom_point(aes(x = bin_pred_prob, y = bin_actual_prob, size = n_plays)) +\n  geom_smooth(aes(x = bin_pred_prob, y = bin_actual_prob), method = \"loess\") +\n  geom_abline(slope = 1, intercept = 0, color = \"black\", lty = 2) +\n  coord_equal() +\n  scale_x_continuous(limits = c(0, 1)) +\n  scale_y_continuous(limits = c(0, 1)) +\n  labs(\n    size = \"Number of plays\",\n    x = \"Estimated dropback percentage\",\n    y = \"Observed dropback percentage\"\n  ) +\n  geom_text(data = ann_text, aes(x = x, y = y, label = lab), size = 3) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    strip.background = element_blank(),\n    strip.text = element_text(size = 12),\n    axis.title = element_text(size = 12),\n    axis.text.y = element_text(size = 12),\n    axis.text.x = element_text(size = 10, angle = 90),\n    legend.title = element_text(size = 12),\n    legend.text = element_text(size = 12),\n    legend.position = \"bottom\"\n  ) +\n  facet_wrap(~situation, ncol = 4)\n\n\n\n\nAnd get the calibration error:\n\n\nxp_cv_cal_error <- xp_cv_loso_calibration_results %>%\n  ungroup() %>%\n  mutate(cal_diff = abs(bin_pred_prob - bin_actual_prob)) %>%\n  group_by(situation) %>%\n  summarize(\n    weight_cal_error = weighted.mean(cal_diff, n_plays, na.rm = TRUE),\n    n_complete = sum(n_complete, na.rm = TRUE)\n  )\n\nround(with(xp_cv_cal_error, weighted.mean(weight_cal_error, n_complete)), 4)\n\n\n[1] 0.008\n\n\n\n\nView source code on GitHub \n\n\n\n\n\n",
    "preview": "posts/2020-09-28-nflfastr-ep-wp-and-cp-models/nflfastr-ep-wp-and-cp-models_files/figure-html5/plot-1.png",
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {},
    "preview_width": 9100,
    "preview_height": 5600
  },
  {
    "path": "posts/2021-02-05-gunslingers-and-game-managers/",
    "title": "Gunslingers and Game Managers",
    "description": "Quantifying Quarterbacks' performance along the \"game manager\" and \"gunslinger\" dimensions.",
    "author": [
      {
        "name": "David Anderson",
        "url": "https://twitter.com/DAnderso13"
      }
    ],
    "date": "2021-02-05",
    "categories": [
      "Quarterback Analysis",
      "nflfastR",
      "Stupid Internet Arguments"
    ],
    "contents": "\n\n\n\nGunslingers vs. Game Managers is the stupidest football argument out there. So, naturally, I love it. This post is going to quantify what gunslingers are, what game managers are, and measure these for every QB since 2005.\nGunslingers are higher risk, higher reward type of quarterbacks - typically they throw for a ton of yards, have a lot of touchdown passes, scramble and run around a lot, throw lots of deep balls, but also take negative plays, and throw interceptions. Brett Favre is the patron saint of gunslingers (unfortunately his prime gunslinging days predate the nflfastR pbp data), but players like Jameis Winston and Rex “Screw It, I’m Going Deep” Grossman are also known as gunslingers, as well.\nGame Managers are lower-risk (and potentially also lower-reward) players. A game manager doesn’t make mistakes - takes very few sacks, doesn’t throw interceptions and completes a percent of their passes.\nFirst, we load our data:\n\n\nlibrary(pacman)\np_load(tidyverse, janitor)\nseasons <- 2005:2020\n\npbp <- purrr::map_df(seasons, function(x) {\n  readRDS(\n    url(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\")\n    )\n  )\n})\n\n\n\nNow I just want the plays that are the QB, so all dropbacks. I am excluding designed QB runs as those exist outside the gunslinger vs game manager paradigm. I’m also going to strip out the season by taking the first 4 digits of the game id.\n\n\npbp %>% filter(qb_dropback == 1, !is.na(epa)) -> passes\n\npasses <- passes %>% mutate(year = substr(game_id,1,4))\n\n\n\nNow we want to summarize each QB’s performance each year: What is a Game Manager? A game manager doesn’t throw interceptions, doesn’t take sacks, doesn’t take negative plays (which we’ll measure by success rate), and complete a lot of passes.\n\n\nStats <- passes %>% filter(home_wp > 0.1, home_wp < 0.9, qb_dropback==1) %>% \n           mutate(QB = ifelse(is.na(passer_player_name), rusher_player_name, passer_player_name)) %>% \n           group_by(QB, year) %>% \n           summarize(numDropbacks = n(),\n                    passAttempts = sum(pass_attempt),\n                    #Game Manager Calculations:\n                    completions = sum(complete_pass),\n                    completionPercent = completions / passAttempts,\n                    ints = sum(interception),\n                    sacks = sum(sack)/numDropbacks,\n                    successRate = mean(success),\n                    \n                    #Gunslinger Calculations:\n                    meanAirYards = mean(air_yards, na.rm=TRUE),\n                    tds = sum(touchdown),\n                    shortOfSticksOnThird = sum(down==3 & air_yards < ydstogo & third_down_failed, na.rm=TRUE),\n                    thirdDownDropbacks = sum(down==3, na.rm=TRUE ),\n                    shortOfSticksOnThirdRate = shortOfSticksOnThird / thirdDownDropbacks,  \n                    passYards = sum(yards_gained[pass_attempt==1], na.rm=TRUE),\n                    rushYards = sum(yards_gained[pass_attempt==0], na.rm=TRUE),\n                    team = last(posteam),\n                    throwAways = sum(is.na(receiver_player_name) & pass_attempt==1),\n                    throwAwayRate = throwAways/numDropbacks,\n                    \n                    #EPA\n                    meanEPA = mean(epa),\n                    sdEPA = sd(epa)\n                    ) %>% \n           filter(passAttempts > 200) %>%\n           arrange(desc(meanEPA)) %>% mutate(ID = paste(QB, \"_\", year))\n\n\nhead(Stats)\n\n\n# A tibble: 6 x 22\n# Groups:   QB [3]\n  QB    year  numDropbacks passAttempts completions completionPerce~\n  <chr> <chr>        <int>        <dbl>       <dbl>            <dbl>\n1 A.Ro~ 2011           467          435         276            0.634\n2 P.Ma~ 2018           544          519         331            0.638\n3 T.Br~ 2007           524          514         338            0.658\n4 A.Ro~ 2020           474          455         313            0.688\n5 P.Ma~ 2020           600          564         367            0.651\n6 P.Ma~ 2019           529          500         322            0.644\n# ... with 16 more variables: ints <dbl>, sacks <dbl>,\n#   successRate <dbl>, meanAirYards <dbl>, tds <dbl>,\n#   shortOfSticksOnThird <int>, thirdDownDropbacks <int>,\n#   shortOfSticksOnThirdRate <dbl>, passYards <dbl>, rushYards <dbl>,\n#   team <chr>, throwAways <int>, throwAwayRate <dbl>, meanEPA <dbl>,\n#   sdEPA <dbl>, ID <chr>\n\nOkay so now lets combine all these into one “gunslinger” score. We’ll standardize each variable so it has a mean of 0 and a standard deviation of 1, using the scale() command. I then weight each feature by how important I think this is:\nTD passes - weight 75% Deep Passes (average air yards) - weight 100% Scramble Yards - weight 25% Not throwing short of Sticks on Third - weight 75% Lots of pass yards - weight 75%\n\n\nGunslinger <- Stats %>% select(QB, year, tds, passYards, meanAirYards, rushYards, shortOfSticksOnThirdRate, team)\ncols <- c('tds', 'passYards', 'rushYards', 'shortOfSticksOnThirdRate', 'meanAirYards')\ntdWeight <- 0.75\npassWeight <- 0.5\nrushWeight <- 0.25\nshortWeight <- 0.75\nairWeight <- 1\nGunslinger[cols] <- scale(Gunslinger[cols])\nGunslinger <- Gunslinger %>% mutate(GunslingScore = tdWeight* tds + passWeight* passYards + rushWeight* rushYards - shortWeight * shortOfSticksOnThirdRate + airWeight* meanAirYards) %>%  arrange(-GunslingScore)\nGunslinger$GunslingScore <- scale(Gunslinger$GunslingScore)\n\n\n\nAnd now for game managers:\nDon’t throw interceptions - 100% High success rate (no negative plays) - 70% Don’t take sacks - 50% High completion rate - 75%\n\n\nManager <- Stats %>% select(QB, year, ints, successRate, sacks, completionPercent, team, meanEPA, sdEPA)\ncols <- c('ints', 'successRate', 'sacks', 'completionPercent')\nManager[cols] <- scale(Manager[cols])\nintWeight <- 1\nsuccessWeight <- 0.70\nsacksWeight <- 0.50\ncompleteWeight <- 75\nManager <- Manager %>% mutate(ManageScore = -1*ints*intWeight + successRate*successWeight - sacks*sacksWeight + completionPercent*completeWeight) %>%  arrange(-ManageScore) \nManager$ManageScore <- scale(Manager$ManageScore)\n\n\n\nNow we joing them together and plot them. To keep the graph from getting cluttered, I’m just plotting 2019.\nLooking at this graph, quarterbacks in the upper left are true “gunslingers” - high performance as gunslingers, and low performance as game managers. Players in the bottom right are pure game managers. Quarterbacks in the upper right are just really good, while those in the lower left are just not very good at anything.\nJameis Winston is a great gunslinger, but a terrible game manager, and to a lesser extent, so was Aaron Rodgers. Drew Brees, however, is a great game manager, and not much of a gunslinger anymore. In the upper right, Pat Mahomes is really good at everything.\n\n\nlibrary(nflfastR)\nlibrary(ggrepel)\nlibrary(ggimage)\n\n\n\nCombined <- Gunslinger %>% left_join(Manager) %>%  left_join(teams_colors_logos, by = c('team' = 'team_abbr')) %>% arrange(-GunslingScore)\n\nCombined %>% filter(year == 2019) %>%  ggplot(aes(x=ManageScore, y=GunslingScore, label=QB)) +\n  geom_image(aes(image = team_logo_espn)) +\n  geom_text_repel(aes(label=QB)) + \n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\", alpha=0.5) +\n  geom_vline(xintercept =  0, color = \"red\", linetype = \"dashed\", alpha=0.5) + \n  labs(x = \"Game Managing Performance\", y=\"Gunslinging Performance\")\n\n\n\n\nIf you disagree with my weights, I have a shiny app where you can change them and generate your own scores: https://davidranderson.shinyapps.io/GunslingersVsGameManagers/\n\n\n\nView source code on GitHub \n\n\n\n\n\n",
    "preview": "posts/2021-02-05-gunslingers-and-game-managers/gunslingers-and-game-managers_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3000
  },
  {
    "path": "posts/2021-01-21-nfl-game-prediction-using-logistic-regression/",
    "title": "Modeling NFL game outcomes using Python and scikit-learn",
    "description": "Use Python and scikit-learn to model NFL game outcomes and build a pre-game win probability model.",
    "author": [
      {
        "name": "Ben Dominguez",
        "url": "https://twitter.com/bendominguez011"
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "python",
      "scikit-learn",
      "Game Prediction"
    ],
    "contents": "\n\nContents\nIntro to NFL game modeling in Python\nFinal Thoughts\n\nIntro to NFL game modeling in Python\nIn this post we are going to cover modeling NFL game outcomes and pre-game win probability using a logistic regression model in Python and scikit-learn. Previous posts on Open Source Football have covered engineering EPA to maximize it’s predictive value, and this post will build partly upon those written by Jack Lichtenstien and John Goldberg.\nThe goal of this post will be to provide you with an introduction to modeling in Python and a baseline model to work from. Python’s de-facto machine learning library, sklearn, is built upon a streamlined API which allows ML code to be iterated upon easily. To switch to a more complex model wouldn’t take much tweaking of the code I’ll provide here, as every supervised algorithm is implemented via sklearn in more or less the same fashion. As with any machine learning task, the bulk of the work will be in the data munging, cleaning, and feature extraction/engineering process.\nFor our features, we will be using exponentially weighted rolling EPA. The window size for the rolling average will be 10 for all teams before week 10 of the season. This means that prior to week 10, some prior season data will be used. If we’re past week 10, the entire, and only the entire, season will be included in the calculation of rolling EPA. This dynamic window size idea was Jack Lichtenstien’s, and his post on the topic is linked above. His post showed that using a dynamic window size was slightly more predictive than using a static 10 game window.\nEPA will be split in to defense and offense for both teams, and then further split in to passing and rushing. This means in total, we’ll have 8 features:\nHome team passing offense EPA/play\nHome team passing defense EPA/play\nHome team rushing offense EPA/play\nHome team rushing defense EPA/play\nAway team passing offense EPA/play\nAway team passing defense EPA/play\nAway team rushing offense EPA/play\nAway team rushing defense EPA/play\nThe target will be a home team win.\nEach of these features will be lagged one period, and then an exponential moving average will be calculated.\nWe’re going to be using Logistic Regression as our model. Logistic Regression is used to model the probability of a binary outcome. The probability we are attempting to model here is the probability a home team wins given the features we’ve laid out above. We’ll see later that our LogisticRegression object has a predict_proba method which shows us the predicted probability of a 1 (home team win) or 0 (away team win). This means the model can be used as a pre-game win probability model as well.\nWe’ll be training the model with data from 1999 - 2019, and leaving 2020 out so we can analyze it further at the end of the post.\nTo start, let’s import the libraries we’ll need for this notebook. You’ll need to have sklearn, pandas, numpy, matplotlib, and nflfastpy installed to be able to run this code. All these libraries can be installed via pip.\n\nimport nflfastpy as nfl\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport matplotlib.ticker as plticker\nimport numpy as np\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nplt.style.use('seaborn-talk')\nplt.style.use('ggplot')\n\npd.set_option('display.max_columns', 7)\n\nThe code block below will pull nflfastR data from the nflfastR-data repository and concatenate the seperate, yearly DataFrames in to a single DataFrame we’ll call data. This code block will take anywhere from 2-5 minutes.\n\ndata = pd.concat([nfl.load_pbp_data(season).assign(season=season) for season in range(1999, 2021)])\n\nThe code below is going to calculate a rolling EPA with a static and dynamic window. I’ve included both, although we’ll only be using the rolling EPA with the dynamic window in our model.\n\n\ndef dynamic_window_ewma(x):\n    \"\"\"\n    Calculate rolling exponentially weighted EPA with a dynamic window size\n    \"\"\"\n    values = np.zeros(len(x))\n    for i, (_, row) in enumerate(x.iterrows()):\n        epa = x.epa_shifted[:i+1]\n        if row.week > 10:\n            values[i] = epa.ewm(min_periods=1, span=row.week).mean().values[-1]\n        else:\n            values[i] = epa.ewm(min_periods=1, span=10).mean().values[-1]\n            \n    return pd.Series(values, index=x.index)\n\n# seperate EPA in to rushing offense, rushing defense, passing offense, passing defense for each team\nrushing_offense_epa = data.loc[data['rush_attempt'] == 1, :]\\\n.groupby(['posteam', 'season', 'week'], as_index=False)['epa'].mean()\n\nrushing_defense_epa = data.loc[data['rush_attempt'] == 1, :]\\\n.groupby(['defteam', 'season', 'week'], as_index=False)['epa'].mean()\n\npassing_offense_epa = data.loc[data['pass_attempt'] == 1, :]\\\n.groupby(['posteam', 'season', 'week'], as_index=False)['epa'].mean()\n\npassing_defense_epa = data.loc[data['pass_attempt'] == 1, :]\\\n.groupby(['defteam', 'season', 'week'], as_index=False)['epa'].mean()\n\n# lag EPA one period back\nrushing_offense_epa['epa_shifted'] = rushing_offense_epa.groupby('posteam')['epa'].shift()\nrushing_defense_epa['epa_shifted'] = rushing_defense_epa.groupby('defteam')['epa'].shift()\npassing_offense_epa['epa_shifted'] = passing_offense_epa.groupby('posteam')['epa'].shift()\npassing_defense_epa['epa_shifted'] = passing_defense_epa.groupby('defteam')['epa'].shift()\n\n# In each case, calculate EWMA with a static window and dynamic window and assign it as a column \nrushing_offense_epa['ewma'] = rushing_offense_epa.groupby('posteam')['epa_shifted']\\\n.transform(lambda x: x.ewm(min_periods=1, span=10).mean())\n\nrushing_offense_epa['ewma_dynamic_window'] = rushing_offense_epa.groupby('posteam')\\\n.apply(dynamic_window_ewma).values\n\nrushing_defense_epa['ewma'] = rushing_defense_epa.groupby('defteam')['epa_shifted']\\\n.transform(lambda x: x.ewm(min_periods=1, span=10).mean())\n\nrushing_defense_epa['ewma_dynamic_window'] = rushing_defense_epa.groupby('defteam')\\\n.apply(dynamic_window_ewma).values\n\npassing_offense_epa['ewma'] = passing_offense_epa.groupby('posteam')['epa_shifted']\\\n.transform(lambda x: x.ewm(min_periods=1, span=10).mean())\n\npassing_offense_epa['ewma_dynamic_window'] = passing_offense_epa.groupby('posteam')\\\n.apply(dynamic_window_ewma).values\n\npassing_defense_epa['ewma'] = passing_defense_epa.groupby('defteam')['epa_shifted']\\\n.transform(lambda x: x.ewm(min_periods=1, span=10).mean())\n\npassing_defense_epa['ewma_dynamic_window'] = passing_defense_epa.groupby('defteam')\\\n.apply(dynamic_window_ewma).values\n\n#Merge all the data together\noffense_epa = rushing_offense_epa.merge(passing_offense_epa, on=['posteam', 'season', 'week'], suffixes=('_rushing', '_passing'))\\\n.rename(columns={'posteam': 'team'})\ndefense_epa = rushing_defense_epa.merge(passing_defense_epa, on=['defteam', 'season', 'week'], suffixes=('_rushing', '_passing'))\\\n.rename(columns={'defteam': 'team'})\nepa = offense_epa.merge(defense_epa, on=['team', 'season', 'week'], suffixes=('_offense', '_defense'))\n\n#remove the first season of data\nepa = epa.loc[epa['season'] != epa['season'].unique()[0], :]\n\nepa = epa.reset_index(drop=True)\n\nepa.head()\n  team  season  week  ...  epa_shifted_passing_defense  ewma_passing_defense  \\\n0  ARI    2000     1  ...                     0.269840              0.119870   \n1  ARI    2000     2  ...                    -0.069413              0.084280   \n2  ARI    2000     4  ...                     0.311383              0.126717   \n3  ARI    2000     5  ...                     0.500345              0.196184   \n4  ARI    2000     6  ...                     0.058499              0.170690   \n\n   ewma_dynamic_window_passing_defense  \n0                             0.119870  \n1                             0.084280  \n2                             0.126717  \n3                             0.196184  \n4                             0.170690  \n\n[5 rows x 19 columns]\n\nWe can plot EPA for the Green Bay Packers alongside our moving averages. We can see that the static window EMA and dynamic window EMA are quite similar, with slight divergences towards season ends.\n\ntm = epa.loc[epa['team'] == 'GB', :].assign(\n    season_week = lambda x: 'w' + x.week.astype(str) + ' (' + x.season.astype(str) + ')'\n).set_index('season_week')\n\nfig, ax = plt.subplots()\n\nloc = plticker.MultipleLocator(base=16) # this locator puts ticks at regular intervals\nax.xaxis.set_major_locator(loc)\nax.tick_params(axis='x', rotation=75) #rotate the x-axis labels a bit\n\nax.plot(tm['epa_shifted_passing_offense'], lw=1, alpha=0.5)\nax.plot(tm['ewma_dynamic_window_passing_offense'], lw=2)\nax.plot(tm['ewma_passing_offense'], lw=2);\nplt.axhline(y=0, color='red', lw=1.5, alpha=0.5)\n\nax.legend(['Passing EPA', 'EWMA on EPA with dynamic window', 'Static 10 EWMA on EPA'])\nax.set_title('GB Passing EPA per play')\nplt.show();\n\n\nNow that we have our features compiled, we can begin to merge in game result data to come up with our target variable.\n\nschedule = data[['season', 'week', 'home_team', 'away_team', 'home_score', 'away_score']]\\\n.drop_duplicates().reset_index(drop=True)\\\n.assign(home_team_win = lambda x: (x.home_score > x.away_score).astype(int))\n\ndf = schedule.merge(epa.rename(columns={'team': 'home_team'}), on=['home_team', 'season', 'week'])\\\n.merge(epa.rename(columns={'team': 'away_team'}), on=['away_team', 'season', 'week'], suffixes=('_home', '_away'))\n\ndf.head()\n   season  week home_team  ... epa_shifted_passing_defense_away  \\\n0    2000     1       NYG  ...                         0.269840   \n1    2000     1       PIT  ...                        -0.208568   \n2    2000     1       WAS  ...                        -0.368611   \n3    2000     1       MIN  ...                         0.365886   \n4    2000     1        LA  ...                        -0.078273   \n\n   ewma_passing_defense_away  ewma_dynamic_window_passing_defense_away  \n0                   0.119870                                  0.119870  \n1                  -0.197401                                 -0.197401  \n2                   0.001883                                  0.001883  \n3                   0.106074                                  0.106074  \n4                  -0.089393                                 -0.089393  \n\n[5 rows x 39 columns]\n\nWe’ll set some variables to isolate our target and features. Here, we are only including the dynamic window columns. I’ve found that the dynamic window does produce a slightly better model score. You’re welcome to use the EWMA features with the static window size.\n\ntarget = 'home_team_win'\nfeatures = [column for column in df.columns if 'ewma' in column and 'dynamic' in column]\nfor feature in features:\n  print(feature)\newma_dynamic_window_rushing_offense_home\newma_dynamic_window_passing_offense_home\newma_dynamic_window_rushing_defense_home\newma_dynamic_window_passing_defense_home\newma_dynamic_window_rushing_offense_away\newma_dynamic_window_passing_offense_away\newma_dynamic_window_rushing_defense_away\newma_dynamic_window_passing_defense_away\n\nHere, we finally train and test our model. In sklearn, each supervised algorithm is implemented in the same fashion, First, you bring in your class (which we did earlier in the code). You then instantiate the class, providing model hyperparameters at instantiation. Here, since we’re just doing Logistic Regression, we have none (although sklearn allows us to provide C as a hyperpameter - which controls regularization). Then, you call the fit method which trains your model. For evaluating model accuracy, you have a couple options. Notice here we did not split our model in to train and test sets, as we’ll be using 10-fold cross validation to train and test instead, using the cross_val_scores function we brought in earlier.\n\ndf = df.dropna()\n\nX = df.loc[df['season'] != 2020, features].values\ny = df.loc[df['season'] != 2020, target].values\n\nclf = LogisticRegression()\nclf.fit(X, y)\nLogisticRegression()\naccuracy_scores = cross_val_score(clf, X, y, cv=10)\nlog_losses = cross_val_score(clf, X, y, cv=10, scoring='neg_log_loss')\n\nprint('Model Accuracy:', np.mean(accuracy_scores))\nModel Accuracy: 0.6352219667367291\nprint('Neg log loss:', np.mean(log_losses))\nNeg log loss: -0.6384736044609878\n\n\nfig, ax = plt.subplots()\n\nfeature_names = ['_'.join(feature_name.split('_')[3:]) for feature_name in features]\n\ncoef_ = clf.coef_[0]\n\nfeatures_coef_sorted = sorted(zip(feature_names, coef_), key=lambda x:x[-1], reverse=True)\n\nfeatures_sorted = [feature for feature, _ in features_coef_sorted]\ncoef_sorted = [coef for _, coef in features_coef_sorted]\n\nax.set_title('Feature importance')\n\nax.barh(features_sorted, coef_sorted);\nplt.show();\n\n\nWe see our model has about a 63.5% accuracy score. Not terrible, considering if you head over to nflpickwatch.com, you’ll see that the best experts tend to cluster around 68%.\nWe also found the negative log loss. This is important because the model could have a lot of value as a win probability model as opposed to a straight pick’em model.\nThe model can definitely be improved. Some possible improvements and things I’d like to explore in future iterations of this model:\nUsing John Goldberg’s idea of opponent-adjusted EPA\nWeighing EPA by win probability\nEngineering other features beyond EPA, including special teams performance\nTry out more complex models + tuning\nSomething fun we can do now is see how the model would have predicted this past season. Notice we took out 2020 data from our training data, and so now we are free to see how the model would have done in 2020 (since none of the 2020 data was used for training).\n\ndf_2020 = df.loc[(df['season'] == 2020)].assign(\n    predicted_winner = lambda x: clf.predict(x[features]),\n    home_team_win_probability = lambda x: clf.predict_proba(x[features])[:, 1]\n)\\\n[['home_team', 'away_team', 'week', 'predicted_winner', 'home_team_win_probability', 'home_team_win']]\n\ndf_2020['actual_winner'] = df_2020.apply(lambda x: x.home_team if x.home_team_win else x.away_team, axis=1)\ndf_2020['predicted_winner'] = df_2020.apply(lambda x: x.home_team if x.predicted_winner == 1 else x.away_team, axis=1)\ndf_2020['win_probability'] = df_2020.apply(lambda x: x.home_team_win_probability if x.predicted_winner == x.home_team else 1 - x.home_team_win_probability, axis=1)\ndf_2020['correct_prediction'] = (df_2020['predicted_winner'] == df_2020['actual_winner']).astype(int)\n\ndf_2020 = df_2020.drop(columns=['home_team_win_probability', 'home_team_win'])\n\ndf_2020.sort_values(by='win_probability', ascending=False).reset_index(drop=True).head(10)\n  home_team away_team  week predicted_winner actual_winner  win_probability  \\\n0        KC       NYJ     8               KC            KC         0.874193   \n1        KC       DEN    13               KC            KC         0.845162   \n2        LA       NYJ    15               LA           NYJ         0.833220   \n3        LA       NYG     4               LA            LA         0.832061   \n4        SF       PHI     4               SF           PHI         0.819993   \n5       MIA       NYJ     6              MIA           MIA         0.802636   \n6       BAL       CLE     1              BAL           BAL         0.794298   \n7        KC       CAR     9               KC            KC         0.793608   \n8       BAL       CIN     5              BAL           BAL         0.779855   \n9       IND       JAX    17              IND           IND         0.773426   \n\n   correct_prediction  \n0                   1  \n1                   1  \n2                   0  \n3                   1  \n4                   0  \n5                   1  \n6                   1  \n7                   1  \n8                   1  \n9                   1  \n\nThese are the 10 games this season the model was most confident about. No surprises here that the KC-NYJ game was the most lopsided game this season.\nWe can also view how our model would have done this season by week. There doesn’t seem to be a clear trend here.\nYou would expect that maybe the model would get better as the season went on, but the data doesn’t make that clear here.\n\ncorrect = df_2020.loc[df_2020['correct_prediction'] == 1].groupby('week')['correct_prediction'].sum()\n\nnum_games = df_2020.groupby('week')['correct_prediction'].size()\n\nresults = correct / num_games\n\nresults\nweek\n1     0.750000\n2     0.875000\n3     0.500000\n4     0.733333\n5     0.714286\n6     0.571429\n7     0.714286\n8     0.571429\n9     0.500000\n10    0.785714\n11    0.714286\n12    0.687500\n13    0.466667\n14    0.562500\n15    0.687500\n16    0.687500\n17    0.750000\n18    0.666667\n19    0.750000\n20    0.500000\nName: correct_prediction, dtype: float64\n\nThe model would have done the best in week 2 of the 2020 season, with a 87.5% accuracy score (14-2)!\n\nprint(df_2020.loc[df_2020['week'] == results.idxmax()].sort_values(by='win_probability', ascending=False))\n     home_team away_team  week predicted_winner actual_winner  \\\n5341        TB       CAR     2               TB            TB   \n5339       HOU       BAL     2              BAL           BAL   \n5345       TEN       JAX     2              TEN           TEN   \n5344        GB       DET     2               GB            GB   \n5353       ARI       WAS     2              ARI           ARI   \n5338       DAL       ATL     2              DAL           DAL   \n5351       CHI       NYG     2              CHI           CHI   \n5343       PIT       DEN     2              PIT           PIT   \n5349       SEA        NE     2              SEA           SEA   \n5346       LAC        KC     2               KC            KC   \n5342       CLE       CIN     2              CLE           CLE   \n5340       MIA       BUF     2              BUF           BUF   \n5352       NYJ        SF     2               SF            SF   \n5350        LV        NO     2               NO            LV   \n5348       IND       MIN     2              IND           IND   \n5347       PHI        LA     2              PHI            LA   \n\n      win_probability  correct_prediction  \n5341         0.763676                   1  \n5339         0.743131                   1  \n5345         0.740741                   1  \n5344         0.733028                   1  \n5353         0.708931                   1  \n5338         0.694735                   1  \n5351         0.645196                   1  \n5343         0.640730                   1  \n5349         0.636207                   1  \n5346         0.596137                   1  \n5342         0.556421                   1  \n5340         0.556119                   1  \n5352         0.553112                   1  \n5350         0.532511                   0  \n5348         0.520469                   1  \n5347         0.515421                   0  \n\nWe can also view how our model would have done in the playoffs by filtering out weeks prior to week 17.\n\ndf_2020.loc[df_2020['week'] > 17]\n     home_team away_team  week predicted_winner actual_winner  \\\n5578       TEN       BAL    18              BAL           BAL   \n5579        NO       CHI    18               NO            NO   \n5580       PIT       CLE    18              PIT           CLE   \n5581       BUF       IND    18              BUF           BUF   \n5582       SEA        LA    18              SEA            LA   \n5583       WAS        TB    18               TB            TB   \n5584       BUF       BAL    19              BUF           BUF   \n5585        KC       CLE    19               KC            KC   \n5586        GB        LA    19               GB            GB   \n5587        NO        TB    19               NO            TB   \n5588        KC       BUF    20               KC            KC   \n5589        GB        TB    20               GB            TB   \n\n      win_probability  correct_prediction  \n5578         0.551507                   1  \n5579         0.755613                   1  \n5580         0.513341                   0  \n5581         0.632864                   1  \n5582         0.589866                   0  \n5583         0.614521                   1  \n5584         0.527067                   1  \n5585         0.587855                   1  \n5586         0.689657                   1  \n5587         0.597603                   0  \n5588         0.530803                   1  \n5589         0.593343                   0  \n\nAnd of course, an NFL game prediction post a couple days out from the super bowl isn’t complete without a super bowl prediction.\n\nimport itertools\n\ndef ewma(data, window):\n    \"\"\"\n    Calculate the most recent value for EWMA given an array of data and a window size\n    \"\"\"\n    alpha = 2 / (window + 1.0)\n    alpha_rev = 1 - alpha\n    scale = 1 / alpha_rev\n    n = data.shape[0]\n    r = np.arange(n)\n    scale_arr = scale**r\n    offset = data[0] * alpha_rev**(r+1)\n    pw0 = alpha * alpha_rev**(n-1)\n    mult = data * pw0 * scale_arr\n    cumsums = mult.cumsum()\n    out = offset + cumsums * scale_arr[::-1]\n    return out[-1]\n\ndata_2020 = data.loc[(data['season'] == 2020)]\noffense = data_2020.loc[(data_2020['posteam'] == 'KC') | (data_2020['posteam'] == 'TB')]\ndefense = data_2020.loc[(data_2020['defteam'] == 'KC') | (data_2020['defteam'] == 'TB')]\n\nrushing_offense = offense.loc[offense['rush_attempt'] == 1]\\\n.groupby(['posteam', 'week'], as_index=False)['epa'].mean().rename(columns={'posteam': 'team'})\npassing_offense = offense.loc[offense['pass_attempt'] == 1]\\\n.groupby(['posteam', 'week'], as_index=False)['epa'].mean().rename(columns={'posteam': 'team'})\nrushing_defense = defense.loc[defense['rush_attempt'] == 1]\\\n.groupby(['defteam', 'week'], as_index=False)['epa'].mean().rename(columns={'defteam': 'team'})\npassing_defense = defense.loc[defense['pass_attempt'] == 1]\\\n.groupby(['defteam', 'week'], as_index=False)['epa'].mean().rename(columns={'defteam': 'team'})\n\nsuper_bowl_X = np.zeros(8)\n\nfor i, (tm, stat_df) in enumerate(itertools.product(['KC', 'TB'], [rushing_offense, passing_offense, rushing_defense, passing_defense])):\n    ewma_value = ewma(stat_df.loc[stat_df['team'] == tm]['epa'].values, 20)\n    super_bowl_X[i] = ewma_value\n\npredicted_winner = clf.predict(super_bowl_X.reshape(1, 8))[0]\npredicted_proba = clf.predict_proba(super_bowl_X.reshape(1, 8))[0]\n\nwinner = 'KC' if predicted_winner else 'TB'\nwin_prob = predicted_proba[-1] if predicted_winner else predicted_proba[0]\n\nprint(f'Model predicts {winner} will win the Super Bowl and has a {round(win_prob*100, 2)}% win probability')\nModel predicts KC will win the Super Bowl and has a 53.96% win probability\n\nFinal Thoughts\nThe goal of this post was to provide you with a baseline model to work from in Python that can be easily iterated upon. This is the first Python modeling post on Open Source Football, and so if this is your first time working with Python’s sklearn, hopefully you learned something.\nThe model came out to be around 63.5% accurate, and can definitely be improved on. I think the model may also have more value as a pre-game win probability model.\nIn future posts, I would like to incorporate EPA adjusted for opponent and also look at how WP affects the predictability of EPA, as it’s an interesting idea and I think there could be some value there. Also, trying out other features besides EPA could certainly help. Turnover rates, time of possession, average number of plays ran, average starting field position, special teams performance, QB specific play, etc., could all prove useful. We saw in the visualization of feature importance that passing EPA per play had the most predictive power for a home team win and away team win. Exploring QB specific features could help improve the model. Of course, that would require roster data for each week.\nWith that said, reach out to me on Twitter (@bendominguez011) if you have any thoughts or ideas on improving the model. And thank you for reading!\n\n\n\n",
    "preview": "posts/2021-01-21-nfl-game-prediction-using-logistic-regression/nfl-game-prediction-using-logistic-regression_files/figure-html5/epa_plot-1.png",
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {},
    "preview_width": 10800,
    "preview_height": 7200
  },
  {
    "path": "posts/2021-01-21-exploring-stability-and-predictive-power-of-penalties-in-the-nfl/",
    "title": "Exploring Stability and Predictive Power of Penalties in the NFL",
    "description": "Use nflfastR to calculate penalty EPA and understand the randomness of penalties to improve prediction accuracy of game outcomes",
    "author": [
      {
        "name": "Jack Lichtenstein",
        "url": "https://twitter.com/jacklich10"
      }
    ],
    "date": "2021-01-21",
    "categories": [
      "nflfastR",
      "EPA",
      "Penalty EPA",
      "Prediction"
    ],
    "contents": "\n\nContents\nLoading in the data\nExploring stability of penalties\nPenalty EPA methodology\nLeveraging instability of penalties for prediction\nFinal Thoughts\n\nIn the @nflfastR era (1999-2020), there have been over 64,000 penalties called! Penalties called in high leverage situations can have huge influences on game outcomes and also advanced metrics like expected points added (EPA). One of the reasons that many highly regarded power ranking systems had the 2020 New Orleans Saints very highly rated was due to their “unluckiness” with penalty rates. This article will explore how stable penalties are across teams. Are certain penalty types a reflection of a team’s underlying talent/skill (thus being useful for out of sample prediction), or are they simply random noise that does not predict future performance? If we find that certain penalties are stable for teams across a season, we can probably find a way to incorporate the EPA lost or gained from penalties to develop better predictive measures.\nLoading in the data\nLet’s start by first reading in play-by-play data from 1999-2020 from @nflfastR.\n\n\nnfl_pbp <- purrr::map_df(1999:2020, function(x) {\n  readr::read_csv(\n    glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.csv.gz\")\n  ) %>%\n    dplyr::select(\n      game_id, play_id, season, week, home_team, away_team, roof,\n      half_seconds_remaining, yardline_100, ydstogo, posteam, defteam,\n      desc, play_type, qb_dropback, qb_kneel, qb_spike, rush_attempt, pass_attempt,\n      ep, epa, contains(\"penalty\"), yards_gained, down,\n      posteam_timeouts_remaining, defteam_timeouts_remaining,\n      -c(penalty_player_id, penalty_player_name, first_down_penalty)\n    ) %>%\n    # change penalty yards to negative yardage if it is on the offense\n    dplyr::mutate(penalty_yards = ifelse(penalty_team == posteam, -penalty_yards, penalty_yards))\n})\n\n\n\nExploring stability of penalties\nSince penalties are relatively rare events, especially when grouping by specific penalty types, we will bucket each team and penalty type into quarter seasons (a 4 game window) and then find the correlation between quarter season penalty percentages across teams during a season.\nNote that we group Encroachment, Defensive Offside and Neutral Zone Infraction penalties as all Defensive Offsides penalties, Illegal Contact, Defensive Holding and Defensive Pass Interference penalties as all Defensive Pass Interference and Illegal Shift, Illegal Formation and Illegal Motion as all Illegal Shift penalties, among other groupings.\n\n\n\n\n\n\nThese results are fairly intuitive! Immediately, we see that the less subjective penalties like offensive False Start and Defensive Offside penalties are the most stable penalty types in football. In other words, the number of False Start penalties an offense commits probably reflect some measure of team skill or discipline that might be helpful in predicting future outcomes. We also notice, that for the most part, offenses control penalties more than defenses. For example, while obviously penalizing a defense, Defensive Too Many Men on Field penalties are controlled more by the offense than the defense! After all, defenses are inherently reactionary to their offensive counterparts.\nGenerally, however, penalties are extremely fluky and unstable metrics for teams, especially for defenses (other than Defensive Offsides), and especially when we take into account how penalty rates have changed over time. As such, how can we take this information into account for prediction. Should we be rewarding defenses with negative EPA (negative is good for defense) when the offense commits a False Start or Delay of Game, for example? Probably not. If we are going to predict future game outcomes, it might be smart to evaluate a team in a sort of “penalty-free” environment (save for the few penalties that might carry some signal for future performance).\nPenalty EPA methodology\nIn the @nflfastR database, it is important to note that the expected points added value does not account for a nullified play by penalty. In other words, a long rush that is called back on an Offensive Holding penalty will be negative EPA, and in some sense, we lose the EPA of the long rush had it not been called back.\nThus, for each play with a penalty, we will calculate the EPA of the nullified play using the nflfastR::calculate_expected_points() function. Then, we can compare this value to the actual observed EPA value from the penalty. This new value, called penalty_epa, is the difference between the EPA a team would have gained/lost if there was no penalty and the actual observed EPA from the penalty. Finally, we calculate no_penalty_epa, which is the difference between the original EPA and penalty EPA, representing the EPA gained/lost in a “penalty-free” world.\nNote that we are not going to look at any special teams penalties. The code for calculating penalty_epa and no_penalty_epa can be found in the source code. If these values are something that interests the community, maybe they can be added to the @nflfastR play by play dataset over the off-season.\nFirst, let’s examine the distributions of penalty EPA by penalty type as a sanity check. These distributions look pretty good and match our expectations. For example, Defensive Pass Interference penalties are positive EPA while Offensive Holding penalties are negative EPA. Notice that there are a couple observations in many penalty types where penalty EPA does not match our expectations. I have tried to fix these edge cases the best I can, but also note that an Illegal Shift penalty that negates a 3rd down failed conversion would actually yield positive EPA for an offense since the down is replayed, giving the offense another chance at conversion.\n\n\n\nWe can also examine the difference between the observed EPA and penalty-free EPA on plays with penalties by simply looking at the distributions of each. This is another way we can see that EPA on plays with penalties is highly volatile! The distribution of observed EPA has large outliers and is almost bimodal around zero (EPA on penalties is often extremely positive or extremely negative due to automatic first downs and large yardage lost or gained). These penalties can have massive influences on games! The “penalty-free” EPA smooths out a lot of this variability, as it pulls EPA towards zero. This is expected, as penalty-free EPA should align with normal values of EPA where most plays do not have significant swings in expected points.\n\n\n\nFinally, to understand the penalty_epa and no_penalty_epa values better on an individual basis, let’s look at two plays from the Seattle Seahawks @ Arizona Cardinals game from week 7 of 2020 as an example:\n\nOff\n      Def\n      \n      Dist. to Endzone\n      desc\n      Penalty\n      EPA\n      No Penalty EPA\n      Penalty EPA\n    \n      \n      3&5\n      88\n      1-K.Murray pass incomplete short middle to 85-D.Arnold on SEA-54-B.Wagner, Unnecessary Roughness, 15 yards, enforced at ARI 12 - No Play.\n      Unnecessary Roughness\n      2.21\n      −1.48\n      3.69\n    \n      \n      3&10\n      48\n      3-R.Wilson pass short left to 14-D.Metcalf for 48 yards, TOUCHDOWN NULLIFIED by Penalty on SEA-83-D.Moore, Offensive Holding, 10 yards, enforced at ARI 42.\n      Offensive Holding\n      −0.36\n      4.72\n      −5.08\n    Table: @jacklich10 | Data: @nflfastR\n    \n\nIn the first play, Arizona had a 3rd and 5 on their own 12 yard line when Bobby Wagner was called for a questionable Unnecessary Roughness penalty. This play generated around 2.21 expected points. Using our methodology, we can see that had no penalty been called, the play would have lost about 1.48 expected points. Therefore, we can conclude that the penalty generated roughly 3.69 expected points (2.21 - (-1.48)) on its own!\nThe second play is an Offensive Holding call that cost the Seahawks more than 5 expected points when DK Metcalf scored a long touchdown on 3rd and 10 that was called back.\nLeveraging instability of penalties for prediction\nIt is fair to wonder from the two plays above, did the Cardinals really earn that first down or earn the Offensive Holding call on defense, or was it simply a bit of randomness that swung their way? With regard to predicting future performance, it might be best to look at no_penalty_epa rather than epa, as it hopefully can do a better job of teasing out the fluky-ness of the penalties.\nIn order to leverage this information, we will try to predict game outcomes using the methodology I outlined in my previous post. In short, we will convert each EPA statistic into a lagging moving average using a dynamic window that starts at 10 games. The lag ensures that we compare a team’s performance against their opponent’s performance up to that point in the season. The dynamic window works such that we will use a ten game window to predict the winner of the 11th game in a season, but for say the 15th game in a season, we will use a 14 game window. We will also make use of exponential and running moving averages for different EPA measures, as we found these to be the most predictive versions of EPA in the post.\nAs a baseline, we will explore the predictive power of EPA before altering EPA for penalties as well as the predictive power of EPA in a completely “penalty-free” environment. Then, we will play around with different ways of manipulating penalty EPA to try to come up with more predictive measures. For example, we will try to penalize offenses for False Start penalties (recall these are the most stable) while living in a “penalty-free” world for all other penalties.\nNote that pre-snap penalties like False Start and Delay of Game in the @nflfastR database occur when qb_dropback is zero - as such, they will fall under “Rush EPA”.\n\n\n\nComparing the predictive power of baseline EPA to “penalty-free” EPA, we notice that:\nPenalties (at least certain types) do carry some signal, as we lose predictive power when removing them from the equation\nEliminating the effects of penalties for offenses, especially pre-snap penalties, significantly decreases its predictive power when there is no QB dropback (Rush EPA)\nEliminating the effects of penalties for defenses mostly increases its predictive power (this aligns with what we saw earlier - that defenses rarely control their penalty rates)\nThis initial exploration gels nicely with our discoveries earlier, mainly that offenses control penalty rates more than defenses. As such, removing penalties from the equation causes offensive strength metrics to lose signal and defensive strength metrics to gain signal by eliminating some noise.\nNext, let’s try to only account for penalties that we know are at least somewhat stable for teams over time. Specifically, we will account for False Start, Offensive Holding, and Defensive Offside penalties for the offense as well as account for Defensive Offside and Defensive Pass Interference penalties for the defense. To be clear, this means that for the penalties listed above, we will use the observed EPA, while on all other penalties we will use our calculated “penalty-free” EPA.\n\n\n\nAwesome! While we failed to achieve the predictive power of baseline rush EPA for offenses, in almost every other category of predictor we increased the signal of EPA metrics, including both total offense and total defense EPA. In other words, when we only use the observed EPA for the more stable penalty types outlined above and use the calculated EPA in a “penalty-free” world otherwise, we can better predict future performance!\n\n\n\nFor fun, let’s examine the offenses that were helped most by opposing defensive penalties as well as defenses that were helped most by opposing offensive penalties in 2020. We see that Tampa Bay’s and Philadelphia’s offenses were helped out most by opposing defensive penalties while Dallas’ and Jacksonville’s defenses were helped by opposing offensive penalties.\n\n\n\n\n\n\nSimilarly, a look at the offenses that hurt themselves the most with penalties as well as defenses that hurt themselves the most with penalties. We see that Minnesota and Kansas City’s offenses lost the most EPA from offensive penalties while Tampa Bay’s and Jacksonville’s defenses lost the most EPA from defensive penalties.\nNote that for an offense like the Jets, while they lost a marginal amount of EPA from their own offensive penalties, this is primarily due to the fact that their penalties did not negate any explosive plays. This is in contrast to a team like the Patriots, who lost the least EPA from their own offensive and defensive penalties due to committing very few penalties in the first place.\n\n\n\n\n\n\nFinal Thoughts\nWhile I played around with various configurations of penalty types to include/not include as signal or noise, this was not a rigorous process. It is possible that a different approach to handling penalty EPA will yield greater strides in predictive power. For example, I played around with weighting observed EPA and “penalty-free” EPA for certain penalty types, though did not find any predictive benefits. One could also experiment with regressing “penalty-free” EPA differently depending on penalty type. Maybe for more stable penalties like False Starts, “penalty-free” EPA should be regressed slightly, while flukier penalties could be regressed more.\nGenerally, due to the rarity and volatility of penalties, adjusting EPA metrics to improve predictive power on plays with penalties is very difficult. Further, as we saw from the stability of team penalty percentages over time, the NFL is non-stationary. It is possible that certain penalty types will become more or less stable over time, and there is always the possibility of NFL rule changes or points of emphasis (like calling very few Holding penalties in 2020) that alter the NFL landscape. Hopefully, the penalty EPA methodology can be refined and explored further by the community in the future.\n\nView source code on GitHub \n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-11-hfa-analysis/",
    "title": "Analyzing Home Field Advantage in the NFL",
    "description": "Multi-season analysis of league and team-specific home-field advantage in the NFL.",
    "author": [
      {
        "name": "Adrian Cadena",
        "url": "https://adriancadena.com/"
      }
    ],
    "date": "2021-01-11",
    "categories": [
      "nflfastR",
      "Articles",
      "Home Field Advantage"
    ],
    "contents": "\n\nContents\nIntroduction\nLeague home field advantage vs. team-specific home field advantage\nAdjusting home field advantage\nHow does home field advantage behave?\nSummary\nMethodology\nFuture research\nReferences\n\nIntroduction\nThe sports betting industry and their enthusiasts continually try to find the correct value of a home field advantage (HFA) in terms of “point-spread” to create betting lines. In particular, NFL bookmakers tend to reward a home field with three points on average; this varies across different home teams per season. The idea is that playing at home is considered an advantage in the NFL, and some teams have a stronger HFA than others. Interested in this topic, I decided to use nflfastR and PFF data to quantify HFA advantage for the league and each team. I also leveraged the opportunity to analyze the behavior of HFA across seasons.\nLeague home field advantage vs. team-specific home field advantage\nAgreeing with consensus, I found the league home field advantage (LHFA) during 2006-2020 to be approximately three points (2.85). However, I found it to be fluid, so a 2.85 LHFA has not been true for every season during that period. In addition to LHFA, I calculated Team-Specific Home Field Advantage (iHFA), a concept suggested by Takoma Park. iHFA represents the additional or fewer points that each team has in terms of HFA compared to the league average. For example, the Ravens had an estimated iHFA of 0.26 points during 2006-2019; meanwhile, the Raiders had -0.37 points below league HFA.\nWhen we add together LHFA and iHFA, we obtain the true HFA for each team. There have been seasons where iHFA is not statistically significant, meaning the difference in iHFA between teams is not meaningful. When that happens, each team receives an iHFA of zero, and their HFA equals the LHFA. iHFA is normally-distributed around a mean of zero during a given period. In other words, the positive and negative iHFA’s for all teams add up to zero during the analyzed period.\nEvidence suggests that LHFA has been shrinking, and iHFA is volatile and unpredictable year over year. These findings are consistent with Robby Greer’s.\nAdjusting home field advantage\nWhen thinking about strong home fields, most people think about Seattle, Pittsburgh, New Orleans, etc. However, such perceptions are biased by the success that those teams have had regardless of their home/away status. Suppose we were to build a model using only home_team and away_team as variables. With that approach, we would end up with the expected teams at the top (NE, PIT, SEA, etc.), similar to common perception. However, this methodology is flawed since our results would capture team strength rather than an HFA. The way to get the real value of a home field is by controlling for variables such as:\nStrength of team and opponent\nCompetence of the head coach and opposing head coach\nQuarterback and opposing quarterback performance\nHow each team performs home vs. away\nAfter adjusting for the listed variables, I found the real value of LHFA and iHFA. Unsurprisingly, these adjustments decreased iHFA for teams with successful head coaches and elite quarterbacks like NE, NO, and SEA. Overperformance by weak teams, bad QB’s, and incompetent HC’s is then explained by the home/away variable. A chart displaying HFA for each team, along with confidence intervals, during 2006-2020 is presented below.\nFigure 1.How does home field advantage behave?\nSports analysts and betting enthusiasts have speculated about a possible shrinkage of LHFA during the last couple of years. In 2019, away teams slightly outscored home teams: 5858 to 5822 during the regular season. In 2020, home teams slightly outscored away teams: 6353 to 6339. Besides the shrinking theory, LHFA appears to have a cyclical behavior, meaning it changes year over year in an “up and down” fashion. The following chart shows how LHFA has moved since 2006 until reaching a negative value in 2019 and then moving back up to a low positive value in 2020.\nFigure 2.Similar to LHFA, iHFA for a given team is not the same across seasons. For example, according to my model, the Panthers were a better home team in 2006 (+2.4 points) than in 2007 (+0 points). iHFA appears to be very unpredictable and almost random on a year over year basis. The following chart presents a two-team example of iHFA behavior (Arizona Cardinals and Philadelphia Eagles).\nFigure 3.Summary\nThis is a list of observations and findings informed by my analysis:\n1- League Home Field Advantage (LHFA) is fluid year over year and appears to have a cyclical behavior.\n2- LHFA being cyclical does not mean it is unpredictable. Time series algorithms could capture cyclicity if any, but a larger sample size would be required to capture this.\n3- Since 2017, LHFA has shrunk (presented a downward trend), but that doesn’t mean it can’t go up again due to its apparent cyclicity\n4- In a larger sample size, LHFA appears to be approximately three points\n5- Team-Specific Home Field Advantage (iHFA) increase or decrease the effect of LHFA in a given season for each team\n6- iHFA seems to be volatile and unpredictable. Future analysis could determine if one or more iHFA’s is statistically better than the rest on a multi-season basis.\nJust for fun, here is true HFA for each team during the 2018 season, which presented significant iHFA among teams. Neither 2019 nor 2020 had significant iHFA; therefore, every team had the same HFA during those two seasons: -0.7 and 0.4 respectively.\nFigure 4.Methodology\nMy response variable was point differential, which is the standard practice. Adjusting for different variables is not a novel approach; in fact, we wouldn’t get reasonable HFA estimates if we didn’t do that. My approach’s novelty lies in analyzing each team in a given game separately, meaning two observations per game instead of one. This methodology allowed me to compare how each team performs home vs. away, ceteris paribus. To get LHFA, I used a fixed-effect binary variable, “home.” To get iHFA, I used a random slope (home) within the group (team), making it (0+home|team). I used fixed effects to control for strength of team/opponent, quarterback/opposing quarterback, and head coach/opposing head coach. A description of the fixed-effect variables is listed below:\n• Strength of team and opponent: used PFF season grades for each team\n• Quarterback play: used PFF game grades for each quarterback in each game\n• Head coach/opposing head coach index: used nflfastR data to build a mixed-effects model. Controlled for different variables and head coach a random effect. Took each head coach’s intercept and re-scaled it 0-100 to generate a grade.\nFuture research\nThings I would like to analyze in the future:\nEvidence and previous work suggest that home-field benefits offense and defense differently. It is plausible to study which side of the ball is benefited the most across seasons and for every team\nTry a different approach to see if one teams’ iHFA’s can be significantly and continuously better/worse than the others during a more extended period\nTest for LHFA cyclicity using a larger sample size\nAnalyze a home field effect on penalties\nControl for other variables: rest time, distance, elevation, divisional opponent, elevation.\nReferences\nGreer, Robby. “An Initial Exploration of Home Field Advantage in the NFL.” Robbygreer.com, Robbygreer.com, 21 Jan. 2020, https://www.nfeloapp.com/analysis/an-initial-exploration-of-home-field-advantage-in-the-nfl.\nPark, Takoma. “Estimating NFL Team-Specific Home-Field Advantage.” Dart, 24 Jan. 2015, dartthrowingchimp.wordpress.com/2015/01/24/estimating-nfl-team-specific-home-field-advantage/.\nWalczak, Sam. “My Model Monday: NFL Home-Field Advantage.” Model 284, 22 Jan. 2018, model284.com/nfl-home-field-advantage/.\nSpecial thanks to Robby Greerre @ greerreNFL and Abhijit Brahme @ captain_abhious for their comments and suggestions.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-29-exploring-rolling-averages-of-epa/",
    "title": "Exploring Rolling Averages of EPA",
    "description": "Use nflfastR to develop lagged rolling averages of EPA to best predict a game's outcome",
    "author": [
      {
        "name": "Jack Lichtenstein",
        "url": "https://twitter.com/jacklich10"
      }
    ],
    "date": "2020-12-29",
    "categories": [
      "nflfastR",
      "EPA",
      "Adjusting EPA",
      "Rolling EPA",
      "Prediction"
    ],
    "contents": "\n\nContents\nLoading in the data\nMethodology\nAnalyzing predictive power\nExploration\nFinal Thoughts\n\nThis post is inspired by a brief twitter thread between Lee Sharpe and Robby Greer as well as Jonathan Goldberg’s previous post on Open Source Football that adjusts EPA/play for opponent using 10 game rolling windows. The goal of this article is to alter EPA/play by adjusting for opponent as well as to determine the best rolling average window to maximize the predictive power of future game outcomes.\nLoading in the data\nLet’s start by first reading in play-by-play data from 1999-2020 from @nflfastR.\n\n\nnfl_pbp <- purrr::map_df(1999:2020, function(x) {\n  readr::read_csv(\n    glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.csv.gz\")\n  ) %>%\n    dplyr::select(-starts_with(\"blocked_\"))\n})\n\n\n\nMethodology\nJust as Jonathan did in his post, we will find every team’s weekly EPA/play on offense and defense. Additionally, instead of simply finding total offense and defense EPA/play, we will break it down into passing and rushing EPA/play for offense and defense.\n\n\n### Get game EPA data\n# Offense EPA\nepa_data <- nfl_pbp %>%\n  dplyr::filter(\n    !is.na(epa), !is.na(ep), !is.na(posteam),\n    play_type == \"pass\" | play_type == \"run\" | penalty == 1, qb_kneel != 1\n  ) %>%\n  dplyr::group_by(game_id, season, week, posteam, home_team) %>%\n  dplyr::summarise(\n    off_dropback_pct = mean(qb_dropback == 1),\n    off_epa = mean(epa),\n    off_pass_epa = mean(epa[qb_dropback == 1]),\n    off_rush_epa = mean(epa[qb_dropback == 0]),\n    off_epa_n = sum(qb_dropback == 1 | qb_dropback == 0),\n    off_pass_epa_n = sum(qb_dropback == 1),\n    off_rush_epa_n = sum(qb_dropback == 0),\n    .groups = \"drop\"\n  ) %>%\n  # Defense EPA\n  dplyr::left_join(nfl_pbp %>%\n    filter(\n      !is.na(epa), !is.na(ep), !is.na(posteam),\n      play_type == \"pass\" | play_type == \"run\" | penalty == 1, qb_kneel != 1\n    ) %>%\n    dplyr::group_by(game_id, season, week, defteam, away_team) %>%\n    dplyr::summarise(\n      def_epa = mean(epa),\n      def_dropback_pct = mean(qb_dropback == 1),\n      def_pass_epa = mean(epa[qb_dropback == 1]),\n      def_rush_epa = mean(epa[qb_dropback == 0]),\n      def_epa_n = sum(qb_dropback == 1 | qb_dropback == 0),\n      def_pass_epa_n = sum(qb_dropback == 1),\n      def_rush_epa_n = sum(qb_dropback == 0),\n      .groups = \"drop\"\n    ),\n  by = c(\"game_id\", \"posteam\" = \"defteam\", \"season\", \"week\")\n  ) %>%\n  dplyr::mutate(opponent = ifelse(posteam == home_team, away_team, home_team)) %>%\n  dplyr::select(\n    game_id, season, week, home_team, away_team, posteam, opponent,\n    off_dropback_pct, off_epa, off_pass_epa, off_rush_epa,\n    off_epa_n, off_pass_epa_n, off_rush_epa_n,\n    def_epa_n, def_pass_epa_n, def_rush_epa_n,\n    def_dropback_pct, def_epa, def_pass_epa, def_rush_epa\n  ) %>%\n  # Not sure why, but there is one instance where the posteam = \"\"\n  dplyr::filter(posteam != \"\")\n\n\n\nWe are going to build our dataset in a very similar manner to what Jonathan did in his post, with a few changes outlined below:\nConvert each EPA statistic into a lagging moving average. The lag ensures that we compare a team’s performance against their opponent’s performance up to that point in the season.\nInstead of weighting each game equally during the window, we will weight EPA by the number of plays in each game of the window.\nInstead of simply converting each statistic into a moving average of the last ten games, we will convert each statistic into a moving average using a dynamic window that ranges from ten games to twenty games (for teams that play in the Super Bowl). In other words, we will use a ten game window to predict the winner of the 11th game, but for say the 15th game, we will use a 14 game window.\nNote that for new seasons, this ten game window serves as a prior for each team in a similar manner to Football Outsiders’ weighted DVOA. For example, to predict week 3, we will use a rolling average that includes 8 games from the previous year along with weeks 1 and 2.\nWe will ignore the 1999 season and only analyze the predictive power of EPA starting with the 2000 season, allowing us to use the 1999 season as a prior.\nFinally, the pracma package offers several types of rolling averages, including simple, weighted, running and exponential rolling averages. We will play around with these types of moving averages to see what maximizes predictive power.\nIf you want to look at all the code for building this dataset, view the source file! Here is the function to compute moving averages based on varying window sizes:\n\n\n# Function to get moving average of a dynamic window from 10 - 20 games\nwt_mov_avg_local <- function(var, weight, window, type, moving = T) {\n  if (length(weight) == 1 & weight[1] == 1) {\n    weight <- rep(1, length(var))\n  }\n  if (moving) {\n    dplyr::case_when(\n      window == 10 ~ pracma::movavg(var * weight, n = 10, type = type) /\n        pracma::movavg(weight, n = 10, type = type),\n      window == 11 ~ pracma::movavg(var * weight, n = 11, type = type) /\n        pracma::movavg(weight, n = 11, type = type),\n      window == 12 ~ pracma::movavg(var * weight, n = 12, type = type) /\n        pracma::movavg(weight, n = 12, type = type),\n      window == 13 ~ pracma::movavg(var * weight, n = 13, type = type) /\n        pracma::movavg(weight, n = 13, type = type),\n      window == 14 ~ pracma::movavg(var * weight, n = 14, type = type) /\n        pracma::movavg(weight, n = 14, type = type),\n      window == 15 ~ pracma::movavg(var * weight, n = 15, type = type) /\n        pracma::movavg(weight, n = 15, type = type),\n      window == 16 ~ pracma::movavg(var * weight, n = 16, type = type) /\n        pracma::movavg(weight, n = 16, type = type),\n      window == 17 ~ pracma::movavg(var * weight, n = 17, type = type) /\n        pracma::movavg(weight, n = 17, type = type),\n      window == 18 ~ pracma::movavg(var * weight, n = 18, type = type) /\n        pracma::movavg(weight, n = 18, type = type),\n      window == 19 ~ pracma::movavg(var * weight, n = 19, type = type) /\n        pracma::movavg(weight, n = 19, type = type),\n      window == 20 ~ pracma::movavg(var * weight, n = 20, type = type) /\n        pracma::movavg(weight, n = 20, type = type)\n    )\n  } else {\n    pracma::movavg(var * weight, n = 10, type = type) /\n      pracma::movavg(weight, n = 10, type = type)\n  }\n}\n\n\n\nAnalyzing predictive power\nFirst, let’s create a dataset where we use simple moving averages along with a static window of just ten games as a baseline (this is the idea in Jonathan’s post) and compare it with a dataset where we use a simple moving averages along with a dynamic window that ranges from ten to twenty games.\n\n\n\nAwesome! It looks like using the dynamic window (which always adds the additional information from all games played during a season) increases the predictive power of EPA/play, rather than using a static window that looks only at a team’s previous ten games.\nNote that the “Complex” category combines pass and rush EPA/play together as follows:\n\\[\\text{off_pass_rush_epa} = \\text{off_dropback_pct}*\\text{off_pass_epa} + (1-\\text{off_dropback_pct})*\\text{off_rush_epa}\\]\nIn other words, it combines a team’s pass and rush EPA/play into a total EPA/play by weighting them by the percentage of time a team drops back to pass. The difference between this and total EPA/play is that the complex version adjusts pass and rush EPA separately according to the strength of opponent pass and rush EPA, whereas total EPA adjusts based on opponent holistically.\nWhat’s interesting is that adjusting defensive EPA/play for opponent offenses faced actually decreases its predictive power, albeit a very small amount. This finding might provide a little support to the “defenses are simply a product of the offenses they face”, as penalizing or rewarding defenses based on the quality of the offense they face might simply be adding noise. Of course, in general, defensive strength as measured through EPA is much less predictive than offensive strength.\nNow that we know that using a dynamic window for the rolling averages works better than a simple static ten game window, let’s play around with different types of moving averages from the pracma package. We will try weighted, running and exponential averages compared to the simple moving averages from above.\n\n\n\nThere is a lot to take in here, so let’s break it down by category of predictor:\nPoint Differential: It seems clear that a running moving average of point differential increases its predictive power the most.\nTotal Offense/Defense EPA: In terms of offensive EPA, an exponential moving average works best (this is primarily driven by passing offense). In terms of defensive EPA, simple or running moving averages work best and it is difficult to discern between the two.\nOffense Pass EPA: An exponential moving average best predicts offensive pass EPA.\nDefense Pass EPA: Simple and running moving averages do about equally.\nOffense/Defense Rush EPA: A running moving average outclasses the other options.\nIn general, weighting recent performance more heavily increases a metric’s predictive strength. Let’s see if we can’t combine the best types of rolling average for each predictor to generate the best set of predictors for the outcome of a game. We will use a running average for point differential, rushing offense/defense, as well as for passing and total defense and an exponential moving average for offensive pass EPA and total offensive EPA.\n\n\n\nSo mixing the types of moving averages yields some benefits and some drawbacks. We are able to achieve the maximum level of predictive power for point differential and rushing offense/defense, however passing offense/defense suffers. It appears that when adjusting passing offense for opponent, it is better to use a lagging exponential moving average of opponent defense, yet this lagging exponential moving average of defense significantly decreases the predictive power of defensive EPA. Obviously, more work can be done to play around with different combinations of moving averages, although some improvements could easily be attributed to random noise. For now, let’s use the mixed dataset to explore!\nExploration\nIf you follow Ben Baldwin on twitter or have visited his awesome website https://rbsdm.com/, you may be familiar with his weekly team tiers scatter plot. Let’s look at the differences between those team tiers (which use unadjusted, cumulative mean EPA) and team tiers using our framework:\n\n\n\nWow, look at the Bills and Ravens! Everybody thinks the Chiefs are unbeatable, but their path to the Super Bowl seems much more treacherous than last years… We also see that the Rams’ and Steelers’ offenses and defenses have regressed since the beginning of the year.\nAs a Jets fan, I can’t help but look at the Jets ruining their quest for Trevor Lawrence as the Jags ensure the losing continues! 😢\nWe can also look at a team’s offensive and defensive efficiency over the course of the season using our methodology. Here’s the Chiefs and some AFC competitors:\n\n\n\nAnd here’s some NFC competitors:\n\n\n\nFinal Thoughts\nThe main objective of this post was to improve the predictiveness of EPA by exploring rolling averages and dynamically moving windows. Certainly, rolling 250 play averages of EPA, for example, can provide important descriptive information. However, as with most predictive measures, this work provides evidence that using the widest window of available data yields the best results. Nonetheless, there are definitely ways to improve this methodology. More work can be done in altering which types of moving averages are used for each facet of play. Additionally, note that the post does not explore weighting EPA for given types of plays. For example, weighting EPA by win probability, increasing the value of EPA on early downs or down-weighting turnovers will probably yield significant increases in its predictive power. It is also somewhat surprising that adjusting pass and rush EPA individually for opponent does not seem to produce much of an advantage, although perhaps a more in-depth technique might yield predictive benefits. Hopefully this analysis is another small iteration in improving EPA as a predictive metric!\nThanks to Sebastian Carl and Ben Baldwin for creating this forum and providing awesome open source football analysis throughout the year, as well as to Jonathan Goldberg, Lee Sharpe and Robby Greer for their inspiration.\n\nView source code on GitHub \n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-10-03-still-elite-what-the-numbers-tell-us-about-aaron-rodgers/",
    "title": "Still elite: What the numbers tell us about Aaron Rodgers",
    "description": "The more you look into the numbers, the better it looks for Aaron Rodgers",
    "author": [
      {
        "name": "Peter Owen",
        "url": "https://twitter.com/random_effects"
      }
    ],
    "date": "2020-10-03",
    "categories": [],
    "contents": "\nTable of Contents\nStill Elite?\nBest evaluation metrics\nInitial Look\nHow to improve on epa/play\nWhat makes Rodgers great?\nStill Elite!\nStill Elite?\nWeek 1 of the 2020 NFL season brought about an MVP caliber display from Rodgers as he torched my beloved Vikings for 43 points. He put up over 0.4 epa/play on more than 40 dropbacks against a Mike Zimmer defense, which has only happened 2 times before in Zimmer’s career as a head coach 1. PFF subsequently rewarded him with a grade of 96.0, the highest week 1 grade that’s been given since Tom Brady’s 96.9 in 2011.\nHe followed this up with another MVP level performance in week 2 receiving the 2nd highest PFF grade of the week and leading the offense to another 40 point game as the Packers comfortably swept aside the Lions.\nThankfully for this piece, that was supposed to go out pre-season, Rodgers maintained his MVP form in week 3 against the Saints achieving the 4th highest grade of the week from PFF leading to another high scoring game for the Packers who ended up with 37 points.\nSo far the Packers have racked up 3.73 points per drive and are one of only 3 teams to be averaging over 40 points through the first 3 games in the play-by-play era 2\nWe’re now almost a year on from Ben Baldwin’s piece on Aaron Rodgers demise, and I thought it would be worth checking what the numbers say about his performance, especially giving his rampant start to the season.\nBest evaluation metrics\nWe can start off with a piece of analysis done by Baldwin himself showing that for 2016-2019 SIS Total points was the best predictor of future performance. PFF offensive grade also comes out well in this analysis so let’s start by looking at these two metrics over the past four years.\nHere’s the leaderboard for SIS Total points in 2019\n\n\n\nRodgers was comfortably the most valuable player by Total Points in 2019 so perhaps the talks of his demise were too soon. A great article from Mike Sando shows how last year Rodgers was negatively affected more by drops than in previous acclaimed years, something that SIS total points takes into account, but raw epa does not.\nRodgers currently sits in 2nd for SIS total points in 2020, carrying on his good performance in the metric from the prior year.\nIf we switch to play grading rather than epa analysis Rodgers still came out as elite over recent times. In his recent ranking of NFL quarterback’s Sam Monson stated that Rodgers ranked as the 4th best Quarterback of the past 2 years.\nThe issue with just using raw epa per play is that although it improves on yardage totals it still misses out on a lot of context (e.g. receivers, defense, game conditions), a lot of which can be accounted for with data that’s available in the nflfastR play-by-play; so let’s have a go at creating our own QB metric and see how it changes things.\nFirst let’s load the data:\n\n\nlibrary(tidyverse)\nlibrary(mgcv)\nlibrary(mixedup)\n\npbp <- map_df(2016 : 2019, ~{\n  readRDS(\n    url(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{.}.rds\")\n    )\n  )\n}) %>% mutate(to_sticks = air_yards - ydstogo,\n         passer30 = fct_lump_n(passer , 30),\n         receiver100 = fct_lump_n(receiver , 100),\n         log_ydstogo = log(ydstogo))\n\ndbs <- pbp %>% filter(!is.na(passer), !is.na(epa))\n\nInitial Look\nLet’s start by ranking QB’s by epa/play since 2016 and see how it looks:\n\n\n\nWe can see that Rodgers ranks 7th in this time-frame, which in itself isn’t bad, but it’s far away from his elite heights of 2011-2015 where he ranks 1st in epa/play by a wide margin.\n\n\n\nBut we can do better than just average epa per play, we know the receivers that QBs throw to on every attempt, we know the defenses they faced on every dropback, we know the down, yardline, and yards to go for every snap; so let’s account for this and come up with a metric that contains much more context and therefore is much more representative of true ability.\nLooking at epa/play since 2016 we can see that Garoppolo (2nd), Nick Mullens (11th), and Drew Lock (12th) 3 are far too high; Rodgers (7th), and Russ (14th) are too low. Let’s see if adjusting for additional context changes our rankings at all.\nHow to improve on epa/play\nFirst of all we can see the value an average QB would generate from the type of play they perform on a particular dropback (take a sack, throw the ball away, scramble, or throw the ball a certain distance downfield). We can think of this as a mixture of decision making and running ability. This can be done by assigning a play it’s epa value on all non-pass attempts, and then assigning the average value of a play on a pass attempt through modelling.\nNext we can then see how a QB outperforms pass attempts in regards to the throwing component (air EPA) and the yards after catch component (YAC EPA), then combine these with the first part to give us a combined view on:\nPlay Quality - How good a QB is at avoiding pressure and getting a good type of throw off or a high quality scramble.\nAir Yards - How good a QB is at completing passes above expectation and generating air EPA\nYards After Catch - How good a QB is at hitting receivers in stride and generating YAC EPA\nWe’re going to do this by including the passer as a random effect in our model that predicts the epa of a play. We will include as much context as possible by including down, distance, yardline, and many other variables. In the 2nd and 3rd model we can also include receiver to adjust for qbs that either have a great/poor supporting cast. We’ll make sure to include shrinkage throughout the models to avoid overfitting as much as possible.\nDoing this gives us the following:\n\n\n\nWe can see straight away that this aligns much better with a consensus ranking of how players performed between 2016-2019.\nGaroppolo moves from 2nd to 11th; Mullens from 11th to 27th, Lock from 12th to 20th. These three seem to either benefit from scheme or just small sample sizes so good to see that the model accounts for both here.\nOn the other side of things Russ moves up from 14th to 9th and Rodgers from 7th to 2nd. Both these players are clearly better than their epa ranking and our model seems to pick up on this, but what exactly causes these changes?\nWhat makes Rodgers great?\nIf we look at the previous table we can see Rodgers is above average in all three components but in particular his effective pass accuracy shown by the air yards component is what stands out. One of the noticeable adjustments made that seem to help him stems from teams faced, if we look at the top ten defenses against air yards EPA we find 4 of the 10 were division rivals\n\n\n# A tibble: 10 x 2\n   team      value\n   <chr>     <dbl>\n 1 2019.NE  -0.051\n 2 2019.SF  -0.036\n 3 2018.CHI -0.032\n 4 2016.DEN -0.031\n 5 2017.DET -0.026\n 6 2017.MIN -0.025\n 7 2018.BUF -0.022\n 8 2017.JAX -0.022\n 9 2017.LAC -0.021\n10 2018.MIN -0.021\n\nAnd Mike Zimmer’s teams were easily the best in the league defensively\n\n\n# A tibble: 8 x 2\n  team            value\n  <chr>           <dbl>\n1 Mike Zimmer    -0.018\n2 John Harbaugh  -0.013\n3 Bill Belichick -0.012\n4 Andy Reid      -0.011\n5 Doug Marrone   -0.01 \n6 Gary Kubiak    -0.009\n7 Sean McDermott -0.009\n8 Ben McAdoo     -0.007\n\nBut this adjustment is relatively small in the grand scheme of things, the main adjustment that benefits both his air yards component and play quality component is rather surprising, considering his reputation, but this is what I found:\n\n\nmdl_3d <- bam(to_sticks ~ te(log_ydstogo , yardline_100) + s(passer30 , bs = \"re\") ,\n            data = dbs %>% filter(!is.na(receiver), !is.na(air_yards), down >= 3) ,\n            select = T , discrete = T)\n\n###\ndf_re_3d <- (extract_random_effects(mdl_3d)) %>%\n  mutate(QB = fct_reorder(.f = group, .x = value, .fun = mean)) %>%\n  filter(QB != \"Other\")\n\n\nplot_3d <- ggplot(df_re_3d, aes(x=value, y=QB)) +\n  geom_errorbar(aes(xmin=value-se, xmax=value+se)) +\n  geom_point() +\n  labs(title = \"Aggression on Late Downs\" ,\n       subtitle = \"Yards thrown past the sticks on 3rd & 4th Down\ncompared to an average QB \",\n       x = \"Yards compared to average\")\n\nplot_3d\n\n\nRodgers is by far the most aggressive QB in the league on late downs and it isn’t particularly close. Given his reputation of being too conservative this came as a big shock to me. The reason behind it is the fact that Rodgers is conservative on early downs and then flips the switch for late downs. It seems like Rodgers was one step ahead of the game as it makes a lot of sense to be conservative on early downs when yards before the sticks have high relative importance, and aggressive on later downs when yards before the sticks have almost zero importance.\nHere’s how much each individual yard before the sticks is worth on 1st down:\n\n\n\nAnd the same again on 3rd down\n\n\n\nWe see that gaining 8 yards (-2 yards to sticks) on 1st and 10 is similar to gaining 10 yards, while gaining 9 yards on 1st down is similar to gaining 12 yards. Therefore a QB who threw for 9 yards on every 1st down would be more valuable than a QB that threw for 11 yards. On 3rd down we see that the only thing that matters is getting to the first down marker. Rodgers is the only QB that seems to throw with this in mind, he thrives in the short passing game on early downs, and then attacks downfield when needed on later downs.\nIn general QB’s can add much more repeated excess value on 3rd downs than they can on 1st. Phillip Rivers was the best QB on 1st downs in this time frame and added ~0.15 epa/play over a replacement QB. Compare this to Rodgers who, although worse than Rivers on 1st downs, performed 0.38 epa/play better than a replacement QB on late downs. Mahomes was even better on late downs, although he benefited from a better receiving corps and is probably due for some regression since he isn’t even close to Rodgers’ level of aggression 4\n\n\n\nOn 1st down it probably just makes sense to aim for a successful play, and defenses seem to defend heavily beyond the sticks so aiming for passes between 5-9 yards downfield is probably ideal. We can show this by looking at success rates by air yards on 1st down; where we see that success rate peaks at ~6 air yards, illustrating the benefit of the mid/short passing game on a fresh set of downs.\n\n\n# A tibble: 10 x 3\n   air_yards success_rate count\n       <dbl>        <dbl> <int>\n 1         3        0.493  1353\n 2         4        0.537  1659\n 3         5        0.654  1881\n 4         6        0.707  1203\n 5         7        0.683   845\n 6         8        0.65    686\n 7         9        0.655   733\n 8        10        0.569   771\n 9        11        0.570   689\n10        12        0.562   632\n\nIn a similar vain to his peerless understanding of pass optimisation based on situation, he laps the league in aggression on defensive offside penalties, again Rodgers has an adept understanding of when to throw deep and when to throw short that no other QB seems to be able to emulate.\nStill Elite!\nSo we’ve shown that with the abundance of data we have we can get a much clearer view on the value of a QB than just ranking by yards per play, or epa per play, and there are still plenty of improvements to be made from here! If we understand the volatility of different play types and how predictive certain situations are we can further improve our model, potentially splitting up into further components to get even more context on what makes every single QB good or bad.\nFor now though we can say that all the best information we have pointed towards Rodgers having an MVP caliber season, and that’s exactly what we’ve seen so far. The quality of teams faced and the quality of the player’s team can having a significant impact on their performance. Davante Adams and Allen Lazard are both expected to miss week 4 with injuries so we may see a fall from grace this week in terms of epa/play, but this just further emphasises the point of how much context matters in the evaluation process.\n\nView source code on GitHub \n\nDak last year, and Tanehill in 2014↩︎\nalong with the Peyton-led 2013 Broncos and the Palmer-led 2015 Cardinals↩︎\nand possibly Lamar (4th) due to his replacement level 2018 season combined with MVP-level 2019 should probably rank him 10-15th overall↩︎\nalthough, you know, he’s Pat Mahomes so he’ll probably just get even better↩︎\n",
    "preview": "posts/2020-10-03-still-elite-what-the-numbers-tell-us-about-aaron-rodgers/still-elite-what-the-numbers-tell-us-about-aaron-rodgers_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-09-26-receiving-by-position/",
    "title": "Receiving by Position",
    "description": "Breaking down the receiving game by position using nflfastR data.",
    "author": [
      {
        "name": "Arthur Gymer",
        "url": "https://twitter.com/awgymer"
      }
    ],
    "date": "2020-09-26",
    "categories": [
      "Figures",
      "nflfastR",
      "Positional breakdown",
      "Receiving"
    ],
    "contents": "\nThe latest version of nflfastR added back the ability to join roster data so I decided to take a look at receiving by position; I particularly wondered if there had been any noticeable increase in TE usage in the passing game over the years.\nFirst up we are going to grab the play-by-play data and roster data and merge them to get the receiving player’s position.\n\n\nseasons <- 2000:2019\npbp <- purrr::map_df(seasons, function(x) {\n  readRDS(\n    url(\n      paste0(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_\", x, \".rds\")\n    )\n  )\n})\npbp <- as.data.table(pbp)\n# These decoding is incredibly slow\n# Decode the player_ids to the old-style gsis_id\n# Filter only plays with a receiver_id and non-NA epa\npbp <- pbp[!is.na(receiver_id) & !is.na(epa)]\nrec_pbp <- decode_player_ids(pbp)\nrec_pbp <- rec_pbp[!is.na(receiver_id)]\n# Get the rosters and filter out anything missing a gsis_id\nrosters <- as.data.table(nflfastR::fast_scraper_roster(seq(2000, 2019), pp = TRUE))\nroster_subset <- rosters[!is.na(gsis_id), .N, by = \"season,gsis_id,team,position\"]\n# Join pbp with the rosters and create a recpos column\nrec_pbp[roster_subset, recpos := i.position, on = .(receiver_id = gsis_id, season = season)]\n# Filter for plays where the receiver is one of the 4 normal offensive receiving positions and select a subset of columns\nrec_pbp <- rec_pbp[recpos %in% c(\"WR\", \"TE\", \"RB\", \"FB\"), .(season, recpos, epa, air_yards, complete_pass, posteam, pass_location, yardline_100)]\n# Create a summary table by season/position\nrecsumm <- rec_pbp[,\n  .(\n    mean_epa = mean(epa),\n    tot_epa = sum(epa),\n    mean_ay = mean(air_yards, na.rm = T),\n    tot_ay = sum(air_yards, na.rm = T),\n    tgt = .N,\n    cp = sum(complete_pass) / .N\n  ),\n  by = \"season,recpos\"\n]\n# Make the recpos a factor for ordering in plots\nrecsumm[, recpos := factor(recpos, levels = c(\"WR\", \"TE\", \"RB\", \"FB\"))]\n\nSo now we have the data let’s see what difference there is in usage by position.\n\n\n\nIt looks like the last 20 years have seen a steady increase in targets for TEs and a large drop off in targets to fullbacks. There’s an interesting reduction in targets to WRs across the past 3 seasons after small rise over the previous decade and I don’t have a good explanation for that.\nLooking at the total EPA accumulated by each position shows a similar rise for TEs and really highlights how using RBs in the passing game isn’t particularly effective; despite having similar numbers of targets to TEs their season-long EPA is next to nothing. Interestingly WR EPA in the past 2 seasons is at its highest despite the reduction in targets.\n\n\n\nSo tight-end targets and EPA are up, but are they just being used more or are they also seeing a change in how they are used?\n\n\n\nWe only have air yards data as far back as 2006 and it doesn’t look like there has been much change in the average air yards on throws to TEs since then. Air Yards aren’t entirely indicative of differing usage - we don’t know where the TE is lining up - but it does suggest the position as a whole haven’t seen a move away from their traditional routes.\nPlotting the smoothed distribution of targets by field position we see that all positions get the majority of their targets further from the goalline - this is pretty obvious when you consider where drives normally start and thus the fact that more passes are thrown there.\n\n\n\nOk so their target distribution looks similar to WRs but what about their share of targets at a given field position? Plotting the proportion of targets binned every 5 yards suggests that TEs get slightly higher share of targets right down by the goalline, largely eating into the RB share and historically some WR targets too, although that seems to have shifted in the past 5 years with WRs keeping their target share more at the goalline now.\n\n\n\nFinally I want to look at the data on a per-team basis. Scheme is important and we might find some teams which use their TEs a bit more unusually.\n\n\n\nIf we look at ratio of target-share for WR/TE there are a couple of teams which stand out, particularly those with high TE usage.\n2003 Ravens: A lethargic passing offense lead by rookie Kyle Boller amasses just 2517 yds\n2011 Patriots: Gronk and Hernandez at their peak\n2019 Eagles: Wentz becomes first QB to throw over 4000 yds without a WR reaching 500 yds\n2019 Ravens: Lamar Jackson marshals an unconventional offense with Mark Andrews as leading pass-catcher and 3 TEs in their top 5\n2019 Raiders: Darren Waller has a breakout season after offseason Antonio Brown is signed and released without playing a game\nAt the other end we see the Ryan Fitzpatrick lead Jets of 2015-16 and a 2002 Steelers team featuring a WR corp of Hines Ward, Plaxico Burress, and Antwaan Randle El.\nIf you’re wondering about that little dark green dot that appears to have a very low TE and WR target share then let me introduce you to the 2000 New York Jets. A 37 year old Vinny Testaverde targeted FBs and RBs a combined 38.9% of the time; FB Richie Anderson notching up 88 receptions to put him 12th in the league and Curtis Martin catching 70 balls for 30th in the league.\n\nView source code on GitHub \n\n\n\n",
    "preview": "posts/2020-09-26-receiving-by-position/receiving-by-position_files/figure-html5/epa-plot-1.png",
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3600
  },
  {
    "path": "posts/2020-09-09-creating-an-expected-field-goal-metric/",
    "title": "Creating an Expected Field Goal Metric",
    "description": "Using nflfastR play-by-play data to measure kicker performance.",
    "author": [
      {
        "name": "Mike Irene",
        "url": "https://twitter.com/mikeyirene"
      }
    ],
    "date": "2020-09-09",
    "categories": [
      "field goal",
      "nflfastR",
      "placekicker"
    ],
    "contents": "\nIntroduction\nPlacekickers are often an underappreciated portion of a football team. They receive plenty of criticism when important field goals are missed, and receive only a slight amount of praise when a big kick is made. A weak kicking game can be especially costly in close games and can turn wins into losses. By quantifying kicker performance, teams can try to prevent the kicker from being the reason for losing games that come down to a key field goal.\nField Goal Percentage is a common metric used to measure kicker performance, but it does not sufficiently account for varying levels of difficulty for each attempt. For example, a 19 yard attempt made in clear skies with no wind is much easier to make than a 50 yard attempt in the snow. To better account for the specific difficulty of each field goal attempt, I created an expected field goal model. This logistic regression model was developed using nflfastR’s play-by-play data from the 2009-2019 NFL seasons and determines the probability of a field goal being made, given the values of certain input variables. This expected field goal (xFG) metric is useful in measuring an individual kicker’s field goal performance, and could potentially be used to help coaches decide if their kicker has a strong probability of making a certain field goal attempt by factoring in a variety of conditions.\nReading and Cleaning the Data\nAfter reading in the play-by-play data, I created calculated columns as potential variables for the regression model. These calculated columns account for many attributes that may impact the result of a field goal attempt: indoors/outdoors, natural grass/artificial turf, precipitation, etc. Most of these fields contain binary Yes/No values, but “wind” and “temp” are continuous variables.\nOne issue I experienced was missing values for nflfastR’s “weather”, “temp”, and “wind” fields. Many of the missing values were a result of dome or closed roof stadiums, so those values were manually imputed with “No” for precipitation, 0 for wind, and 70 for temp. For missing values in outdoor stadiums, the impute.mean function replaced any missing values with the average value for that variable.\n\n\nlibrary(nflfastR)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(gt)\n\n# Read in data from nflfastR\nseasons <- 2009:2019\npbp <- map_df(seasons, function(x) {\n  readRDS(\n    url(\n      paste0(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_\", x, \".rds\")\n    )\n  )\n})\n\n# Create function to replace NA with mean value for temp and wind columns\nimpute.mean <- function(x) replace(x, is.na(x), mean(x, na.rm = TRUE))\n\n# Create data frame with kicks from 2009-2019, including new factors for model\nfg_data <- pbp %>%\n  filter(play_type == \"field_goal\", field_goal_result != \"blocked\") %>%\n  mutate(\n    made = as.factor(if_else(field_goal_result == \"made\", 1, 0)),\n    div_game = factor(div_game, levels = c(0, 1), labels = c(\"No\", \"Yes\")),\n    TieorTakeLead = as.factor(if_else(score_differential >= -3 & score_differential <= 0, \"Yes\", \"No\")),\n    GW_FG_Att = as.factor(if_else(score_differential >= -3 & score_differential <= 0 & qtr == 4 & as.numeric(gsub(\":\", \"\", (time))) < 100, \"Yes\", \"No\")),\n    RoadTeam = as.factor(if_else(posteam == away_team, \"Yes\", \"No\")),\n    Indoors = as.factor(if_else(roof == \"closed\" | roof == \"dome\", \"Yes\", \"No\")),\n    HighAltitude = as.factor(if_else(home_team == \"DEN\", \"Yes\", \"No\")),\n    NaturalGrass = as.factor(if_else(grepl(\"grass\", surface, ignore.case = TRUE), \"Yes\", \"No\")),\n    Precip = as.factor(case_when(\n      grepl(\"closed\", roof, ignore.case = TRUE) ~ \"No\",\n      grepl(\"dome\", roof, ignore.case = TRUE) ~ \"No\",\n      grepl(\"snow\", weather, ignore.case = TRUE) ~ \"Yes\",\n      grepl(\"showers\", weather, ignore.case = TRUE) ~ \"Yes\",\n      grepl(\"0% Chance of Rain\", weather, ignore.case = TRUE) ~ \"No\",\n      grepl(\"Cloudy, chance of rain increasing up to 75%\", weather, ignore.case = TRUE) ~ \"Yes\",\n      grepl(\"Cloudy, chance of rain\", weather, ignore.case = TRUE) ~ \"No\",\n      grepl(\"Zero Percent Chance of Rain\", weather, ignore.case = TRUE) ~ \"No\",\n      grepl(\"Rain Chance 40\", weather, ignore.case = TRUE) ~ \"No\",\n      grepl(\"30% Chance of Rain\", weather, ignore.case = TRUE) ~ \"No\",\n      grepl(\"No chance of rain\", weather, ignore.case = TRUE) ~ \"No\",\n      grepl(\"Cloudy, Humid, Chance of Rain\", weather, ignore.case = TRUE) ~ \"No\",\n      grepl(\"rain\", weather, ignore.case = TRUE) ~ \"Yes\",\n      TRUE ~ \"No\"\n    )),\n    wind = case_when(\n      is.na(wind) & roof != \"outdoors\" ~ 0,\n      is.na(wind) & roof == \"outdoors\" ~ impute.mean(wind),\n      !is.na(wind) ~ as.numeric(wind),\n      TRUE ~ 0\n    ),\n    temp = case_when(\n      is.na(temp) & roof != \"outdoors\" ~ 70,\n      is.na(temp) & roof == \"outdoors\" ~ impute.mean(temp),\n      !is.na(temp) ~ as.numeric(temp),\n      TRUE ~ 0\n    )\n  )\n\nExploratory Variable Analysis\nI created a series of line charts and scatter plots to take a look at which independent variables might have an impact on Field Goal Percentage (number of made kicks divided by all attempts). The line charts take a look at the impact of each categorical variable (combined with distance) on field goal percentage, while the scatter plots focus on continuous variables like wind and temperature.\n\n\n# Line charts of field goal rate by distance - split by categorical variables\nfg_data %>%\n  select(kick_distance, field_goal_result, div_game, TieorTakeLead:Precip) %>%\n  gather(metric, value, div_game:Precip) %>%\n  group_by(kick_distance, metric, value) %>%\n  summarize(\n    `fg%` = sum(field_goal_result == \"made\") / n(),\n    Attempts = n()\n  ) %>%\n  arrange(kick_distance) %>%\n  ggplot(aes(x = kick_distance, y = `fg%`, group = value, color = value)) +\n  geom_line() +\n  scale_y_continuous(labels = scales::percent, name = \"Field Goal Percentage\") +\n  scale_x_continuous(breaks = seq(15, 60, by = 5), limits = c(15, 65), name = \"Distance\") +\n  theme(legend.position = \"top\") +\n  facet_wrap(~metric)\n\n\n# Scatter plots of field goal result by distance and wind/temp  - split by continuous variables\nfg_data %>%\n  ggplot(aes(x = kick_distance, y = wind, group = field_goal_result, color = field_goal_result)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(15, 60, by = 5), limits = c(15, 65), name = \"Distance\") +\n  theme(legend.position = \"top\") +\n  coord_flip()\n\n\n# Scatter plots of field goal result by distance and wind/temp  - split by continuous variables\nfg_data %>%\n  ggplot(aes(x = kick_distance, y = temp, group = field_goal_result, color = field_goal_result)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(15, 60, by = 5), limits = c(15, 65), name = \"Distance\") +\n  theme(legend.position = \"top\") +\n  coord_flip()\n\n\nFor the categorical variables, the line charts show some larger variances in Field Goal Percentage based on precipitation (Precip), field surface (NaturalGrass), and if the kick was a game-winning attempt (GW_FG_Att). The scatter plots show some potential variability in the field goal result at higher wind speeds and lower temperatures.\nCreating the Expected Field Goal (xFG) Model\nTo create the xFG model, the data was split into train (80%) and test (20%) subsets. The train subset was used to create the model, while the test subset was used to evaluate the performance of the xFG model. First, the logistic regression model was run using all variables, then a second model was created using only the statistically significant variables. The second model (xFG_model) was then used to predict the outcome of field goal attempts from the test subset. If the predicted “made” value was greater than 0.5 (50%), then the model predicted the field goal attempt was successful.\n\n\n# Create Train and Test splits (80/20)\nset.seed(123)\ntrain <- fg_data %>% sample_frac(.8)\ntest <- setdiff(fg_data, train)\n\n\n# Create Logistic Regression Model using all variables\nxFG_model_all <- glm(made ~\nkick_distance + div_game + NaturalGrass + temp + wind + TieorTakeLead + GW_FG_Att + RoadTeam + Indoors + Precip,\nfamily = binomial(logit),\ndata = train\n)\nsummary(xFG_model_all)\n\n# Final xField Goal Model. Includes significant variables only\nxFG_model <- glm(made ~\nkick_distance + GW_FG_Att + wind + Precip,\nfamily = binomial(logit),\ndata = train\n)\nsummary(xFG_model)\n\n\n\n# Predict using test set. create confusion matrix\npredictions <- predict(xFG_model, test)\nconfusionMatrix(as.factor(if_else(predictions >= .5, 1, 0)), as.factor(test$made))\n\n# Predict xFG using model\nxFG <- predict(xFG_model, fg_data, type = \"response\")\npred_fg_data <- data.frame(fg_data, xFG)\n\nThe xFG model was able to predict the field goal result with 86% accuracy. The model was much better at predicting if a field goal would be made (88%) than if the field goal would be missed (49%). For predictive purposes, this model does not provide a significant lift in accuracy over the 86% field goal percentage of our test sample. To improve on the predictive power of the xFG model, more detailed weather data could be pulled into the model to get the exact weather measurements at time of the field goal attempt.\nxFG and FGOE\nHowever, the xFG metric can still be useful in identifying kickers that converted more (or less) field goal attempts than expected. The chart below shows the top 10 kickers in Field Goals Over Expected (FGOE) for the 2019 Season:\n\n\n# Field Goals over Expected Chart\npred_fg_data %>%\n  mutate(season = substr(game_id, 1, 4)) %>%\n  filter(season == 2019) %>%\n  group_by(kicker_player_name, posteam) %>%\n  summarize(\n    xFG = sum(xFG), ActualFGMade = sum(field_goal_result == \"made\"),\n    FGOE = ActualFGMade - xFG\n  ) %>%\n  arrange(-FGOE) %>%\n  ungroup() %>%\n  slice(1:10) %>%\n  mutate(Rank = paste0(row_number())) %>%\n  gt() %>%\n  tab_header(title = \"Field Goals Over Expected - 2019 NFL Season\") %>%\n  cols_move_to_start(columns = vars(Rank)) %>%\n  cols_label(\n    kicker_player_name = \"Kicker\",\n    posteam = \"Team\",\n    ActualFGMade = \"FG Made\"\n  ) %>%\n  fmt_number(columns = vars(xFG, FGOE), decimals = 1) %>%\n  cols_align(align = \"center\", columns = vars(Rank, kicker_player_name, posteam, xFG, ActualFGMade, FGOE)) %>%\n  tab_style(style = cell_text(size = \"large\"), locations = cells_title(groups = \"title\")) %>%\n  tab_style(style = cell_text(align = \"center\", size = \"medium\"), locations = cells_body()) %>%\n  tab_source_note(source_note = \"\") %>%\n  text_transform(\n    locations = cells_body(vars(posteam)),\n    fn = function(x) web_image(url = paste0(\"https://a.espncdn.com/i/teamlogos/nfl/500/\", x, \".png\"))\n  ) %>%\n  data_color(columns = vars(FGOE), colors = \"grey90\", autocolor_text = FALSE) %>%\n  cols_width(vars(posteam) ~ px(45))\nField Goals Over Expected - 2019 NFL Season\n    \n    Rank\n      Kicker\n      Team\n      xFG\n      FG Made\n      FGOE\n    1\n      J.Tucker\n      \n      26.1\n      30\n      3.9\n    2\n      J.Lambo\n      \n      29.6\n      33\n      3.4\n    3\n      H.Butker\n      \n      33.5\n      36\n      2.5\n    4\n      K.Forbath\n      \n      8.3\n      10\n      1.7\n    5\n      C.Boswell\n      \n      27.3\n      29\n      1.7\n    6\n      D.Bailey\n      \n      28.5\n      30\n      1.5\n    7\n      M.Crosby\n      \n      20.6\n      22\n      1.4\n    8\n      R.Bullock\n      \n      25.9\n      27\n      1.1\n    9\n      B.McManus\n      \n      26.9\n      28\n      1.1\n    10\n      M.Prater\n      \n      25.0\n      26\n      1.0\n    \n    \n\nUnsurprisingly, All-Pro Justin Tucker led all kickers in FGOE during the 2019 season. Interestingly, Kai Forbath only kicked in 4 games in 2019, but performed well enough in those opportunities to rank 4th among all kickers in FGOE.\nConclusion\nThe xFG metric is a useful tool to measure kicker performance because it accounts for the difficulty of each kick, including factors for distance, weather, and game situation. A kicker’s xFG can be compared to the actual number of field goals made to create FGOE. FGOE can help identify a kicker’s impact by rewarding kickers that make difficult field goal attempts, and penalizing those that miss attempts with a high probability of being made.\n\n\n",
    "preview": "posts/2020-09-09-creating-an-expected-field-goal-metric/distill-preview.png",
    "last_modified": "2023-04-17T16:10:44+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-09-07-estimating-runpass-tendencies-with-tidymodels-and-nflfastr/",
    "title": "Estimating Run/Pass Tendencies with tidyModels and  nflfastR",
    "description": "This article shows how to use tidyModels to predict QB dropbacks and uses a multilevel model to show which teams are run/pass heavy after accounting for game script",
    "author": [
      {
        "name": "Richard Anderson",
        "url": "http://richjand.rbind.io"
      }
    ],
    "date": "2020-09-08",
    "categories": [
      "tidyModels",
      "nflfastR",
      "stan"
    ],
    "contents": "\n\nContents\nEstimating Pr(QB Dropback)\nTidying my xgboost\nPrepping the Data\nPrepping the Model\nParameter Tuning\nModel Evaluation\n\nA Quick Look At Team Tendencies\nTeam Effects\n\nConclusions\n\nAs a Seahawks fan who spends a good amount of time online I’ve been exposed to a lot of discussion about the value of running vs. passing. A point used in favor of a rushing-focused attack is that good teams tend to run the ball a lot. This is usually met with the response that teams who are ahead will run to kill clock and minimize risk, meaning the causal arrow is backwards. The Patriots always run a lot, but the Patriots are always ahead so of course they are going to run.\nA common strategy to establish whether a team is run/pass heavy is to identify a subset of plays where the team is not bound by needing to pass to catch up or run to kill clock (See Warren Sharp’s Tweet) and see what decisions teams actually made. If we see that the Patriots pass while games are competitive and run when they are closing out the game then we know that the Pats winning isn’t caused by rushing. The problem with this approach is that it tends to throw away a lot of useful information. Seeing a team run on 2nd and 11 (again, Seahawks fan here) tells us something very different than seeing a team run on 2nd and 1 just as throwing on 3rd and 1 tells us something different than throwing on 3rd and 10. Thanks to the awesome people at nflscrapR and nflfastR we can build that kind of context into our analysis.\nThe basic roadmap for this post is\nUse tidyModels to classify plays as dropback/non-dropback\nUse model outputs as a variable in a multilevel model to estimate team tendencies\nLook at some basic findings from the multilevel model\nEstimating Pr(QB Dropback)\nThe thing we ultimately want to understand is team tendencies. Once we account for the state of the game and any other information of interest, does it seem that team x is more run-focused or pass-focused? This post is basically an exercise in feature engineering where we’re trying to create a measure (dropback probability) that we can use as an input in another model that we’ll use to understand team tendencies. The model we want to build to is:\n\n\\(y_{it} \\sim Bernoulli(p_{it})\\)\n\\(logit(p_{it}) = \\alpha + \\gamma_{t} + \\beta_{1}\\hat{p}_i + \\boldsymbol\\beta\\textbf{X}_{i}\\)\n\nwhere \\(y_{it}\\) is going to be whether team \\(t\\) called a pass on play \\(i\\), \\(\\gamma_{t}\\) is a team effect which will be our measure of team strategy, and \\(\\boldsymbol\\beta\\textbf{X}_{i}\\) is going to be any other information we want to include such as quarterback ability, quality of defense, weather, or anything else of interest. \\(\\hat{p}_i\\) is the probability of a QB dropback that we’ll generate with our model below. In effect, this will give us an expectation from which we’ll measure deviances at the team level.\nPart of the impetus for this project was to learn how to use the tidyModels and parsnip packages so we will cover how these packages were used to build the model in some detail. If you’re interested in learning more about using tidyModels you should check out posts by Julia Silge and Rebecca Barter which were extremely helpful in getting up and running in the tidyModels universe.\nThis is a classification problem where we will predict whether or not a play will be a QB dropback. I predict the probability of a QB dropback using the nflfastR-provided variables that collectively capture the game state at the time of the play. These variables aren’t an exhaustive list of what goes into the game state, but hopefully capture most of the information relevant to teams in making the decision to run or pass. The variables are:\nDown (limited to 1,2,3)\nYards for first down\nYard line\nScore Differential\nQuarter\nTime remaining in half\nNumber of timeouts for the offense and defense\nNote that a QB dropback is not the same as saying a pass occurred. QB dropbacks are plays where the offense intended to pass, even if they did not end up in an attempted pass (sacks, scrambles, etc…).\nWe’ll use an xgboost model because we know there are non-linearities in the relationship between independent variables and dependent variable as well as some complex interactions between the variables. I can’t say anything about xgboost that hasn’t been said better in a million other data science posts so I’ll just say that I, like so many others, have found xgboost extremely useful for a variety of machine learning projects.\nTidying my xgboost\nFirst I’ll include everything to get the data set up. Note that I’m loading a few pre-built models. The code needed to build all of these objects is included but since each takes a long time to generate I’m just going to use saved versions.\n\n\nlibrary(rstan)\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(vip)\nlibrary(tidymodels)\nlibrary(workflows)\nlibrary(dials)\nlibrary(tune)\nlibrary(DT)\nlibrary(arm)\nlibrary(tidybayes)\nlibrary(ggrepel)\n\nset.seed(1234)\n\nseasons <- 2016:2019\ndat <- purrr::map_df(seasons, function(x) {\n  readRDS(\n    url(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\")\n    )\n  )\n})\n\npost16 <- filter(\n  dat,\n  season_type == \"REG\" &\n    down %in% c(1, 2, 3) &\n    !is.na(qb_dropback) &\n    !is.na(score_differential)\n) %>%\n  mutate(\n    qb_dropback = factor(qb_dropback),\n    off_to = if_else(posteam_type == \"away\", away_timeouts_remaining, home_timeouts_remaining),\n    def_to = if_else(posteam_type == \"away\", home_timeouts_remaining, away_timeouts_remaining)\n  ) %>%\n  dplyr::select(qb_dropback, down, ydstogo, yardline_100, score_differential, qtr, half_seconds_remaining, off_to, def_to)\n\nxgb_res <- readRDS(\"objects/xgb-grid-search.RDS\") ## Loading hyperparameter grid results\nfinal_mod <- readRDS(\"objects/final-mod-test-dat.RDS\") ## Loading xgboost model\nfinal_qb_mod <- readRDS(\"objects/final-full-xgb.RDS\") ## loading xgboost model\nfit_no_epa <- readRDS(\"objects/no_epa_model.RDS\") ## loading stan model\n\nsamps_no_epa <- rstan::extract(fit_no_epa, pars = \"mu\")$mu ## Extract mu estimates\nquantile_025_no_epa <- apply(samps_no_epa, 2, quantile, .025) ## Calculate 2.5th percentile of mu estimates\nquantile_975_no_epa <- apply(samps_no_epa, 2, quantile, .975) ## Extract 97.5th percentile of mu estimates\nmean_no_epa <- apply(samps_no_epa, 2, mean) ## extract mean estimates\n\nteams <- dat %>%\n  filter(!is.na(posteam)) %>%\n  dplyr::select(posteam, season, qb_dropback) %>%\n  mutate(\n    team_string = str_c(posteam, \"-\", season),\n    team_idx = as.numeric(factor(team_string))\n  ) %>%\n  group_by(posteam, season) %>%\n  summarise(\n    team_idx = max(team_idx),\n    dropback_pct = mean(qb_dropback)\n  ) %>%\n  ungroup()\n\nteams$q_025_no_epa <- quantile_025_no_epa\nteams$q_975_no_epa <- quantile_975_no_epa\nteams$mean_no_epa <- mean_no_epa\nteams$display_name <- factor(str_c(teams$posteam, \" - \", teams$season))\nteams$display_name <- fct_reorder(teams$display_name, teams$mean_no_epa)\nteams <- teams %>%\n  group_by(season) %>%\n  mutate(\n    qb_dropback_rank = rank(desc(dropback_pct)),\n    qb_dropback_est_rank = rank(desc(mean_no_epa))\n  )\n\n\n\nPrepping the Data\nThe first step is going to be to split the data into train and test which we can do with the initial_split function. By default this function will use 75% of the data for training and the remaining 25% for testing. We’ll look at 2016-2019, which leaves ~100k observations for training and ~35k observations for testing.\n\n\ndat_split <- initial_split(post16)\ndat_train <- training(dat_split)\ndat_test <- testing(dat_split)\n\n\n\nWe’re going be tuning our xgboost hyperparameters so we’ll want to perform some cross-validation to see which hyperparameters give us the best performance. We can create cross-validation sets using vfold_cv().\n\n\nqb_folds <- vfold_cv(dat_train)\n\n\n\nPrepping the Model\nNext we’ll define a recipe using the recipe() function from the recipes package. Recipes involve setting a formula that looks like what you use to train most models in R and doing any pre-processing (scaling, normalizing, imputing, etc…) that you want to do to your variables. The nice thing about the recipe formulation is that it is the same regardless of which model you’ll ultimately be using so you don’t need to remember how data needs to be fed into glmnet vs. xgboost vs. glm. xgboost doesn’t require that data be regularized or normalized so we can specify our recipe as in the formula below, but if you do need to do some kind of pre-processing you can check out the dozens of packages in recipes that begin with step_.\n\n\nqb_recipe <- recipe(qb_dropback ~ down +\n  ydstogo +\n  yardline_100 +\n  score_differential +\n  qtr +\n  half_seconds_remaining +\n  off_to +\n  def_to,\ndata = dat_train\n)\n\n\n\nNow that we have a recipe we will get our model set up. We’re going to use a boosted tree model which carries with it a bunch of tuneable hyperparameters. We will fix the number of trees to keep cross-validation from getting out of hand and tell the model to stop when there has been no improvement in 100 rounds. Everything else is going to be selected based on model fit.\nThe set_engine() specifies the package that the model is coming from so if you preferred to use gbm instead of xgboost you would specify set_engine(“gbm”).\n\n\nqb_model <-\n  boost_tree(\n    mtry = tune(),\n    trees = 2000,\n    min_n = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    loss_reduction = tune(),\n    sample_size = tune(),\n    stop_iter = 100\n  ) %>%\n  set_engine(\"xgboost\") %>%\n  set_mode(\"classification\")\n\n\n\nFinally, we’re going to specify a workflow which is going to gather the recipe and model we built above. This is going to make it very easy to do parameter tuning and model building without repeatedly specifying the same information.\n\n\nqb_workflow <- workflow() %>%\n  add_recipe(qb_recipe) %>%\n  add_model(qb_model)\n\n\n\nParameter Tuning\nNow it’s time to actually do some modeling! We’ll use our cross-validation folds to try a bunch of different potential hyperparameter values and return which gives us the best out of sample fit. We’ll try 40 different combinations sampled from across the hyperparameter space. Note that the mtry and sample_size parameters require additional arguments. mtry() refers to the number of columns to be sampled at each split. This is one where you need to be careful. If the data frame you specify for finizalize has more variables than you actually plan on training with, you will waste your time testing mtry values that don’t make any sense for your problem. The sample_size argument requires a number between 0 and 1 as it’s the proportion of the data that you’ll use in the fitting routine.\n\n\nxgb_grid <- grid_latin_hypercube(\n  finalize(mtry(), dat_train),\n  min_n(),\n  tree_depth(),\n  learn_rate(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  size = 40\n)\n\n\n\nTuning your grid is as easy as specifying a workflow, your cross-validation data, and the grid of values to be tested. save_pred = TRUE is going to save all of the cross-validation predictions for later evaluation. Note that this is going to take awhile. A grid of 40 took ~6 hours on my machine. I’d set this off overnight and save the results so you can reload the object without rebuilding every time.\n\n\nxgb_res <- tune_grid(\n  qb_workflow,\n  resamples = qb_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\n\n\nJulia Silge’s post has a nice plot to show the relationship between different parameter values and model performance that we’ll going to use here. On the y-axis we have the AUC of the model and on the x-axis we have the value of the hyperparameter. We’re looking to see if there are any obvious correlations between performance and hyperparameter value and if we might need to expand the range of tested values. It’s tough to draw any sweeping conclusions though it looks like higher values of mtry and, to a certain extent, tree depth perform better. It also doesn’t appear that the best values of our hyperparameters are on the edges of our plots. Were it the case that performance was clearly increasing with higher tree depth and we didn’t see a point at which model performance began to decline we would want to extend the range of hyperparameters that we test to make sure that we aren’t setting those values too low.\n\n\nxgb_res %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  dplyr::select(mean, mtry:sample_size) %>%\n  pivot_longer(mtry:sample_size,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\") +\n  theme_minimal()\n\n\n\n\nWe can extract the best-performing set of hyperparameters using the select_best() function and use those values to finalize our workflow.\n\n\nbest_auc <- select_best(xgb_res, \"roc_auc\")\n\nqb_xgb <- finalize_workflow(\n  qb_workflow,\n  parameters = best_auc\n)\n\n\n\nAt this point we’re ready to evaluate the performance of the model trained on our training data with our chosen hyperparameters on our test data which we can do with the last_fit() function. We’ll need to give the function our finalized workflow as well as our split data.\n\n\nfinal_mod <- last_fit(qb_xgb, dat_split)\n\n\n\nModel Evaluation\nWe can find out just how well the model did using collect_metrics(). We ended up with 69% accuracy and an AUC of .76 which seems about right given the application. If we could perfectly predict dropback probability from game state it would be very easy to be an NFL defensive coordinator! Again, Julia Silge did a great job visualizing model outputs in her post so we will basically lift her code for this ROC curve plot\n\n\ncollect_metrics(final_mod)\n\n\n# A tibble: 2 x 3\n  .metric  .estimator .estimate\n  <chr>    <chr>          <dbl>\n1 accuracy binary         0.691\n2 roc_auc  binary         0.760\n\nfinal_mod %>%\n  collect_predictions() %>%\n  roc_curve(qb_dropback, .pred_0) %>%\n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(size = 1.5, color = \"midnightblue\") +\n  xlab(\"1 - Specificity\") +\n  ylab(\"Sensitivity\") +\n  geom_abline(\n    lty = 2, alpha = 0.5,\n    color = \"gray50\",\n    size = 1.2\n  ) +\n  ggtitle(\"ROC Curve\") +\n  theme_minimal()\n\n\n\n\nAs a final check on our results let’s look at calibration in our test data. We want our predicted dropback probabilities to be similar to the actual dropback probabilities and it looks like that’s the case! There’s only 14 plays in the far right dot so I’m not going to lose any sleep over it.\n\n\nfinal_mod %>%\n  collect_predictions() %>%\n  mutate(pred_rounded = round(.pred_1, 1)) %>%\n  group_by(pred_rounded) %>%\n  summarise(\n    mean_prediction = mean(.pred_1),\n    mean_actual = mean(as.numeric(qb_dropback) - 1),\n    n = n(),\n    se = sd(as.numeric(qb_dropback) - 1 - .pred_1) / sqrt(n)\n  ) %>%\n  ggplot(aes(x = pred_rounded, y = mean_actual)) +\n  geom_abline() +\n  geom_point(aes(size = n)) +\n  theme_minimal() +\n  xlab(\"Predicted Probability\") +\n  ylab(\"Actual Probability\") +\n  ggtitle(\"Calibration Plot, Test Data\") +\n  ylim(0, 1) +\n  xlim(0, 1)\n\n\n\n\nFinally, now that we’ve built some confidence in the model we’re going to build (using fit()) and predict (using predict()) the model on all data since 2016.\n\n\nfinal_qb_mod <- fit(qb_xgb, post16)\n\n\n\n\n\npost16_pred_dat <- filter(dat, season >= 2016 &\n  season_type == \"REG\" &\n  down %in% c(1, 2, 3) &\n  !is.na(qb_dropback) &\n  !is.na(score_differential)) %>%\n  mutate(\n    qb_dropback = factor(qb_dropback),\n    off_to = if_else(posteam_type == \"away\", away_timeouts_remaining, home_timeouts_remaining),\n    def_to = if_else(posteam_type == \"away\", home_timeouts_remaining, away_timeouts_remaining)\n  ) %>%\n  dplyr::select(qb_dropback, down, ydstogo, yardline_100, score_differential, qtr, half_seconds_remaining, off_to, def_to, epa, posteam, defteam, season)\n\npost16_pred_dat$dropback_prob <- predict(final_qb_mod, new_data = post16_pred_dat, type = \"prob\")$.pred_1\n\n\n\nAs a basic sanity check let’s make sure the model thinks passing is more likely in situations that we would expect. Generally speaking, throwing is more likely on third down and more likely with more yards to go which is what we’d hope to see.\n\n\n\nA Quick Look At Team Tendencies\nIn the future we’ll want to build a model that builds in additional information, but for now we can build a simple model to get an idea of which teams were more or less likely to pass than we would expect based on game script alone. Going back to the equation at the top of the post, we’ll fit a multilevel model where we predict the probability of a QB dropback as a function of our predicted dropback probability along with team random effects. We can interpret these effects as the degree to which teams differ from the expectation set out by the model we made above.\nWe’ll fit the model in stan, a popular language for fitting Bayesian models and one that people find especially useful for multilevel models. The stan code and the code to build the model in R is displayed below.\n\ndata{\n  int<lower = 0> N; //number of observations\n  int<lower = 1> I; //number of team/seasons\n  int<lower = 0, upper = 1> y[N]; //qb_dropback\n  int<lower = 0, upper = I> ii[N]; //team/season indicator\n  vector[N] phat; //fitted probability from xgboost model\n}\nparameters{\n  vector[I] mu_raw; //team/season random effects\n  real beta_phat; //effect of p_hat, should be ~ 1\n  real alpha; //intercept\n  real<lower = 0> sigma_mu; //standard deviation of random effects\n}\ntransformed parameters{\n  vector[I] mu = sigma_mu * mu_raw;\n}\nmodel{\n  alpha ~ normal(0, .25);\n  beta_phat ~ normal(1,.25);\n  mu_raw ~ normal(0,1);\n  sigma_mu ~ normal(0,1);\n  \n  y ~ bernoulli_logit(alpha + mu[ii] + beta_phat * phat);\n}\n\n\n\nstan_mod <- stan_model(file = \"/stan-models/pass-prob-stan-model-no-epa.stan\")\n\nstan_dat_no_epa <- list(\n  N = nrow(final_pred_dat),\n  I = max(final_pred_dat$team_idx),\n  y = as.numeric(final_pred_dat$qb_dropback) - 1,\n  ii = final_pred_dat$team_idx,\n  phat = arm::logit(final_pred_dat$dropback_prob)\n)\n\nfit_no_epa <- sampling(stan_mod, data = stan_dat_no_epa, cores = 4, chains = 4, iter = 1000)\n\n\n\nBelow we’ll print some parameters from the model. alpha is the intercept, beta_phat is the coefficient on the predicted pass probability from our xgboost model, and sigma_mu is the standard deviation in team effects. We’d expect a coefficient of 1 on beta_phat, so I should probably go back and look at why it’s coming out a little high. While there’s clearly a difference between beta_phat and our expectation, it’s pretty small in substantive terms. If our xgboost model was saying that the probability of a pass is .6, this model would suggest that that true probability is something like .61 for an average team. The .18 value of sigma_mu means that our predicted probabilities for different teams would range from about .52 on the low end and .69 on the high end for a play where an average team is at .6.\n\n\nprint(fit_no_epa, pars = c(\"alpha\", \"beta_phat\", \"sigma_mu\"))\n\n\nInference for Stan model: pass-prob-stan-model-no-epa.\n4 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=2000.\n\n           mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nalpha     -0.03       0 0.02 -0.07 -0.05 -0.03 -0.02  0.00   425 1.00\nbeta_phat  1.19       0 0.01  1.17  1.18  1.19  1.19  1.20  3526 1.00\nsigma_mu   0.18       0 0.01  0.16  0.18  0.18  0.19  0.21   599 1.01\n\nSamples were drawn using NUTS(diag_e) at Sun Aug 16 20:23:53 2020.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\nTeam Effects\nWe can extract the samples from our model and use them to get our mean parameter estimates as well as the uncertainty in those estimates.\n\n\nsamps_no_epa <- rstan::extract(fit_no_epa, pars = \"mu\")$mu\nquantile_025_no_epa <- apply(samps_no_epa, 2, quantile, .025)\nquantile_975_no_epa <- apply(samps_no_epa, 2, quantile, .975)\nmean_no_epa <- apply(samps_no_epa, 2, mean)\n\nteams <- dat %>%\n  filter(season >= 2016 & !is.na(posteam)) %>%\n  dplyr::select(posteam, season, qb_dropback) %>%\n  mutate(\n    team_string = str_c(posteam, \"-\", season),\n    team_idx = as.numeric(factor(team_string))\n  ) %>%\n  group_by(posteam, season) %>%\n  summarise(\n    team_idx = max(team_idx),\n    dropback_pct = mean(qb_dropback)\n  ) %>%\n  ungroup()\n\nteams$q_025_no_epa <- quantile_025_no_epa\nteams$q_975_no_epa <- quantile_975_no_epa\nteams$mean_no_epa <- mean_no_epa\nteams$display_name <- factor(str_c(teams$posteam, \" - \", teams$season))\nteams$display_name <- fct_reorder(teams$display_name, teams$mean_no_epa)\n\n\n\nThe plots below show the estimated team effects. Note that the effects on the x-axis are on the log-odds scale. The 2018 Seahawks estimate of -.47 means that we would predict a Seahawks pass with probability .38 in a situation where the league-wide probability is .5. We would predict the 2018 Steelers to pass with probability .62 in that same situation.\nOne interesting thing is that, beyond 2018, the Seahawks haven’t been that big of an outlier. They were among the pass-heavier teams in 2016-17 and only slightly below average in 2019. We also see that some teams who run the ball a lot like the Patriots, Rams, and Saints show up as being more aggressive than dropback% would lead us to believe.\n2016\n\n\n\n2017\n\n\n\n2018\n\n\n\n2019\n\n\n\nThe last thing I’ll show is how my estimated pass-heaviness correlates with QB Dropback%. To make the plot below I converted the model estimates and the actual QB dropback% into within-season ranks. Teams above the line are pass-heavier than their unadjusted dropback% would lead us to believe. Teams below the line are run-heavier. I highlight the Patriots to come back to the point at the beginning of the post. The Patriots consistently run more than average but are among the pass-heavier teams once game script is accounted for.\n\n\nteams %>%\n  mutate(\n    display_name = if_else(posteam %in% c(\"NE\"), as.character(display_name), \"\"),\n    posteam = if_else(posteam %in% c(\"NE\"), as.character(posteam), \"\")\n  ) %>%\n  ggplot(aes(y = qb_dropback_rank, x = qb_dropback_est_rank)) +\n  geom_text_repel(aes(label = display_name)) +\n  geom_point(aes(colour = posteam, shape = posteam), size = 2) +\n  ylab(\"Adjusted Dropback% Rank\") +\n  xlab(\"Actual Dropback% Rank\") +\n  geom_smooth(method = \"lm\", alpha = .25) +\n  scale_colour_manual(values = c(\"gray\", \"blue4\")) +\n  theme_minimal() +\n  guides(colour = F, shape = F) +\n  labs(title = \"Adjusted Dropback Rank vs. Actual Dropback Rank\")\n\n\n\n\nConclusions\nWe showed here that we can use NFL play-by-play data to measure game script and better understand team tendencies. After adjusting for game script we find that teams that run the ball the most are not necessarily the run-heaviest teams. Except the 2018 Seahawks. They were the run heaviest team.\nTo go back to the Seahawks, this doesn’t really address the “Let Russ Cook” debate. The 2019 Seahawks weren’t overly run-heavy when game script is taken into account but a big part of this debate is that the Seahawks have a great quarterback which should probably influence how much they use him! In a future post I’ll build QB ability into our model which will give us a better idea of how big an outlier the Seahawks are.\n\nView source code on GitHub \n\n\n\n\n",
    "preview": "posts/2020-09-07-estimating-runpass-tendencies-with-tidymodels-and-nflfastr/estimating-runpass-tendencies-with-tidymodels-and-nflfastr_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-08-21-rerun-exonerating-punters-for-large-returns/",
    "title": "RERUN",
    "description": "Exonerating punters for long returns",
    "author": [
      {
        "name": "Dennis Brookner",
        "url": {}
      },
      {
        "name": "Raphael Laden-Guindon",
        "url": {}
      }
    ],
    "date": "2020-09-06",
    "categories": [],
    "contents": "\nTable of Contents\nRERUN: Return-edited real unbiased netTwo big assumptions\nEra adjustment\nTop punters by this new metric\n\nRERUN: Return-edited real unbiased net\nAnyone who follows our twitter (which, you know, please do!) might have seen our thread a while back about the responsibility of punters for big returns:\n\n\nEvery time a dynamic returner goes off for 20+ yds, we think, was that the punter's fault? And even in less extreme cases, are we swaying our evaluations of punters based on the quality of the coverage team in front of them? We'll spend this thread musing on that question\n\n— Puntalytics (@ThePuntRunts) March 5, 2020\n\nWe’ll spend this article revisiting this topic in a little more detail.\nOh also, we made an R package for puntalytics\nWe’ve been working on puntr, which contains some simple tools for our typical puntalytics. We’ll use puntr here to import and clean our data; we could also use puntr to spit all our stats, but that would defeat the purpose of an article walking through this all!\n\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"Puntalytics/puntr\")\nlibrary(tidyverse)\n\npunts_raw <- puntr::import_punts(1999:2019)\npunts <- punts_raw %>% puntr::trust_the_process()\n\nThe puntr::import_punts() function does exactly what it sounds like; it grabs the data from our github here. The data is just the nflfastR play-by-play, but already filtered down to only punt plays, which makes for a much smaller and more manageable data frame.\nWe’ve wrapped all the standard data cleaning steps into puntr::trust_the_process(). Regretfully, this is a little bit black-box-y, but here are the most important things it does:\nreturns only the columns relevant to puntalytics; custom column selection can be achieved via puntr::punt_trim(), and then using the trim = FALSE flag in puntr::trust_the_process()\ncreates a NetYards column, which is surprisingly not in the default nflfastR data frame\nadjusts touchbacks to treat them as punts to the 20, rather than punts to the 0; the latter is the default, but is silly!\nrenames kick_distance to GrossYards, for our own sanity and prettiness on axis labels\nadds team_logo_espn, team_color, and team_color2 using nflfastR\nTwo big assumptions\nLet’s make two big-but-reasonable-ish assumptions, and then carry out the ramifications\nA punter is 100 % responsible for whether a punt is returned (vs. being downed, out-of-bounds, fair caught, or a touchback)\nA punter is 0 % responsible for the length of that return, once the returner chooses to return\nFor starters, let’s make a note of those returned punts\n\n\npunts <- punts %>%\n  mutate(returned = pmap_dbl(list(punt_out_of_bounds==0 &\n                                  punt_downed==0 &\n                                  punt_fair_catch==0 &\n                                  touchback==0,\n                                  1, 0),\n                             if_else))\n\nAnd now, let’s make new columns for GrossYards and return_yards that only contain values when returned == 1, and are NA everywhere else:\n\n\npunts <- punts %>%\n  mutate(GrossYards_r = pmap_dbl(list(returned==1, \n                                      GrossYards, NA_real_),\n                                      if_else)) %>%\n  mutate(return_yards_r = pmap_dbl(list(returned==1,\n                                        return_yards, NA_real_),\n                                        if_else))\n\nWhat’s the relationship between GrossYards_r and return_yards_r? Let’s plot that, and throw in a trend line while we’re at it.\n\n\nlibrary(ggplot2)\n\nggplot(data=punts, mapping=aes(GrossYards_r, return_yards_r)) +\n  geom_point(alpha = 0.06) +\n  geom_smooth(method=\"loess\", span=0.65) +\n  labs(title = \"How far punts of various lengths are typically returned\",\n       subtitle = \"(when they're returned at all!)\",\n       y=\"Return Yards\", x=\"Gross Yards\", caption=\"figure @ThePuntRunts | data @nflfastR\") +\n  theme_bw()\n\n\nThe crux of this analysis is moving every point on this graph up or down to the trendline!\nAssigning a RERUN value to punts\nFirst, let’s use the same smoothing as in geom_smooth() above to create a new column return_smooth:\n\n\npunts <- punts %>%\n  mutate(return_smooth = loess(formula = return_yards_r ~ GrossYards_r,\n                                        data = punts,\n                                        span=0.65,\n                                        na.action = na.exclude) %>% predict)\n\n(That na.action = na.exclude flag is a life-saver.)\nNow, we’re ready to use return_smooth to create a new version of net! This is what we call RERUN: return-edited real unbiased net. Note that for non-returned punts, RERUN is just equal to GrossYards.\n\n\npunts <- punts %>%\n  mutate(RERUN = pmap_dbl(list(returned==1,\n                               GrossYards_r - return_smooth, GrossYards),\n                          if_else))\n\nSanity checks\nAs a first sanity check, nothing that we did should adjust the overall league average numbers in the data set. Is this true?\n\n\npunts %>% summarise(Net = mean(NetYards),\n                    RERUN = mean(RERUN))\n\n# A tibble: 1 x 2\n    Net RERUN\n  <dbl> <dbl>\n1  37.9  37.9\n\nAwesome! But, there’s one more detail we have to be sure to look at:\nEra adjustment\nLet’s plot Net and RERUN each season in the dataset (1999 - 2019). Remember that RERUN buoys punters with bad coverage; this means that RERUN > Net indicates bad coverage, and Net > RERUN indicates good coverage.\n\n\ntrends <- punts %>% group_by(season) %>%\n  summarise(Net = mean(NetYards),\n            RERUN = mean(RERUN)) %>%\n  gather(Net, RERUN, key=\"type\", value=\"distance\") # gather is a tidyr function and it rocks\n\nggplot(data = trends, mapping = aes(x = season, y = distance, fill = type)) +\n  geom_bar(position = \"dodge\", stat=\"identity\") +\n  coord_cartesian(ylim=c(32,42)) +\n  theme_bw() +\n  labs(title = \"Trends in the league-average difference between Net and RERUN\",\n       y=\"Yards\", x=\"Season\", caption=\"figure @ThePuntRunts | data @nflfastR\")\n\n\nWe notice two trends here. One is that in general, Net has increased over time, and quite a bit! This is actually a pretty persistent problem in puntalytics; we’ve dealt with it in various places, including in our EPA metric, but we won’t address it here. (Side note, stay tuned for an article about our EPA metric, hopefully soon!)\nMore pressing to our current analysis is that by the rough definitions we just set, coverage has also been improving consistently. We’ll account for that by adjusting our “average” (or “expected”) return length based on which season that return happened! Thankfully, the purrr package, along with tidyr::nest(), make this pretty straightforward:\n\n\n# the same model as before, but now expressed as a function:\nrerun_model <- function(input) {\n  loess(formula = return_yards_r ~ GrossYards_r,\n        data = input,\n        span=0.65,\n        na.action = na.exclude)\n}\n\npunts <- punts %>%\n  group_by(season) %>%\n  nest() %>%\n  mutate(model = map(data, rerun_model)) %>%\n  mutate(return_smooth2 = map(model, predict)) %>%\n  unnest(c(data, return_smooth2)) %>%\n  mutate(RERUN2 = pmap_dbl(list(returned==1,\n                                GrossYards_r - return_smooth2, GrossYards),\n                           if_else)) %>%\n  ungroup()\n\nNow if we repeat the same plot as above, we see that we’ve successfully era-adjusted our treatment of coverage (while leaving the trend in net un-adjusted).\n\n\n\nTop punters by this new metric\nHooray! We derived a new metric for punters. Might as well check out the all-time (nflfastR-era) leader board! (As mentioned above, this will be biased towards recent punters.)\n\n\n## Custom mode function, because R somehow doesn't have one built-in??\ngetmode_local <- function(v) {\n  uniqv <- unique(v)\n  uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\nmini <- punts %>%\n  group_by(punter_player_name, season) %>%\n  filter(n() > 32) %>%\n  summarise(RERUN = mean(RERUN2),\n            Gross = mean(GrossYards),\n            Net = mean(NetYards),\n            NumPunts= n(),\n            team_logo_espn = getmode_local(team_logo_espn),\n            team_color = getmode_local(team_color),\n            team_color2 = getmode_local(team_color2)) %>%\n  rename(Name = punter_player_name) %>%\n  arrange(desc(RERUN))\n\nmini %>%\n  mutate(across(is.numeric, round, 1)) %>%\n  select(Name, season, RERUN, Net, Gross, NumPunts) %>%\n  rmarkdown::paged_table()\n\n\n\nIn general, RERUN isn’t that different from regular-old net (and we still get a who’s-who of top punters). When is it the most different? Well, if a punter’s coverage team consistently allows larger-than-average returns, a punter’s net would suffer, but their RERUN wouldn’t. RERUN should be stable even in the face of a poor coverage team.\nIn 2019, which punters were helped or hurt the most by their coverage teams?\n\n\nlibrary(ggimage)\nlibrary(ggrepel)\n\nmini19 <- mini %>%\n  filter(season==2019)\n\nggplot(mini19, mapping=aes(Net, RERUN, label = Name)) +\n  geom_abline() +\n  geom_image(aes(image = team_logo_espn)) +\n  geom_text_repel(size=3, point.padding = 0.6, force=1.25) +\n  annotate(\"text\", x=42.5, y=38.5, label=\"Helped by coverage team\", size=5, fontface = \"bold\") +\n  annotate(\"text\", x=39.5, y=42.5, label=\"Hurt by coverage team\", size=5, fontface = \"bold\") +\n  labs(title = \"Were punters in 2019 helped or hurt by their coverage teams?\",\n       caption=\"figure @ThePuntRunts | data @nflfastR\") +\n  theme_bw()\n\n\nThat’s all we’ve got for now - thanks so much for reading! If you liked what you saw, be sure to follow us on twitter for more analysis like this (including our ongoing project of developing an EPA metric for punters) plus punting highlights throughout the season.\n\n\n",
    "preview": "posts/2020-08-21-rerun-exonerating-punters-for-large-returns/rerun-exonerating-punters-for-large-returns_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-08-25-defense-and-rest-time-re-visited/",
    "title": "Defense and rest time re-visited",
    "description": "Does incorporating actual rest time help us predict how a defense will do?",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin"
      }
    ],
    "date": "2020-08-31",
    "categories": [
      "nflfastR",
      "Articles"
    ],
    "contents": "\n\nContents\nIntroduction\nOverview of rest time\nRest plays versus rest time\nRest time versus field position\nRest time versus defensive results\nWrapping up\n\nIntroduction\nIn this post, we revisit an old piece I did for Football Outsiders. In that piece, I looked at the relationship between defensive rest time and defensive performance, finding no evidence of a relationship. However, without a direct measure of rest time, I had to use plays of rest and time of possession of rest rather than the actual time in minutes that had gone by since the defense took the field. In the time since, nflfastR’s release means we now have the hour, minute, and second of each snap, so we can know how much time goes by between when the defense last took the field and the start of their subsequent defensive drive.\nA few quick notes before we dive in. This piece uses data from the 2015 through 2019 seasons. It would be possible to add more but each additional season involves some manual cleaning to remove games with clock errors and games with delays mid-game (e.g. from lightening), both of which cause all the figures to look bad. In total, I remove five games with delays and six games with assorted problems with the clock time. With five seasons of data, we still have nearly 30,000 drives to analyze.\nBecause this is Open Source Football, you can see how I obtained and cleaned the data by checking the source code (the source code for all posts is located here).\nOverview of rest time\nWith that out of the way, let’s jump in. Which defenses had the shortest rest times before they had to re-take the field since 2015?\n\n\nGame\n\n\nDefense\n\n\nRest seconds\n\n\nPrior drive result\n\n\n2018 Texans @ Colts\n\n\nIND\n\n\n101\n\n\nTurnover\n\n\n2018 Texans @ Patriots\n\n\nNE\n\n\n106\n\n\nTurnover\n\n\n2015 Dolphins @ Patriots\n\n\nMIA\n\n\n107\n\n\nTurnover\n\n\n2015 Jaguars @ Texans\n\n\nHOU\n\n\n108\n\n\nTurnover\n\n\n2016 Raiders @ Broncos\n\n\nLV\n\n\n109\n\n\nTurnover\n\n\nThe shortest defensive rest time since 2015 was the Colts in 2018. At 35 seconds past 2:26pm Eastern, the Texans punted. On the subsequent play, Andrew Luck was sacked by JJ Watt, which led to a fumble recovered by the Texans. The Colts defense then took the field again at 16 seconds past 2:28pm Eastern, 101 seconds after they had last been on the field (this isn’t perfect, but I’m counting punts as a defensve being on a field. This doesn’t make a big difference either way).\nWhat about the longest rest times? Due to the length of Super Bowl halftimes, a lot of Super Bowls show up here:\n\n\nGame\n\n\nDefense\n\n\nRest minutes\n\n\nPrior drive result\n\n\n2016 Super Bowl\n\n\nNE\n\n\n68\n\n\nField goal\n\n\n2019 Super Bowl\n\n\nSF\n\n\n56\n\n\nField goal\n\n\n2016 Chiefs @ Chargers\n\n\nKC\n\n\n55\n\n\nTouchdown\n\n\n2015 Super Bowl\n\n\nCAR\n\n\n55\n\n\nMissed field goal\n\n\n2016 Eagles @ Lions\n\n\nPHI\n\n\n54\n\n\nTouchdown\n\n\nThe longest a defense has rested since 2015 came in Super Bowl LI. Midway through the second quarter, Matt Ryan threw a touchdown pass to Austin Hooper. The time was 7:27pm. On the subsequent drive, the Patriots drove down the field until Robert Alford picked off Tom Brady and returned it for a touchdown. The Patriots then received the ball again, driving down the field and kicking a field goal right before the second half. Following a typically long Super Bowl halftime show, the Falcons received the opening kickoff. By the time the Patriots’ defense took the field again, it was 8:36pm (the other games shown had long drives on either side of the half).\nWhile we’ve been focusing on the extremes so far, let’s take a look at the distribution of how long a defense rests before it re-takes the field.\n\n\n\nAs shown above, defenses typically get around 5-20 minutes of rest, with the modal time being exactly 10 minutes (I’m truncating drives over 40 minutes for this plot since they are rare, though as we’ve already seen, they exist).\nRest plays versus rest time\nLet’s return to what we’re actually interested in. Is using the total plays a defense has spent resting a reasonable proxy for how long they have actually rested?\n\n\n\nIn the figure above, each point represents one defensive drive, with the horizontal axis being how many plays throughout the game the defense has spent on the sidelines up to that point, and the vertical axis the number of minutes.\nI’ve broken drives down by half because all defenses get some extra rest time at halftime, but aside from that, there’s a very strong relationship between the number of plays a defense has spent resting and the actual amount of time it has been on the sidelines. Thus, an initial finding is that we probably haven’t been missing much by just measuring plays of rest time rather than actual time.\nWhile the above shows the cumulative amount of time spent resting, we can see a similar pattern when only focusing on the amount of rest time before a given drive:\n\n\n\nAgain, teams get some extra rest during halftime so the initial drive of the third quarter for each defense is systematically different than the others. Aside from that, the number of plays and the amount of time a defense has recently spent off the field tracks pretty closely.\nSomething that jumps out in the figure above is the number of drives where the defense only had a play or two of rest. These are typically turnovers where the defense doesn’t have much time to rest before it has to go right back on the field. Thus, we would expect a relationship between how long a defense has rested and the field position it finds itself in.\nRest time versus field position\nWe indeed observe a relationship between field position versus time spent resting, where low rest time is associated with bad field position from the perspective of the defense. If your offense went three and out or committed a quick turnover, you have probably put your defense in a bad position.\n\n\n\nThis means that, as in the original piece, it’s important to separate out field position effects from rest time effects. If a defense had short rest time, it’s likely that it’s also beginning with poor field position.\nRest time versus defensive results\nTo handle the field position issue, I subtract off the expected points at a given yardline to obtain points over expected. Another factor to consider is the ends of halves. A lot of the short rest time drives are near the end of the half when networks have already gotten in the TV timeouts required, so there isn’t an extended break between possessions like normal. But end-of-half situations are also less likely to score because of the clock being a factor. So in creating the expected points for a drive, I also account for the seconds remaining in a half when the drive started.\nIf rested defenses perform better, all else equal, we would expect to see a decreasing relationship between points per drive over expected and rest time. Let’s take a look:\n\n\n\nInstead of a decreasing relationship, we have a weakly positive one, but pretty close to zero throughout the bulk of the distribution (e.g., between 5 and 20 minutes). So there doesn’t seem to be much here.\nWrapping up\nThis has hopefully been a useful peak at what we can do with the newly-available time of day information in nflfastR. After taking a tour of this data applied to rest time, I don’t think I’ve seen anything to change the conclusions about defensive rest obtained from measuring plays or time of possession. The most direct way an offense affects its team’s defense is through field position, with all other effects (including rest time) being secondary at best.\nIn the future, it might be interesting to look at whether repeated short drives by an offense have a measurable effect on defensive performance. Given what we’ve already seen, my guess would be no, but it can’t hurt to find out.\n\n\n\n",
    "preview": "posts/2020-08-25-defense-and-rest-time-re-visited/defense-and-rest-time-re-visited_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-08-30-calculating-expected-fantasy-points-for-receivers/",
    "title": "Calculating Expected Fantasy Points for Receivers",
    "description": "Use the nflfastR xYAC & CP models to calculate how many fantasy points an average receiver would expect to earn on each target.",
    "author": [
      {
        "name": "Anthony Reinhard",
        "url": "https://twitter.com/reinhurdler"
      }
    ],
    "date": "2020-08-31",
    "categories": [
      "Fantasy Football",
      "nflfastR",
      "nflfastR xYAC Model"
    ],
    "contents": "\nTable of Contents\nIntro\nThe YAC Distribution Function\nThe Data\nThe Distribution\nNext Steps\nIntro\nIn fantasy football, volume will always be king. That being said, not all volume is necessarily equal. A running back who gets five carries inside the five yard line will be far more valuable than a running back who gets five carries 80 yards from the end zone because of the likelihood of the first player scoring a touchdown. Touchdowns (and other big plays) have an outsized significance in fantasy, and advanced play-by-play data can help us identify instances where players are overachieving at an unsustainable rate.\nIn early August of this year, Ben introduced a model that will estimate yards after the catch based on air yards, down & distance, yard line, and a number of other factors. The model provides the probability that a player will advance to any particular yard line, given that they make the catch. Upon seeing this, I was immediately reminded of an article I had seen some time ago by Mike Clay, which was aimed at estimating the opportunity a player had to score a touchdown based on where they received the ball. Since nflfastR already has a completion probability model, it would be relatively simple to combine these models together to calculate the outcome of any single result for a given pass play. I shared some initial thoughts/visuals on twitter for this idea, and I thought it would be great if others had some code to play around with this as well.\nSomething important to remember about this concept is that it merely calculates expected fantasy points for the average receiver. Receivers who are especially skilled at gaining yards after the catch or who catch more passes than expected will typically over perform their mean expectation on this metric. I’ll also add to the latter point: we can say with some degree of confidence that, all else being equal, the quarterback is most likely more responsible for completing a pass than the receiver. This means that for the purposes of the expected fantasy points we’ll be calculating below, we should take the quality of quarterback into account.\nAll of this code will be written for PPR scoring, but it would not be difficult to adjust any of this for standard scoring, half-point PPR, or even Scott Fish Bowl scoring (although you would need some kind of roster table to look up positions). I will also add that this will not take fumbles, two-point conversions, or rushing plays into account.\nThe YAC Distribution Function\nAs of this writing, nflfastR does not have a built-in function that provides the full distribution of outcomes for YAC on each play. While that may be available at some point in the future, the easiest solution right now is to make our own adjustments to the add_xyac function so that we can get the raw xYAC model output. The intended purpose of add_xyac is to add five fields (xyac_epa, xyac_success, xyac_fd, xyac_mean_yardage, and xyac_median_yardage) to the play-by-play data frame. We’re going to break the function up into blocks, remove 12 rows that basically serve to summarize the model output, and reassemble it so that the function will return a row for each possible yardage outcome. While we won’t actually be need nflfastR library for this, we will be sourcing the file that has the xyac functions and also the file that makes some mutations.\n\n\nlibrary(tidyverse)\n\nsource('https://github.com/mrcaseb/nflfastR/raw/master/R/helper_add_xyac.R')\nsource('https://github.com/mrcaseb/nflfastR/raw/master/R/helper_add_nflscrapr_mutations.R')\n\n# duplicate the add_xyac() function that we sourced above\nadd_xyac_dist <- add_xyac\n\n# separate each block of code in the add_xyac_dist() function into blocks\nadd_xyac_blocks <- body(add_xyac_dist) %>% as.list\n\n# we want to remove lines 51 to 62 from the 5th item in the list\nadd_xyac_blocks[[5]] <- add_xyac_blocks[[5]] %>% \n  format %>% \n  .[-(51:62)] %>% \n  paste(collapse = '\\n') %>% \n  str2lang\n\n# replace the body of add_xyac_dist() with our new edited function\nbody(add_xyac_dist) <- add_xyac_blocks %>% as.call\n\nThe Data\nNow that we’ve got our function squared away, we can focus on assembling the data. We’ll pull in the 2019 data set and keep only regular season pass plays from scrimmage where a player was actually targeted. The table we ultimately create will summarize expected stats and actual stats for each player last season. This can obviously be summarized at the game level as well.\n\n\npbp_df <- readRDS(url('https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_2019.rds'))\n\navg_exp_fp_df <- pbp_df %>% \n  filter(pass_attempt==1 & season_type=='REG' & two_point_attempt==0 & !is.na(receiver_id)) %>% \n  add_xyac_dist %>% \n  select(season = season.x, game_id, play_id, posteam = posteam.x, receiver, yardline_100 = yardline_100.x, air_yards = air_yards.x, actual_yards_gained = yards_gained, complete_pass, cp, yac_prob = prob, gain) %>% \n  mutate(\n    gain = ifelse(yardline_100==air_yards, yardline_100, gain),\n    yac_prob = ifelse(yardline_100==air_yards, 1, yac_prob),\n    PPR_points = 1 + gain/10 + ifelse(gain == yardline_100, 6, 0),\n    catch_run_prob = cp * yac_prob,\n    exp_PPR_points = PPR_points * catch_run_prob,\n    exp_yards = gain * catch_run_prob,\n    actual_outcome = ifelse(actual_yards_gained==gain & complete_pass==1, 1, 0),\n    actual_PPR_points = ifelse(actual_outcome==1, PPR_points, 0),\n    target = 0,\n    game_played = 0\n  )  %>% \n  group_by(game_id, receiver) %>% \n  mutate(game_played = ifelse(row_number()==1,1,0)) %>% \n  ungroup %>% \n  group_by(game_id, play_id, receiver) %>% \n  mutate(target = ifelse(row_number()==1,1,0)) %>% \n  ungroup %>% \n  group_by(posteam, receiver) %>% \n  summarize(\n    games = sum(game_played, na.rm = T),\n    targets = sum(target, na.rm = T),\n    catches = sum(actual_outcome, na.rm = T),\n    yards = sum(ifelse(actual_outcome==1, gain, 0), na.rm = T),\n    td = sum(ifelse(gain==yardline_100, actual_outcome, 0), na.rm = T),\n    PPR_pts = sum(actual_PPR_points, na.rm = T),\n    exp_catches = sum(ifelse(target==1, cp, NA), na.rm = T),\n    exp_yards = sum(exp_yards, na.rm = T),\n    exp_td = sum(ifelse(gain==yardline_100, catch_run_prob, 0), na.rm = T),\n    exp_PPR_pts = sum(exp_PPR_points, na.rm = T)\n  ) %>% \n  ungroup\n\nLet’s make a table using the gt package to show the top 25 players in expected fantasy points last season. It looks like OBJ under performed pretty severely, while Cooper Kupp scored about four and a half more TDs than expected!\n\n\nlibrary(gt)\n\n# make the table\navg_exp_fp_df %>% \n  arrange(-exp_PPR_pts) %>% \n  dplyr::slice(1:25) %>% \n  mutate(Rank = paste0('#',row_number())) %>%\n  gt() %>%\n  tab_header(title = 'Expected Receiving PPR Fantasy Points, 2019') %>% \n  cols_move_to_start(columns = vars(Rank)) %>% \n  cols_label(\n    games = 'GP',\n    receiver = '',\n    posteam = '',\n    targets = 'Targ',\n    catches = 'Rec',\n    yards = 'Yds',\n    td = 'TD',\n    PPR_pts = 'FP',\n    exp_catches = 'Rec',\n    exp_yards = 'Yds',\n    exp_td = 'TD',\n    exp_PPR_pts = 'FP'\n  ) %>% \n  fmt_number(columns = vars(exp_td, PPR_pts, exp_PPR_pts), decimals = 1) %>% \n  fmt_number(columns = vars(yards, exp_yards, exp_catches), decimals = 0, sep_mark = ',') %>% \n  tab_style(style = cell_text(size = 'x-large'), locations = cells_title(groups = 'title')) %>% \n  tab_style(style = cell_text(align = 'center', size = 'medium'), locations = cells_body()) %>% \n  tab_style(style = cell_text(align = 'left'), locations = cells_body(vars(receiver))) %>% \n  tab_spanner(label = 'Actual', columns = vars(catches, yards, td, PPR_pts)) %>% \n  tab_spanner(label = 'Expected', columns = vars(exp_catches, exp_yards, exp_td, exp_PPR_pts)) %>% \n  tab_source_note(source_note = '') %>% \n  data_color(\n    columns = vars(PPR_pts, exp_PPR_pts),\n    colors = scales::col_numeric(palette = c('grey97', 'darkorange1'), domain = c(180,380)),\n    autocolor_text = FALSE\n  ) %>%\n  text_transform(\n    locations = cells_body(vars(posteam)),\n    fn = function(x) web_image(url = paste0('https://a.espncdn.com/i/teamlogos/nfl/500/',x,'.png'))\n  ) %>% \n  cols_width(vars(posteam) ~ px(45)) %>% \n  tab_options(\n    table.font.color = 'darkblue',\n    data_row.padding = '2px',\n    row_group.padding = '3px',\n    column_labels.border.bottom.color = 'darkblue',\n    column_labels.border.bottom.width = 1.4,\n    table_body.border.top.color = 'darkblue',\n    row_group.border.top.width = 1.5,\n    row_group.border.top.color = '#999999',\n    table_body.border.bottom.width = 0.7,\n    table_body.border.bottom.color = '#999999',\n    row_group.border.bottom.width = 1,\n    row_group.border.bottom.color = 'darkblue',\n    table.border.top.color = 'transparent',\n    table.background.color = '#F2F2F2',\n    table.border.bottom.color = 'transparent',\n    row.striping.background_color = '#FFFFFF',\n    row.striping.include_table_body = TRUE\n  )\nExpected Receiving PPR Fantasy Points, 2019\n    \n    Rank\n      \n      \n      GP\n      Targ\n      \n        Actual\n      \n      \n        Expected\n      \n    Rec\n      Yds\n      TD\n      FP\n      Rec\n      Yds\n      TD\n      FP\n    #1\n      \n      M.Thomas\n      16\n      185\n      149\n      1,725\n      9\n      375.5\n      128\n      1,480\n      6.8\n      317.2\n    #2\n      \n      J.Jones\n      15\n      157\n      99\n      1,394\n      6\n      274.4\n      98\n      1,444\n      6.7\n      282.1\n    #3\n      \n      A.Robinson\n      16\n      154\n      98\n      1,147\n      7\n      254.7\n      95\n      1,311\n      7.2\n      269.4\n    #4\n      \n      D.Hopkins\n      15\n      150\n      104\n      1,165\n      7\n      262.5\n      98\n      1,251\n      7.4\n      267.3\n    #5\n      \n      J.Edelman\n      16\n      153\n      99\n      1,117\n      6\n      246.7\n      100\n      1,230\n      6.8\n      263.7\n    #6\n      \n      K.Allen\n      16\n      149\n      104\n      1,199\n      6\n      259.9\n      93\n      1,192\n      6.9\n      253.3\n    #7\n      \n      T.Boyd\n      16\n      148\n      90\n      1,046\n      5\n      224.6\n      96\n      1,193\n      4.5\n      241.8\n    #8\n      \n      T.Kelce\n      16\n      136\n      96\n      1,248\n      5\n      250.8\n      88\n      1,066\n      6.8\n      235.7\n    #9\n      \n      O.Beckham\n      16\n      133\n      73\n      946\n      3\n      185.6\n      80\n      1,147\n      6.7\n      235.1\n    #10\n      \n      Z.Ertz\n      15\n      135\n      88\n      916\n      6\n      215.6\n      88\n      1,063\n      6.7\n      234.4\n    #11\n      \n      D.Parker\n      16\n      128\n      72\n      1,202\n      9\n      246.2\n      75\n      1,169\n      6.9\n      232.9\n    #12\n      \n      J.Landry\n      16\n      138\n      83\n      1,180\n      6\n      237.0\n      85\n      1,093\n      6.3\n      232.5\n    #13\n      \n      D.Moore\n      15\n      135\n      87\n      1,175\n      4\n      228.5\n      84\n      1,120\n      5.7\n      230.5\n    #14\n      \n      M.Evans\n      13\n      118\n      67\n      1,157\n      8\n      230.7\n      65\n      1,076\n      9.5\n      229.7\n    #15\n      \n      K.Golladay\n      16\n      116\n      65\n      1,190\n      11\n      250.0\n      65\n      1,060\n      9.6\n      229.1\n    #16\n      \n      R.Woods\n      15\n      139\n      90\n      1,134\n      2\n      215.4\n      92\n      1,118\n      3.8\n      227.1\n    #17\n      \n      D.Adams\n      12\n      127\n      83\n      997\n      5\n      212.7\n      82\n      997\n      7.4\n      225.9\n    #18\n      \n      C.Sutton\n      16\n      124\n      72\n      1,112\n      6\n      219.2\n      73\n      1,020\n      7.7\n      221.0\n    #19\n      \n      C.Kupp\n      16\n      134\n      94\n      1,161\n      10\n      270.1\n      89\n      977\n      5.6\n      220.1\n    #20\n      \n      C.Godwin\n      14\n      121\n      86\n      1,333\n      9\n      273.3\n      76\n      1,024\n      6.3\n      216.7\n    #21\n      \n      A.Cooper\n      16\n      119\n      79\n      1,189\n      8\n      245.9\n      74\n      1,109\n      5.2\n      215.7\n    #22\n      \n      C.McCaffrey\n      16\n      142\n      116\n      1,005\n      4\n      240.5\n      112\n      825\n      3.1\n      213.8\n    #23\n      \n      D.Chark Jr.\n      15\n      118\n      73\n      1,008\n      8\n      221.8\n      72\n      987\n      6.8\n      211.0\n    #24\n      \n      T.Lockett\n      16\n      110\n      82\n      1,057\n      8\n      235.7\n      65\n      906\n      7.6\n      201.1\n    #25\n      \n      J.Brown\n      15\n      115\n      72\n      1,060\n      6\n      214.0\n      65\n      1,039\n      4.9\n      198.8\n    \n    \n\nThe Distribution\nEstimating the mean is informative, but doesn’t give us much depth. A great thing about these models is they make it easy to estimate the distribution of outcomes for expected fantasy points. For this example, let’s take a look at Sammy Watkins Week 1 explosion and subsequent Week 2 letdown from last season.\n\n\nfant_pt_dist_df <- pbp_df %>% \n  filter(pass_attempt==1 & season_type=='REG' & two_point_attempt==0 & !is.na(receiver_id) & receiver == 'S.Watkins' & week <= 2) %>% \n  add_xyac_dist %>% \n  select(season = season.x, game_id, play_id, posteam = posteam.x, receiver, yardline_100 = yardline_100.x, air_yards = air_yards.x, actual_yards_gained = yards_gained, complete_pass, cp, yac_prob = prob, gain) %>% \n  mutate(\n    gain = ifelse(yardline_100==air_yards, yardline_100, gain),\n    yac_prob = ifelse(yardline_100==air_yards, 1, yac_prob),\n    PPR_points = 1 + gain/10 + ifelse(gain == yardline_100, 6, 0),\n    catch_run_prob = cp * yac_prob,\n    exp_PPR_points = PPR_points * catch_run_prob,\n    actual_outcome = ifelse(actual_yards_gained==gain & complete_pass==1, 1, 0),\n    actual_PPR_points = ifelse(actual_outcome==1, PPR_points, 0),\n    target = 0,\n    game_played = 0\n  )\n\nincomplete_df <- fant_pt_dist_df %>% \n  mutate(\n    gain = 0,\n    PPR_points = 0,\n    yac_prob = 0,\n    exp_PPR_points = 0,\n    complete_pass = 0,\n    catch_run_prob = 1 - cp,\n    actual_outcome = NA,\n    actual_PPR_points = NA,\n    target = 1\n  ) %>% \n  distinct %>% \n  group_by(game_id, receiver) %>% \n  mutate(game_played = ifelse(row_number()==1,1,0)) %>% \n  ungroup\n\nNow we can take the outcomes above and simulate each play 10,000 times and summarize them at the player level. This will take a couple of minutes in this case, but may take a bit more time depending on the number of plays you’re trying to simulate outcomes for.\n\n\n# make a data frame to loop around\nsampling_df <- rbind(incomplete_df, fant_pt_dist_df) %>% \n  select(season, game_id, play_id, posteam, receiver, catch_run_prob, PPR_points) %>% \n  group_by(game_id, play_id)\n         \n# do sim\nsim_df <- do.call(rbind, lapply(1:10000, function(x) {\n  sampling_df %>% \n    mutate(sim_res = sample(PPR_points, 1, prob = catch_run_prob)) %>% \n    select(season, game_id, play_id, posteam, receiver, sim_res) %>% \n    distinct %>% \n    group_by(game_id, posteam, receiver) %>% \n    summarize(sim_tot = sum(sim_res, na.rm = T), .groups = 'drop') %>% \n    return\n}))\n\nsim_df <- sim_df %>% mutate(sim = 1)\n\n# calculate how many points were actually scored\nactual_df <- fant_pt_dist_df %>%\n  group_by(game_id, posteam, receiver) %>% \n  summarize(sim_tot = sum(actual_PPR_points, na.rm = T), .groups = 'drop') %>% \n  mutate(sim = 0)\n\n# figure out what percentile the actual values fall in\npercentile_df <- rbind(sim_df, actual_df) %>% \n  group_by(game_id, posteam, receiver) %>% \n  mutate(perc = percent_rank(sim_tot)) %>% \n  filter(sim == 0)\n\nWatkins converted his 11 targets into 9 catches for 198 yards and three scores in Week 1, good for 46.8 PPR fantasy points which is in the 99th percentile of the outcomes that we simulated. Despite being targeted 13 times in Week 2, Watkins finished with a mere 10.9 PPR fantasy points. This outcome fell in the 4th percentile.\n\n\nlibrary(scales)\n\n\nggplot(data = sim_df, aes(x = sim_tot, group = game_id, color = game_id, fill = game_id)) +\n  geom_density(alpha = 0.1, size = 1) +\n  geom_spoke(data = percentile_df, aes(angle = pi/2, radius = 0.01, y = 0), size = 1, show.legend = F)  + \n  geom_label(data = percentile_df, aes(y = 0.01, label = paste0('Actual\\n',game_id,'\\n',number(round(perc*100,2),accuracy = 0.1), ' perc.')), size = 2, fill = 'grey98', show.legend = F)  + \n  scale_x_continuous(expand = expansion(mult = c(0.01, 0.01))) +\n  scale_y_continuous(labels = percent_format(accuracy = 1), expand = expansion(mult = c(0, 0.05))) +\n  scale_color_manual(values = c('#ff7f00','#9932cc')) +\n  scale_fill_manual(values = c('#ff7f00','#9932cc')) +\n  labs(title = 'Sammy Watkins Expected PPR Fantasy Point Distribution',\n       subtitle = 'Based on 10,000 Simulations',\n       y = 'Density',\n       x = 'Expected PPR Fantasy Points',\n       color = NULL,\n       fill = NULL) +\n  theme(\n    line = element_line(lineend = 'round', color='darkblue'),\n    text = element_text(color='darkblue'),\n    plot.background = element_rect(fill = 'grey95', color = 'transparent'),\n    panel.border = element_rect(color = 'darkblue', fill = NA),\n    panel.background = element_rect(fill = 'white', color = 'transparent'),\n    axis.ticks = element_line(color = 'darkblue', size = 0.5),\n    axis.ticks.length = unit(2.75, 'pt'),\n    axis.title = element_text(size = 8),\n    axis.text = element_text(size = 7, color = 'darkblue'),\n    plot.title = element_text(size = 14),\n    plot.subtitle = element_text(size = 8),\n    plot.caption = element_text(size = 5),\n    legend.background = element_rect(fill = 'grey90', color = 'darkblue'),\n    legend.key = element_blank(),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color='grey85', size = 0.3),\n    axis.title.y = element_text(angle = 0, vjust = 0.5),\n    legend.position = 'bottom'\n  ) \n\n\nComparing Watkins’ first two weeks, one could reasonably assume that he would eventually steady out in the high teens or low 20s and wind up as a top three option by the end of the season. Unfortunately, Watkins was unable to retain his target share (volume is still king!) and later missed essentially three games due to injury.\nThis example captures the strengths and weakness of this process pretty well. On one hand, we’ve identified that Kansas City’s offense is capable of serving up a juicy fantasy game (shocker). We also were able to set more realistic expectations for Sammy Watkins moving forward by dismissing some outlier performances. On the other hand, we still can’t assume any single game is incredibly predictive week-to-week, as Watkins relinquished his alpha role shortly after these performances. A lot of a receiver’s volume will ultimately depend on the game script and health of his peers.\nNext Steps\nThere is plenty of room to explore this approach further, especially when it comes to making this metric more predictive. One thought I’ve had is to incorporate a prior for the CPOE of starting QB and YAC over expected for the receiving player. The YAC & CP models also might look different if positions were taken into account. They are not included in the model as of this writing due to concerns over data availability. I would also be interested in seeing how this might look for QBs, although fantasy points from rushing plays would be excluded. In theory, one could create a rushing play model in the same way as the xYAC model.\nBig thanks to Ben & Sebastian for creating and maintaining the models used here. This would have been impossible without them!\n\n\n",
    "preview": "posts/2020-08-30-calculating-expected-fantasy-points-for-receivers/calculating-expected-fantasy-points-for-receivers_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-08-29-adding-espn-and-538-game-predictions-to-nflfastr-data/",
    "title": "Adding ESPN and 538 Game Predictions to nflfastR Data",
    "description": "Here, we'll look at how to scrape ESPN's and 538's pregame predictions and merge them into nflfastR data",
    "author": [
      {
        "name": "Jonathan Goldberg",
        "url": "https://twitter.com/gberg1303"
      }
    ],
    "date": "2020-08-29",
    "categories": [
      "Scraping",
      "Game Predicitions",
      "nflfastR"
    ],
    "contents": "\nIn this article, we are going to (1) take a look at how to scrape pregame predictions from 538 and ESPN and (2) how to merge those predictions into nflfastR’s dataset.\nLet’s start by loading up the nflfastR data. To save some time, we’re only going to load the schedules for the last two seasons (2018 and 2019).\n\n\nlibrary(tidyverse)\nNFL_Games <- nflfastR::fast_scraper_schedules(2018:2019) %>%\n  dplyr::mutate(\n    gameday = lubridate::as_date(gameday)\n  )\n\n538:\n538’s data is super easy to grab. They have a repo of NFL games, each team’s ELO ratings before the game, and their predictions for the outcome of the game.\n\n\nFiveThirtyEight_Predictions <- read_csv(\"https://projects.fivethirtyeight.com/nfl-api/nfl_elo.csv\")\n\nThe last thing we need to do is to change some of their labels for NFL teams. Washington and Oakland (Las Vegas) are the two problem children.\n\n\nFiveThirtyEight_Predictions <- FiveThirtyEight_Predictions %>%\n  dplyr::mutate(\n    team1 = gsub(\"WSH\", \"WAS\", team1),\n    team2 = gsub(\"WSH\", \"WAS\", team2),\n    team1 = gsub(\"LAR\", \"LA\", team1),\n    team2 = gsub(\"LAR\", \"LA\", team2)\n  ) %>%\n  dplyr::select(date, season, team1, team2, elo_prob1) %>%\n  dplyr::rename(fivethirtyeight_home_wp = elo_prob1)\n\nNow we can merge!\n\n\nNFL_Games <- NFL_Games %>%\n  left_join(\n    FiveThirtyEight_Predictions,\n    by = c(\"home_team\" = \"team1\", \"away_team\" = \"team2\", \"gameday\" = \"date\", \"season\")\n  )\n\nESPN:\nESPN is going to be pretty tricky, but they also include moneyline odds from some sportsbooks. It may be worth the extra effort to convert them to probabilities.\nUnfortunately, the data on ESPN’s pregame predictions cannot be simply scraped from their website nor are they in a repo. To complicate this even more, ESPN has a unique identification system that does not follow a reproducible pattern. To get their game IDs, you have to scrape them from the schedule page for each week of the NFL season.\nI’ve built a function that will help with exactly that.\n\n\nget_game_ids <- function(season, season_type = c(\"preseason\", \"regular\", \"postseason\")) {\n  current_year <- as.double(substr(Sys.Date(), 1, 4))\n  espn_game_ids <- data.frame()\n\n  if (!season_type %in% c(\"preseason\", \"regular\", \"postseason\", \"all\")) {\n    stop(\"Please choose season_type of 'regular',  'playoffs', 'postseason', or 'all'\")\n  }\n\n  if (!dplyr::between(as.numeric(season), 2002, current_year)) {\n    stop(paste(\"Please choose season between 2002 and\", current_year))\n  }\n\n  if (lubridate::month(Sys.Date()) < 12 & lubridate::month(Sys.Date()) > 2 & season_type == \"postseason\" & current_year == season | season_type == \"postseason\" & lubridate::month(Sys.Date()) <= 2 & current_year == season) {\n    stop(paste(\"Unfortunately, the NFL Playoff Games have not been determined yet\"))\n  }\n\n  message(\n    dplyr::if_else(\n      season_type == \"regular\",\n      glue::glue(\"Scraping from {season} {season_type} season!\"),\n      glue::glue(\"Scraping from {season} {season_type}!\")\n    )\n  )\n\n  season_type <- ifelse(season_type == \"preseason\", \"1\", season_type)\n  season_type <- ifelse(season_type == \"regular\", \"2\", season_type)\n  season_type <- ifelse(season_type == \"postseason\", \"3\", season_type)\n\n  weeks <- ifelse(season_type == \"2\", 17, 5)\n\n  espn_game_ids <- purrr::map_df(1:weeks, function(week) {\n    url <- glue::glue(\"https://www.espn.com/nfl/schedule/_/week/{week}/year/{season}/seasontype/{season_type}\")\n\n    webpage <- xml2::read_html(url)\n\n    links <- webpage %>%\n      rvest::html_nodes(\"a\") %>%\n      rvest::html_attr(\"href\")\n\n    espn_gameid <- links %>%\n      as.tibble() %>%\n      dplyr::filter(str_detect(value, \"gameId\") == TRUE) %>%\n      dplyr::pull(value) %>%\n      stringr::str_remove(., \"/nfl/game/_/gameId/\")\n\n    bye_teams <- webpage %>%\n      rvest::html_nodes(\".odd.byeweek\") %>%\n      rvest::html_nodes(\"abbr\") %>%\n      rvest::html_text()\n\n    home_team <- webpage %>%\n      rvest::html_nodes(\".home-wrapper\") %>%\n      rvest::html_nodes(\"abbr\") %>%\n      rvest::html_text()\n\n    away_team <- webpage %>%\n      rvest::html_nodes(\"abbr\") %>%\n      rvest::html_text()\n    away_team <- away_team[!away_team %in% home_team]\n    away_team <- away_team[!away_team %in% bye_teams]\n\n    placeholder <- data.frame(\n      home_team,\n      away_team,\n      espn_gameid\n    ) %>%\n      dplyr::mutate(\n        season_type = season_type,\n        season = season,\n        week = ifelse(season_type == 3, 17 + week, week)\n      )\n\n    espn_game_ids <- dplyr::bind_rows(espn_game_ids, placeholder)\n    return(espn_game_ids)\n  })\n\n  ### Fix Several Names for Compatibility with nflfastR Data game_ids\n  espn_game_ids <- espn_game_ids %>%\n    dplyr::mutate(\n      home_team = gsub(\"WSH\", \"WAS\", home_team),\n      away_team = gsub(\"WSH\", \"WAS\", away_team),\n      home_team = gsub(\"LAR\", \"LA\", home_team),\n      away_team = gsub(\"LAR\", \"LA\", away_team)\n    ) %>%\n    # Add nflfastR game_ids\n    dplyr::mutate(\n      week = ifelse(week == 22, week - 1, week),\n      alt_gameid = paste0(season, \"_\", ifelse(week >= 10, paste0(week), paste0(0, week)), \"_\", away_team, \"_\", home_team)\n    )\n\n  return(espn_game_ids)\n}\n\nThis is only step one, but pulling the pregame predictions from ESPN’s API is rather easy now that we have the IDs for each game. We simply plug them into the link to their json file, do some cleaning along the way, and extract the prediction.\n\n\n# Get Game IDs\nESPN_Games <- purrr::map_df(2018:2019, function(x) {\n  get_game_ids(x, season_type = \"regular\")\n})\n\nhead(ESPN_Games)\n\n  home_team away_team espn_gameid season_type season week\n1       PHI       ATL   401030710           2   2018    1\n2       CLE       PIT   401030718           2   2018    1\n3       IND       CIN   401030717           2   2018    1\n4       MIA       TEN   401030716           2   2018    1\n5       MIN        SF   401030715           2   2018    1\n6        NE       HOU   401030714           2   2018    1\n       alt_gameid\n1 2018_01_ATL_PHI\n2 2018_01_PIT_CLE\n3 2018_01_CIN_IND\n4 2018_01_TEN_MIA\n5  2018_01_SF_MIN\n6  2018_01_HOU_NE\n\n\n\n# Pull Pregame Predictions\nESPN_Game_Predictions <- purrr::map_df(ESPN_Games$espn_gameid, function(espn_game_id) {\n  pregame_predictions <- data.frame(espn_gameid = espn_game_id)\n\n  # Pull the JSon\n  game_json <- httr::GET(url = glue::glue(\"http://site.api.espn.com/apis/site/v2/sports/football/nfl/summary?event={espn_game_id}\")) %>%\n    httr::content(as = \"text\", encoding = \"UTF-8\") %>%\n    jsonlite::fromJSON(flatten = TRUE)\n\n\n  # Pull the game data from the ID dataframe\n  if (\"gameProjection\" %in% names(game_json[[\"predictor\"]][[\"homeTeam\"]]) == TRUE) {\n    pregame_predictions <- pregame_predictions %>%\n      dplyr::mutate(\n        espn_home_wp = as.numeric(game_json[[\"predictor\"]][[\"homeTeam\"]][[\"gameProjection\"]]) / 100\n      )\n    message(\n      paste(\"Pulling predictions for\", pregame_predictions$alt_gameid)\n    )\n  }\n\n\n  # Grab and convert the Moneylines from Oddsmakers\n  if (\"pickcenter\" %in% names(game_json) == TRUE &\n    \"provider.name\" %in% names(game_json[[\"pickcenter\"]]) == TRUE &\n    \"homeTeamOdds.moneyLine\" %in% names(game_json[[\"pickcenter\"]]) == TRUE\n  ) {\n    vegas_odds <- data.frame(\n      providers = game_json[[\"pickcenter\"]][[\"provider.name\"]],\n      odds = ifelse(game_json[[\"pickcenter\"]][[\"homeTeamOdds.moneyLine\"]] > 0, 100 / (game_json[[\"pickcenter\"]][[\"homeTeamOdds.moneyLine\"]] + 100), game_json[[\"pickcenter\"]][[\"homeTeamOdds.moneyLine\"]] / (game_json[[\"pickcenter\"]][[\"homeTeamOdds.moneyLine\"]] - 100))\n    ) %>%\n      tidyr::pivot_wider(names_from = providers, values_from = odds)\n\n    pregame_predictions <- cbind(\n      pregame_predictions, vegas_odds\n    )\n  }\n\n  return(pregame_predictions)\n})\n\n# Merge ESPN Data together\nESPN_Games <- ESPN_Games %>%\n  left_join(\n    ESPN_Game_Predictions,\n    by = c(\"espn_gameid\" = \"espn_gameid\")\n  )\n\n# Merge back to main data\nNFL_Games <- NFL_Games %>%\n  left_join(\n    ESPN_Games %>% select(alt_gameid, espn_home_wp, Caesars, numberfire, teamrankings, consensus),\n    by = c(\"game_id\" = \"alt_gameid\")\n  )\n\nNow that it’s all together, let’s take a look at the accuracy of ESPN’s, 538’s, Numberfire’s predictions over the last two seasons.\n\n\n### Do some data wrangling first\nNFL_Games <- NFL_Games %>%\n  mutate(\n    home_win = ifelse(home_score > away_score, 1, 0),\n    correct_espn = ifelse(ifelse(espn_home_wp > .5, 1, 0) == home_win, 1, 0),\n    correct_numberfire = ifelse(ifelse(numberfire > .5, 1, 0) == home_win, 1, 0),\n    correct_fivethirtyeight = ifelse(ifelse(fivethirtyeight_home_wp > .5, 1, 0) == home_win, 1, 0)\n  )\nAccuracy_Dataset <- NFL_Games %>%\n  # Filter out Playoff Games\n  filter(game_type == \"REG\") %>%\n  # Pivot Longer to allow group_by and summarize\n  pivot_longer(\n    cols = starts_with(\"correct_\"),\n    names_to = \"predictor\",\n    names_prefix = \"correct_\",\n    values_to = \"correct\"\n  ) %>%\n  # Group and Summarize\n  group_by(predictor) %>%\n  summarise(\n    games = sum(!is.na(correct)),\n    games_correct = sum(correct, na.rm = TRUE),\n    percent_correct = round(mean(correct, na.rm = TRUE), 3)\n  )\n\n### Merge Over Brier Scores\nAccuracy_Dataset <- Accuracy_Dataset %>%\n  left_join(data.frame(\n    predictor = c(\"espn\", \"fivethirtyeight\", \"numberfire\"),\n    brier_score = c(\n      DescTools::BrierScore(NFL_Games %>% filter(!is.na(espn_home_wp)) %>% pull(home_win), NFL_Games %>% filter(!is.na(espn_home_wp)) %>% pull(espn_home_wp)),\n      DescTools::BrierScore(NFL_Games$home_win, NFL_Games$fivethirtyeight_home_wp),\n      DescTools::BrierScore(NFL_Games %>% filter(!is.na(numberfire)) %>% pull(home_win), NFL_Games %>% filter(!is.na(espn_home_wp)) %>% pull(numberfire))\n    )\n  ),\n  by = \"predictor\"\n  ) %>%\n  mutate(brier_score = round(brier_score, 3))\n\n\n\n\nIt’s unsurprising that Numberfire is significantly more accurate than the ESPN or 538 predictions. As an oddsmaker, they can see where bets are placed and update their lines accordingly. Essentially, Numberfire benefits from more information.\nI’m looking forward to the future investigations into predicting NFL games that may come from this data!\n\n\n",
    "preview": {},
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-29-faceted-and-animated-heatmaps/",
    "title": "Faceted and Animated Heatmaps",
    "description": "Combining lessons from multiple posts to create faceted or animated heatmaps.",
    "author": [
      {
        "name": "Analytics Darkweb",
        "url": "https://twitter.com/footballdaRkweb"
      }
    ],
    "date": "2020-08-29",
    "categories": [
      "Figures",
      "Animation",
      "nflfastR"
    ],
    "contents": "\nThomas Mock from Rstudio has done it again and shown us how to pull in different heatmap options into R. You can see his blogpost here!\nEarlier I had posted a Gist talking about animated plots, but decided why not add the two together and make some faceted and animated heatmaps to really let us dig down into whatever subsets we want. Maybe we want to see some season splits or changes year over year?\nFirst, let’s pull in our data.\n\n\nlibrary(tidyverse)\nlibrary(arrow)\n\nsouce_url <- \"https://raw.githubusercontent.com/ArrowheadAnalytics/next-gen-scrapy-2.0/master/pass_and_game_data.csv\"\n\npass_map_df <- \n  data.table::fread(souce_url) %>%\n  na.omit() %>%\n  select(-V1)\n\npbp <- \n  open_dataset(\"D:/nflfastR/\", format = \"feather\") %>% \n  filter(season >= 2017, play_type == \"pass\") %>% \n  collect()\n\nSince this dataset is pretty small I won’t convert the NGS scrappy data (shouts to OG scrappy legend Sarah Mallepalle) to feather, but you can learn how here.\nNext, let’s figure out a good way to merge our data, we’ll need to do some aggrigation of the pbp to get something useful here. Luckly, we’ve already trimmed it down to a small size.\n\n\nepa_yac <- \n  pbp %>%\n  group_by(old_game_id, passer_player_name, posteam) %>%\n  summarise(\n    mean_epa = mean(epa, na.rm = TRUE),\n    mean_YAC = mean(yards_after_catch, na.rm = TRUE)\n  )\n\nLet’s say we didn’t want to just look at one player, but we wanted to look at the passing patters of every QB since 2017 who meets some threshold, then we could learn something about the underlying nature of EPA, or YAC.\nNow, if you’ll excuse me here I’m going to lift a little bit of code from Thomas’ blog post I linked above since he already gave us the structure to plot on the field as a function. We all stand on the shoulder of giants.\n\n\n#### Code blog from Thomas Mock\nback_col <- \"white\"\nfront_col <- \"black\"\n\nnot_div_5 <- function(x) {\n  # select only elements of the vector not divisible by 5\n  x[x %% 5 != 0]\n}\n\ncenter_df <- tibble(\n  x_coord = c(rep(-3.1, 60), rep(3.1, 60)),\n  y_coord = seq(-14, 59, 1) %>% rep(2) %>% not_div_5(),\n  text = \"--\"\n)\n\n# line labels\nannotate_df <- tibble(\n  x_coord = c(12.88, -12.88) %>% rep(each = 5),\n  y_coord = seq(10, 50, 10) %>% rep(2),\n  text = seq(10, 50, 10) %>% rep(2) %>% str_replace(\"(.)(.)\", \"\\\\1 \\\\2\"),\n  rotation = c(90, 270) %>% rep(each = 5)\n)\n\n# yardlines\nyardline_df <- tibble(\n  y = seq(-15, 60, 5),\n  yend = seq(-15, 60, 5),\n  x = rep(-56 / 2, 16),\n  xend = rep(56 / 2, 16)\n)\n\n# sidelines\nsideline_df <- tibble(\n  y = c(-15.15, -15.15),\n  yend = c(60.15, 60.15),\n  x = c(-56 / 2, 56 / 2),\n  xend = c(-56 / 2, 56 / 2)\n)\n\nadd_field <- function() {\n  list(\n    coord_cartesian(\n      xlim = c(-53.333 / 2, 53.333 / 2),\n      ylim = c(-15, 60)\n    ),\n    geom_text(\n      data = annotate_df, aes(label = text, angle = rotation),\n      color = front_col, size = 8\n    ),\n    geom_segment(\n      data = yardline_df, color = front_col, size = 1,\n      aes(x = x, y = y, xend = xend, yend = yend)\n    ),\n    geom_segment(\n      x = -56 / 2, y = 0, xend = 56 / 2, yend = 0,\n      color = \"blue\", size = 1, alpha = 0.5\n    ),\n    geom_segment(\n      data = sideline_df, color = front_col, size = 2,\n      aes(x = x, y = y, xend = xend, yend = yend)\n    ),\n    geom_text(\n      data = center_df,\n      aes(label = text), color = front_col, vjust = 0.32\n    ),\n    theme_void(),\n    theme(\n      strip.text = element_text(size = 20, color = front_col),\n      plot.background = element_rect(fill = back_col, color = NA),\n      legend.position = \"none\",\n      plot.margin = unit(c(2, 1, 0.5, 1), unit = \"cm\"),\n      plot.caption = element_text(color = front_col),\n      plot.title = element_text(color = front_col),\n      plot.subtitle = element_text(color = front_col),\n      panel.background = element_rect(fill = back_col, color = NA),\n      panel.border = element_blank()\n    )\n  )\n}\n\nNext we need to do a little rangling here to get our player names to match. We don’t have id numbers here, but since we are only dealing with QBs we should be able to join on name and team, unless someone knows of two QBs on the same team with the same names!\n\n\npass_map_df %>%\n  separate(name, into = c(\"first\", \"last\"), \"\\\\s\") %>%\n  mutate(\n    passer_player_name = paste0(str_extract(first, \"\\\\w\"), \".\", last)\n  ) %>%\n  inner_join(epa_yac, by = c(\"passer_player_name\", \"team\" = \"posteam\", \"game_id\" = \"old_game_id\"))\n\n          game_id  first   last  pass_type team week x_coord y_coord\n    1: 2017091004 Carson Palmer   COMPLETE  ARI    1   -23.5    14.6\n    2: 2017091004 Carson Palmer   COMPLETE  ARI    1     2.8     9.3\n    3: 2017091004 Carson Palmer   COMPLETE  ARI    1    18.6    -1.1\n    4: 2017091004 Carson Palmer   COMPLETE  ARI    1    -8.4     8.3\n    5: 2017091004 Carson Palmer   COMPLETE  ARI    1   -15.5     8.0\n   ---                                                              \n37817: 2019122904  Casey Keenum INCOMPLETE  WAS   17    14.6     2.0\n37818: 2019122904  Casey Keenum INCOMPLETE  WAS   17    24.5    14.8\n37819: 2019122904  Casey Keenum INCOMPLETE  WAS   17    11.7    -7.6\n37820: 2019122904  Casey Keenum INCOMPLETE  WAS   17    10.9    14.1\n37821: 2019122904  Casey Keenum INCOMPLETE  WAS   17    -9.8    12.3\n       type home_team away_team season\n    1:  reg       DET       ARI   2017\n    2:  reg       DET       ARI   2017\n    3:  reg       DET       ARI   2017\n    4:  reg       DET       ARI   2017\n    5:  reg       DET       ARI   2017\n   ---                                \n37817:  reg       DAL       WAS   2019\n37818:  reg       DAL       WAS   2019\n37819:  reg       DAL       WAS   2019\n37820:  reg       DAL       WAS   2019\n37821:  reg       DAL       WAS   2019\n                                                                       game_url\n    1: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    2: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    3: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    4: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    5: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n   ---                                                                         \n37817: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n37818: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n37819: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n37820: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n37821: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n       home_score away_score passer_player_name   mean_epa mean_YAC\n    1:         35         23           C.Palmer -0.2199358 4.407407\n    2:         35         23           C.Palmer -0.2199358 4.407407\n    3:         35         23           C.Palmer -0.2199358 4.407407\n    4:         35         23           C.Palmer -0.2199358 4.407407\n    5:         35         23           C.Palmer -0.2199358 4.407407\n   ---                                                             \n37817:         47         16           C.Keenum -0.6127941 5.833333\n37818:         47         16           C.Keenum -0.6127941 5.833333\n37819:         47         16           C.Keenum -0.6127941 5.833333\n37820:         47         16           C.Keenum -0.6127941 5.833333\n37821:         47         16           C.Keenum -0.6127941 5.833333\n\nLooks like we dropped a few entries, probably a team name mismatch, so let’s fix that.\n\n\nfastR_teams <- epa_yac$posteam %>% unique()\n\nscrappy_teams <- pass_map_df$team %>% unique()\n\nsetdiff(fastR_teams, scrappy_teams)\n\n[1] \"LV\"\n\nAs suspected, the scrappy team names reflect OAK while nflfastR lists LV.\n\n\npass_map_df %>%\n  separate(name, into = c(\"first\", \"last\"), \"\\\\s\") %>%\n  mutate(\n    passer_player_name = paste0(str_extract(first, \"\\\\w\"), \".\", last),\n    team = ifelse(team == \"OAK\", \"LV\", team)\n  ) %>%\n  inner_join(epa_yac, by = c(\"passer_player_name\", \"team\" = \"posteam\", \"game_id\" = \"old_game_id\"))\n\n          game_id  first   last  pass_type team week x_coord y_coord\n    1: 2017091004 Carson Palmer   COMPLETE  ARI    1   -23.5    14.6\n    2: 2017091004 Carson Palmer   COMPLETE  ARI    1     2.8     9.3\n    3: 2017091004 Carson Palmer   COMPLETE  ARI    1    18.6    -1.1\n    4: 2017091004 Carson Palmer   COMPLETE  ARI    1    -8.4     8.3\n    5: 2017091004 Carson Palmer   COMPLETE  ARI    1   -15.5     8.0\n   ---                                                              \n39280: 2019122904  Casey Keenum INCOMPLETE  WAS   17    14.6     2.0\n39281: 2019122904  Casey Keenum INCOMPLETE  WAS   17    24.5    14.8\n39282: 2019122904  Casey Keenum INCOMPLETE  WAS   17    11.7    -7.6\n39283: 2019122904  Casey Keenum INCOMPLETE  WAS   17    10.9    14.1\n39284: 2019122904  Casey Keenum INCOMPLETE  WAS   17    -9.8    12.3\n       type home_team away_team season\n    1:  reg       DET       ARI   2017\n    2:  reg       DET       ARI   2017\n    3:  reg       DET       ARI   2017\n    4:  reg       DET       ARI   2017\n    5:  reg       DET       ARI   2017\n   ---                                \n39280:  reg       DAL       WAS   2019\n39281:  reg       DAL       WAS   2019\n39282:  reg       DAL       WAS   2019\n39283:  reg       DAL       WAS   2019\n39284:  reg       DAL       WAS   2019\n                                                                       game_url\n    1: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    2: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    3: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    4: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n    5: http://www.nfl.com/liveupdate/game-center/2017091004/2017091004_gtd.json\n   ---                                                                         \n39280: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n39281: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n39282: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n39283: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n39284: http://www.nfl.com/liveupdate/game-center/2019122904/2019122904_gtd.json\n       home_score away_score passer_player_name   mean_epa mean_YAC\n    1:         35         23           C.Palmer -0.2199358 4.407407\n    2:         35         23           C.Palmer -0.2199358 4.407407\n    3:         35         23           C.Palmer -0.2199358 4.407407\n    4:         35         23           C.Palmer -0.2199358 4.407407\n    5:         35         23           C.Palmer -0.2199358 4.407407\n   ---                                                             \n39280:         47         16           C.Keenum -0.6127941 5.833333\n39281:         47         16           C.Keenum -0.6127941 5.833333\n39282:         47         16           C.Keenum -0.6127941 5.833333\n39283:         47         16           C.Keenum -0.6127941 5.833333\n39284:         47         16           C.Keenum -0.6127941 5.833333\n\nOkay we’re closer, but still missing some rows. Probably a QB name mismatch. Let’s find them.\n\n\npass_map_df <- \n  pass_map_df %>%\n  separate(name, into = c(\"first\", \"last\"), \"\\\\s\") %>%\n  mutate(\n    passer_player_name = paste0(str_extract(first, \"\\\\w\"), \".\", last),\n    team = ifelse(team == \"OAK\", \"LV\", team)\n  )\n\nfastR_qbs <- epa_yac$passer_player_name %>% unique()\n\nscrappy_qbs <- pass_map_df$passer_player_name %>% unique()\n\nsetdiff(fastR_qbs, scrappy_qbs)\n\n  [1] \"M.Stafford\"    \"B.Bortles\"     \"D.Prescott\"    \"R.Quigley\"    \n  [5] \"J.Landry\"      \"J.Hekker\"      \"C.Henne\"       \"R.Mallett\"    \n  [9] \"M.Haack\"       \"M.Cassel\"      \"L.Edwards\"     \"M.Gray\"       \n [13] \"T.McEvoy\"      \"P.O'Donnell\"   \"T.Cohen\"       \"C.Beathard\"   \n [17] \"M.Lee\"         \"R.Golden\"      \"C.Rush\"        \"G.Smith\"      \n [21] \"S.Koch\"        \"E.Decker\"      \"S.Vereen\"      \"T.Kelce\"      \n [25] \"K.Clemens\"     \"J.Ryan\"        \"C.Kupp\"        \"J.Rudock\"     \n [29] \"J.Webb\"        \"B.Nortman\"     \"M.Palardy\"     \"A.McCarron\"   \n [33] \"L.Fitzgerald\"  \"L.Jones\"       \"G.Tate\"        \"J.Callahan\"   \n [37] \"R.Cobb\"        \"T.Bray\"        \"W.Snead\"       \"T.Burton\"     \n [41] \"N.Agholor\"     \"D.Henry\"       \"K.Byard\"       \"A.Wilson\"     \n [45] \"J.Scott\"       \"C.Bojorquez\"   \"D.Hopkins\"     \"E.Sanders\"    \n [49] \"N.Mullens\"     \"J.Dobbs\"       \"B.Anger\"       \"C.Beasley\"    \n [53] \"D.Hilliard\"    \"L.Cooke\"       \"L.Thomas\"      \"D.Jennings\"   \n [57] \"T.Boyd\"        \"E.Ebron\"       \"C.McCoy\"       \"C.Wadman\"     \n [61] \"A.Miller\"      \"C.Daniel\"      \"A.Brown\"       \"C.Boswell\"    \n [65] \"R.Griffin III\" \"B.Ellington\"   \"M.Sanchez\"     \"Z.Jones\"      \n [69] \"J.Johnson\"     \"K.Lauletta\"    \"S.Martin\"      \"C.McCaffrey\"  \n [73] \"D.Westbrook\"   \"M.Darr\"        \"K.Stills\"      \"M.Prater\"     \n [77] \"G.Gilbert\"     \"T.Way\"         \"D.Pettis\"      \"J.Stidham\"    \n [81] \"P.Williams\"    \"J.Samuels\"     \"A.Kamara\"      \"J.Elliott\"    \n [85] \"G.Minshew II\"  \"Z.Pascal\"      \"B.Kern\"        \"M.Wishnowsky\" \n [89] \"Jos.Allen\"     \"R.Dixon\"       \"A.Lee\"         \"D.Colquitt\"   \n [93] \"K.Barner\"      \"C.Sutton\"      \"B.Powell\"      \"S.Sims\"       \n [97] \"T.Boyle\"       \"J.Brown\"       \"A.Erickson\"    \"J.White\"      \n[101] \"J.Gordon\"      \"A.Tanney\"      \"A.Beck\"        \"K.Hunt\"       \n[105] \"K.Harmon\"      \"S.Diggs\"       \"S.Watkins\"    \n\nIt appears as though a lot of names are non-QBs such as Sam Koch or Dustin Colquitt. Punters doing trick plays etc. But some of these are QBs we need to fix like RG3, Dak Prescott, and Matt Stafford.\n\n\nepa_yac <- \n  epa_yac %>%\n  mutate(\n    passer_player_name = ifelse(passer_player_name == \"Jos.Allen\", \"J.Allen\", passer_player_name),\n    passer_player_name = ifelse(passer_player_name == \"G.Minshew II\", \"G.Minshew\", passer_player_name),\n    passer_player_name = ifelse(passer_player_name == \"R.Griffin III\", \"R.Griffin\", passer_player_name)\n  )\n\npass_map_df <- \n  pass_map_df %>%\n  mutate(\n    passer_player_name = ifelse(passer_player_name == \"R.Prescott\", \"D.Prescott\", passer_player_name),\n    passer_player_name = ifelse(passer_player_name == \"J.Stafford\", \"M.Stafford\", passer_player_name),\n    passer_player_name = ifelse(passer_player_name == \"R.Bortles\", \"B.Bortles\", passer_player_name)\n  )\n\nYou get the point, you can use this code and repair more names to match but I am going to proclaim victory and move on.\n\n\npass_map_df <- \n  pass_map_df %>%\n  inner_join(epa_yac, by = c(\"passer_player_name\", \"team\" = \"posteam\", \"game_id\" = \"old_game_id\"))\n\nFor our first split, let’s leverage these two datasets and see if we can see some structural differences between EPA and YAC performance.\n\n\npass_map_df %>%\n  mutate(\n    epa_below_zero = ifelse(mean_epa <= 0, \"Negative EPA\", \"Postive EPA\"),\n    epa_below_zero = as.factor(epa_below_zero)\n  ) %>%\n  ggplot(aes(x = x_coord, y = y_coord)) +\n  geom_density_2d_filled(\n    aes(fill = ..level.., color = ..level..),\n    contour_var = \"ndensity\", # normalize across facets\n    breaks = seq(0.1, 1.0, length.out = 10)\n  ) +\n  facet_wrap(~epa_below_zero) + \n  add_field()\n\n\nLet’s try the same thing but for YAC!\n\n\npass_map_df %>%\n  mutate(\n    yac_below_zero = ifelse(mean_YAC <= 0, \"Negative YAC\", \"Postive YAC\"),\n    yac_below_zero = as.factor(yac_below_zero)\n  ) %>%\n  ggplot(aes(x = x_coord, y = y_coord)) +\n  geom_density_2d_filled(\n    aes(fill = ..level.., color = ..level..),\n    contour_var = \"ndensity\", # normalize across facets\n    breaks = seq(0.1, 1.0, length.out = 10)\n  ) +\n  facet_wrap(~yac_below_zero) + \n  add_field()\n\n\nThe finding structure and drawing any conclusions is left as an exersize for the reader.\nBut let’s now take a look at QB performance over time.\n\n\npass_map_df %>%\n  filter(passer_player_name == \"D.Prescott\") %>%\n  ggplot(aes(x = x_coord, y = y_coord)) +\n  geom_density_2d_filled(\n    aes(fill = ..level.., color = ..level..),\n    contour_var = \"ndensity\", # normalize across facets\n    breaks = seq(0.1, 1.0, length.out = 10)\n  ) +\n  facet_wrap(~season) + \n  add_field() + \n  labs(title = \"Dak Prescott Targets by year, 2017-2019\")\n\n\nIt apears that as the years have gone on, DAK as less likely to throw to his left. Interesting, maybe something is there. Maybe the departure of personnel, or it could simply be new play designs. But what if we wanted to take this and make an animation instead of looking at these plots side by side.\n\n\nlibrary(gganimate)\n\np <- \n  pass_map_df %>%\n  filter(passer_player_name == \"D.Prescott\") %>%\n  ggplot(aes(x = x_coord, y = y_coord)) +\n  geom_density_2d_filled(\n    aes(fill = ..level.., color = ..level..),\n    contour_var = \"ndensity\", # normalize across facets\n    breaks = seq(0.1, 1.0, length.out = 10)\n  ) + \n  transition_states(season, transition_length = 2, state_length = 1) +\n  labs(\n    x = \"X Coordinate\",\n    y = \"Y coordinate relative to LOS\",\n    caption = \"Data: @nflfastR & next-gen-scrappy\",\n    title = \"{closest_state} Dak Prescott Targets\"\n  ) + \n  enter_fade()+\n  exit_fade() + \n  add_field()\n\nNow we’re ready to create our heatmap that changes over time. Imagine all the exploring we could do with this!\n\n\nanimate(p, width = 400, height = 600)\n\n\nWe could look at Patrick Mahomes over time, or Russell Wilson week to week. If we could get play_ids somehow we could improve our EPA plots above and look at negative EPA vs positive by down, distance. Perhaps if we assumed the order of the scrappy plays is in game order, we could match it to the nflfastR set and see what we find. Try it out and see!\nOne last thing, let’s combine what we’ve done and add in some of the work of Josh Hermsmeyer to do direct QB comparisons.\n\n\nqb_density_compare <- function(pass_df, qb1_name, qb2_name, n = 100){\n  \n  # filter to qb1\n  qb1 <- pass_df %>% \n    select(x_coord, y_coord, name) %>% \n    filter(str_detect(name, qb1_name))\n  \n  #filter to qb2\n  qb2 <- pass_df %>% \n    select(x_coord, y_coord, name) %>% \n    filter(str_detect(name, qb2_name))\n  \n  # get x/y coords as vectors\n  qb1_x <- pull(qb1, x_coord)\n  qb1_y <- pull(qb1, y_coord)\n  \n  # get x/y coords as vectors\n  qb2_x <- pull(qb2, x_coord)\n  qb2_y <- pull(qb2, y_coord)\n\n  # get x and y range to compute comparisons across\n  x_rng = range(c(qb1_x, qb2_x))\n  y_rng = range(c(qb1_y, qb2_y))\n  \n  # Calculate the 2d density estimate over the common range\n  d2_qb1 = MASS::kde2d(qb1_x, qb1_y, lims=c(x_rng, y_rng), n=n)\n  d2_qb2 = MASS::kde2d(qb2_x, qb2_y, lims=c(x_rng, y_rng), n=n)\n  \n  # create diff df\n  qb_diff <- d2_qb1\n  \n  # matrix subtraction density from qb2 from qb1\n  qb_diff$z <- d2_qb1$z - d2_qb2$z\n  \n  # add matrix col names\n  colnames(qb_diff$z) = qb_diff$y\n  \n  #### return tidy tibble ####\n  qb_diff$z %>% \n    # each col_name is actually the y_coord from the matrix\n    as_tibble() %>% \n    # add back the x_coord\n    mutate(x_coord= qb_diff$x) %>% \n    pivot_longer(-x_coord, names_to = \"y_coord\", values_to = \"z\") %>% \n    mutate(y_coord = as.double(y_coord))\n\n}\n\npass_map_df <- pass_map_df %>% rename(name = passer_player_name)\n\ncompared_z <- data.frame()\n\nfor (i in seq(2017, 2019, 1)) {\n  compared_z <- rbind(qb_density_compare(pass_map_df[pass_map_df$season == i], \"P.Mahomes\", \"A.Rodgers\", n = 200) %>% mutate(season = i), compared_z) \n}\n\nlibrary(scales)\np <-\n  compared_z %>%\n  # mutate(season = as.factor(season)) %>%\n  ggplot(aes(x_coord, y_coord)) +\n  # geom_tile(aes(x_coord, y_coord, fill=z))  +\n  stat_contour(geom = \"polygon\", \n                 aes(colour=..level.., z = z, fill = ..level..), \n               breaks = seq(min(compared_z$z), max(compared_z$z), length.out = 10)\n               ) +\n  scale_fill_gradient2(low=\"blue\",mid=\"white\", high=\"red\", midpoint=0) +\n  scale_colour_gradient2(low=muted(\"blue\"), mid=\"white\", high=muted(\"red\"), midpoint=0) +\n  add_field() +\n  theme(legend.position = \"bottom\", legend.key.width = unit(2, \"cm\"),\n        plot.title = element_text(size = 20, hjust = 0.5, face = \"bold\"),\n        plot.subtitle = element_text(size = 12, hjust = 0.5),\n        plot.caption = element_text(face = \"bold\")) +\n  labs(title = \"{current_frame}, Mahomes (QB1) vs Rodgers (QB2)\",\n       subtitle = \"Red = More by QB1, Blue = More by QB2\",\n       caption = \"\\n\\nPlot: @thomas_mock | Data: @ChiefsAnalytics\") +\n  guides(colour=FALSE) + \n  transition_manual(factor(season, levels = c('2017', '2018', '2019'))) +\n  enter_fade()+\n  exit_fade()\n\nanimate(p, width = 400, height = 600)\n\n\n\n\n",
    "preview": "posts/2020-08-29-faceted-and-animated-heatmaps/faceted-and-animated-heatmaps_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-08-29-player-density-and-completion-surface-estimates/",
    "title": "Player Density and Completion Surface Estimates",
    "description": "Methods for modeling density estimates and expected completion percentages across the football field for individual players.",
    "author": [
      {
        "name": "Ethan Douglas",
        "url": "https://twitter.com/ChiefsAnalytics"
      }
    ],
    "date": "2020-08-29",
    "categories": [
      "nflfastR",
      "python"
    ],
    "contents": "\nTable of Contents\nDensity Estimates and Expected Completion Surfaces\nDensity Estimates\nExpected Completion Surfaces\n\nDensity Estimates and Expected Completion Surfaces\nIn this post I will cover\nUsing the scipy library to create your own kernel density estimator\nUsing this estimator to easily compare the densities of two players\nModeling the expected completion % of a pass, and plotting these probabilities as a surface over the field\nModeling the expected completion % of a pass for a particular player or team, and comparing that to the rest of the league\nIn my last post I gave some examples of how you can use the seaborn library in python to plot heat maps of NFL passing locations. For this post I’m going to pick right back up where we left off - performing kernal density estimates (KDEs) with the scipy library rather than relying on the seaborn library. The advantage here is that we can get an estimate of the density of the passes, and then “slice and dice” that estimate however we want, performing calculations on the output. I’ll show you why that can be useful.\nAs a reminder, we wanted to compare the densities of Patrick Mahomes and Derek Carr. Our graph worked fine - but what if we wanted to overlay those densities, so that the true differences were more apparent? That’s where the more manual (but still very much not manual) KDE comes in. One important note before we begin: while I’ve since modified the code a bit, as I mentioned in the previous post this Next Gen Stats scraper was first created by Sarah Mallepalle et al. (2019). I cannot recommend reading this paper enough!\nDensity Estimates\n\n\n#imports\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom pygam import LogisticGAM, s, f, te\n#I'm surpressing warnings here because the PyGAM library warns you that the p-values are smaller than likely, which I am not concerned with.\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#load passing location data\ndf = pd.read_csv('https://raw.githubusercontent.com/ArrowheadAnalytics/next-gen-scrapy-2.0/master/pass_and_game_data.csv', low_memory=False)\n#There's an additional index row we don't need, so I am getting rid of it here\ndf = df.iloc[0:,1:]\ndf.dropna(inplace=True)\n\nWhat we’re going to do here is create a helper function, which will allow is to perform the kernal density. The key to making these KDEs comparable between players is the grid size. By keeping these constant between players we are comparing apples to apples.\n\n\n#Function that will help us get our data in the right shape every time we want to do this estimate\ndef kde_helper(df,name):\n    '''Function to get data in the correct form for the KDE function\n    inputs: dataframe, player name\n    output: KDE applied to mesh grid, ready for plotting'''\n    #Creating a mesh grid dividing each yard in half (so 4 units in a square yard),\n    #between the boundaries of the x and y coordinates (the min and max of our data) supplied.\n    m1 = df['x_coord'].loc[(df['name'].str.contains(name))]\n    m2 = df['y_coord'].loc[(df['name'].str.contains(name))]\n    #By using the same size grid each time we perform these estimates, we can make direct apples to apples comparisons between players. \n    #What we're doing with this line is creating a \"mesh grid\" (think matrix) which we'll eventually evaluate the KDE on\n    X, Y = np.mgrid[-30:30:121j, -10:60:141j]\n    #flatten and stack these grids, giving a 2xn array of positions where n = 121*141 (the # of steps for each direction)\n    #Basically what we are getting here is a \"coordinate\" for every single step we've created.\n    #We start the x min at -30, so there will be 141 -30s - because -30 will be paired with every step we've created in the y direction.\n    positions = np.vstack([X.ravel(), Y.ravel()])\n    #Stack the values we care about in a 2xm array (basically transposing them here), where m is just the length of our supplied data\n    values = np.vstack([m1, m2])\n    #Perform the kernel estimation on the values we care about - you can think of this as \"training\" the kernel estimator\n    kernel = stats.gaussian_kde(values)\n    #Generate probabilities at the positions specified, transpose them, and put them back into the grid shape for plotting\n    Z = np.reshape(kernel(positions).T, X.shape)\n    return Z\n\nNow that we’ve got the helper function, we can try it out!\n\n\n#We'll start with Mahomes\nname='Mahomes'\nmahomes_kde = kde_helper(df,name)\n#That was easy! Now Carr\nname='Carr'\ncarr_kde = kde_helper(df,name)\n\nSo we’ve got our two estimates. There are different ways you can play around with these estimates to find insights but what I’m going to focus on is the difference between the two estimates. The nice thing is this is really easy to do, you just subtract them.\n\n\n#again - simple!\ndiff_kde = mahomes_kde - carr_kde\n\nThe plotting here is a bit different than what we did in the previous post. We’ll be using matplotlib’s imshow() function, which is what you use to display image files like JPEGs. imshow accepts a matrix as an input, either MxN (what we have with our kde - a colormap), MxNx3 (how a traditional “picture” is stored - RGB values for each pixel location), or MxNx4 (adding an additional layer of our matrix to control the transparency level)\n\n\n#Set our style\nplt.style.use('seaborn-talk')\n\nfig, ax1 =plt.subplots(1,1)\n\n#This line is where the magic happens. Because of the way we performed the KDE, we have to rotate our data 270 degrees to plot in the orientation we want (np.rot90)\n#Next, we want to make sure a pixel in the left direction is the same coordinate distance as a pixel in the vertical direction, so we set aspect to equal\n#The extent is setting the coordinate system of the displayed image (along with the \"origin\" parameter). This is necessary to make sure that what we are indicating is the 20 yard line shows up as the 20 yardline in the pic\n#Next, we want to normalize our colormap so that 0 is in the exact middle of the colormap. We can do this by having vmin and vmax have the same absolute value\n#Lastly, we set the colormap parameter. I like \"diverging\" colormaps that have white in the middle for comparison plots, so it is clear which values are positive, negative, and 0.\nplt.imshow(np.fliplr(np.rot90(diff_kde,3)),\n           origin='lower', aspect='equal',\n           extent=[-30, 30, -10, 60],\n           norm = mpl.colors.Normalize(vmin=-0.0005, vmax=0.0005),\n           cmap='RdBu_r')\n#Add a \"colorbar\", a scale so people know what color represents what\ncbar = plt.colorbar()\ncbar.set_label(\"\\nMahomes (Red) - Carr (Blue) passing densities\")\n#We don't really care about the values here, only the relative differences. \n#The values will change depending on how small we slice up our field. So, I only want to show the viewer what 0 is.\ncbar.set_ticks([0])\n#Set title, remove ticks and labels\nax1.set_title('Mahomes vs Carr - NFL Passing Densities')\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-10,60)\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-10,60,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='k',ls='-',alpha=0.5, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='k',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nax1.annotate('Line of Scrimmage', (16,0),\n             textcoords=\"offset points\", # how to position the text\n                 xytext=(0,5), # distance from text to points (x,y)\n                 ha='center',fontsize = 9) # horizontal alignment can be left, right or center\n\n\n\nThis plot lets us see the differences in the densities between the two players, but there’s a lot of color there. Depending on the device you’re viewing this chart on, it may be hard to know what areas of the field to focus on. In order to help better direct the viewer to the most prominent differences, we can “mask” the image so that we only show the extreme differences.\n\n\n#Here's our mask. It may seem weird to use \"masked_inside\" here when we want the values on the extremes (outside these numbers), but keep in mind this is the \"masked\" array - so the mask_inside will hide all values inside these boundaries\n#You can manually set these numbers, but for simplicity and consistency I'm going to go with the top and bottom quartiles of our differences. Show I'm showing the top 25% units where Mahomes has higher density than Carr, and the top 25% where Carr has higher density than Mahomse\ndiff_masked = np.ma.masked_inside(diff_kde, np.percentile(diff_kde, 25), np.percentile(diff_kde, 75))\n\nplt.style.use('seaborn-talk')\n\nfig, ax1 =plt.subplots(1,1)\n\n\nplt.imshow(np.fliplr(np.rot90(diff_masked,3)),\n           origin='lower', aspect='equal',\n           extent=[-30, 30, -10, 60],\n           norm = mpl.colors.Normalize(vmin=-0.0005, vmax=0.0005),\n           cmap='RdBu_r')\n\n#Set title, remove ticks and labels\nax1.set_title('Mahomes (red) vs Carr (blue) - NFL Passing Densities')\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-10,60)\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-10,60,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='k',ls='-',alpha=0.5, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='k',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nax1.annotate('Line of Scrimmage', (16,0),\n             textcoords=\"offset points\", # how to position the text\n                 xytext=(0,5), # distance from text to points (x,y)\n                 ha='center',fontsize = 9) # horizontal alignment can be left, right or center\n\n\nSo now a viewer can pretty easily see the most relevant differences between two players. In this case, Carr is far more likely to target players around the line of scrimmage, while Mahomes is more likely to do “deep” screens (<-5 yards) or passes past the 10 yardline. If you aren’t a fan of the red and blue, you can play around with all of the available matplotlib colormaps. Again, I recommend a diverging map for this kind of plot but you can certainly get creative.\nExpected Completion Surfaces\nWhile densities can help tell us tendencies, they don’t tell us how well a player performed when targeting a certain area of the field. Ideally, we’d like to match this pass location data to play by play data and look at the expected points added of each throw, but due to the inconsistincies with the way different stadiums record air yards that’s quite difficult to do (though I highly encourage any ambitious reader to try. You’d add a lot to this field if you can pull it off). Since we don’t have expected points, we’ll try the next best thing: expected completion percentage.\nIn our dataset, we have the x and y coordinate of the pass, the player who threw the ball, the team they threw it against, some final game information (final score, game location) and whether or not the pass was completed. For now we’ll just estimate probabilities for the whole league, focusing on just the x and y coordinate of the pass. Now I’m not a statistician, so I can’t say for certain what model is best for this task. Thankfully, the amazing creators of the original NGS scraper are trained statisticians, and they’ve laid out in their paper why generalized additive models would be a good choice for this task. To quickly summarise, they allow us to both capture potential nonlinearities while also giving us a very smooth output, which is both nice for plotting and likely matches the reality of throwing a football (it is unlikely that there are very jagged differences or harsh cutoffs in difficulty as pass locations move throughout the field, but rather we’d expect the change in the “true” completion percentage to be smooth).\n\n\n#We have to do a bit of cleaning to get the data in a form we can use for the model. First, we need to convert out pass_type column into a binary variable instead of the categorical complete, incomplete, touchdown, and interception. \ndf['is_complete'] = 0\ndf.loc[((df['pass_type']=='COMPLETE') | (df['pass_type']=='TOUCHDOWN')), 'is_complete'] = 1\n\n#Now let's see the distribution of our outcome\nprint(df.is_complete.mean())\n\n0.6532767626998791\n\nOur classes are a bit unbalanced. We have more complete passes than incomplete, though not too drastically so. This class imbalance would be more important if we had imbalanced penalties for assigning incorrect classes. In other words, if we cared more about false negatives than false positives. In this case, it is no worse to predict an incomplete pass complete, than it is to predict a complete pass incomplete (unlike many systems we may try to model in the medical field). So the main reason we care about class imbalance here is when it comes to assessing the performance of our model; because 65% of our passes are complete, just predicting every single pass will be complete will already get us to 65% accuracy. This post isn’t meant to be a deep dive in classification, so we’re not going to address the class imbalance further.\nLet’s use a similar model structure to the one introduced by Mallepalle et al. (2019) Sticking with python, we’ll take advantage of the PyGAM library here.\n\n\n#Get the features and outcomes we care about\nX = df[['x_coord','y_coord']]\ny = df[['is_complete']]\n#Fit our model\ngam = LogisticGAM().fit(X, y)\n#Test the accuracy of our model\ngam.summary()\n\nLogisticGAM                                                                                               \n=============================================== ==========================================================\nDistribution:                      BinomialDist Effective DoF:                                     30.9458\nLink Function:                        LogitLink Log Likelihood:                                -25666.4423\nNumber of Samples:                        43839 AIC:                                            51394.7762\n                                                AICc:                                           51394.8243\n                                                UBRE:                                               3.1729\n                                                Scale:                                                 1.0\n                                                Pseudo R-Squared:                                   0.0928\n==========================================================================================================\nFeature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n================================= ==================== ============ ============ ============ ============\ns(0)                              [0.6]                20           17.1         0.00e+00     ***         \ns(1)                              [0.6]                20           13.8         0.00e+00     ***         \nintercept                                              1            0.0          7.85e-05     ***         \n==========================================================================================================\nSignificance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nWARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n         which can cause p-values to appear significant when they are not.\n\nWARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n         are typically lower than they should be, meaning that the tests reject the null too readily.\n\ngam.accuracy(X,y)\n\n0.7013389903966788\n\nSo with a quick simple model we’ve improved the accuracy of just assuming every pass will be complete, but we’re still incorrectly classifying 30% of passes. This isn’t too surprising though - we’ve got many different quarterbacks throwing the ball to many different wide receivers against many different defenses. Just including the location of the pass shouldn’t get us too accurate of a model, or we’d start to think that players don’t matter!\nOne very useful aspect of GAMs is that because they are an additive model, we can explore how each feature is influencing the model output by holding the other features constant at their average value. Let’s plot what that looks like.\n\n\n##I'll confess I just copy and pasted this straight from the pygam documentation, you could definitely clean these up further and add relevant titles. \nfor i, term in enumerate(gam.terms):\n    if term.isintercept:\n        continue\n        \n\n    XX = gam.generate_X_grid(term=i)\n    pdep, confi = gam.partial_dependence(term=i, X=XX, width=0.95)\n\n    plt.figure()\n    plt.plot(XX[:, term.feature], pdep)\n    plt.plot(XX[:, term.feature], confi, c='r', ls='--')\n    plt.title(repr(term))\n    if i == 0:\n      plt.show()\n\n\nI find these plots to be super cool (yes I’m a nerd but hey, you’re the one reading an open-source football post!) because they can let us easily see where the decreased probability in throwing a pass in certain areas of the field come from. One thing that immediately jumps out from these plots is that the only real influence of the x coordinate is passes close to either sideline. Otherwise, it is the depth of the pass (y coordinate) that is the driver behind the difficulty in completing it. This is exactly why a linear model would not do well here - the difference in completion probability from x coordinate 28 to x coordinate 25 is far different than the difference in cp from x coordinate 18 to x coordinate 15.\nOkay so we’ve fit our model and explored the feature dependencies a bit, but how do we go about visualizing this?\nSimilar to our KDE plotting, we’ll build a helper function for this\n\n\ndef gam_helper(df):\n    x = df[['x_coord','y_coord']]\n    y = df['is_complete']\n        #Similar to our KDE helper, we want a mesh grid that we will eventually evaluate the model on\n    X, Y = np.mgrid[-30:30:121j, -10:60:141j]\n        #Once again we want to flatten and stack our coordinates\n    positions = np.vstack([X.ravel(), Y.ravel()])\n        #Instead of a kde we fit a gam. Here I'm adjusting the number of splines to avoid overfitting, since we aren't doing any sort of hold out or cross validation in this post\n    gam = LogisticGAM(s(0, n_splines=8) + s(1, n_splines=8) + te(0,1)).fit(x, y)\n        #Generate probabilities at the positions specified, transpose them, and put them back into the grid shape for plotting\n    Z = np.reshape(gam.predict_mu(positions.T).T, X.shape)\n    return Z\n    \n#Call our function\npass_gam = gam_helper(df)\n\n\n#Plot our output, same code as before\nplt.style.use('seaborn-talk')\n\nfig, ax1 =plt.subplots(1,1)\n\n#This is where the magic happens here. Because of the way we performed the KDE, we have to rotate our data 270 degrees to plot in the orientation we want (np.rot90)\n#Next, we want to make sure a pixel in the left direction is the same coordinate distance as a pixel in the vertical direction, so we set aspect to equal\n#The extent is setting the coordinate system of the displayed image (along with the \"origin\" parameter). This is necessary to make sure that what we are indicating is the 20 yard line shows up as the 20 yardline in the pic\n#Next, we want to normalize our colormap so that 0 is in the exact middle of the colormap. We can do this by having vmin and vmax have the same absolute value\n#Lastly, we set the colormap parameter. I like \"diverging\" colormaps that have white in the middle for comparison plots, so it is clear which values are positive, negative, and 0.\nplt.imshow(np.fliplr(np.rot90(pass_gam,3)),\n           origin='lower', aspect='equal',\n           extent=[-30, 30, -10, 60],\n           norm = mpl.colors.Normalize(vmin=0, vmax=1),\n           cmap='PiYG')\n#Add a \"colorbar\", a scale so people know what color represents what\ncbar = plt.colorbar()\ncbar.set_label(\"\\nEstimated Completion Probability\")\n\n#Set title, remove ticks and labels\nax1.set_title('League-wide Estimated Completion Probability')\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-10,60)\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-10,60,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='k',ls='-',alpha=0.5, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='k',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nax1.annotate('Line of Scrimmage', (16,0),\n             textcoords=\"offset points\", # how to position the text\n                 xytext=(0,5), # distance from text to points (x,y)\n                 ha='center',fontsize = 9) # horizontal alignment can be left, right or center\n\n\nThere are a few different ways we can expand on this. First, we could play around with the model more. We didn’t do any hold out or cross validation in our model, we just checked the accuracy of the model on the data it was trained on.\nAdditionally, the original expected completion surface model introduced by Mallepalle et. al (2019) used smooth tensor products (ti) for all terms, whereas the python GAM library does not have this functionality - instead I just used spline terms and a tensor product term. So, our results differ a bit (though they should be expected to differ some because I’ve included the 2019 season which was not in the original paper). In general for statistical modeling I prefer and recommend using R, however I wanted to try keeping this post all in python.\nA logical next step is to estimate completion probabilities for a given QB or against a given defense. The simple way of doing this is very straightforward. You just filter your dataframe using .loc to get the QB or team you want, and repeat the process above. However, you’re going to be left with a model that is very likely to be “overfit” (admittedly I’ve done this quite a bit on twitter, but as I said before - I’m not a statistician!). Derek Carr for instance only has a handful of passes deep, but we would not expect that small sample size to be representative of the “true” completion percentage if Carr threw to every square yard on the field thousands of times. To combat that, one approach you can use is what Mallepalle et. al did and use a 2-Dimensional Naive Bayesian approach where you leverage the sample size of the entire league but give it less weight as the QB of interest has completed a greater number of passes in a given location of the field. Source code for this from the Mallepalle et al. paper can be found here, which is what I drew from below (though the source code is in R, it’s quite straightforward to adapt to python.)\n\n\n#league-wide data\n#median number of passes \nmed_n_passes = df.groupby(by='name')['x_coord'].count().median()\n#league-wide comp. probability estimates\nleague_gam = gam_helper(df)\n#league-wide kde\nleague_kde = kde_helper(df, '')\n\n#QB data\nqb_name = 'Mahomes'\nqb_df = df.loc[(df['name'].str.contains(qb_name))]\n#Qb passes\nn_qb = len(qb_df)\n#QB comp. prob\nqb_gam = gam_helper(qb_df)\n#QB kde\nqb_kde = kde_helper(df, qb_name)\n\n#Everyone's favorite phrase - regress to the mean!\nregressed_model = (med_n_passes*league_gam*league_kde + n_qb*qb_gam*qb_kde) / (med_n_passes*league_kde + n_qb*qb_kde)\n\nPlotting this using the exact same code as before should show us how like Mahomes is to complete a pass at any part of the field, accounting for how little we know about his true ability in each area of the field.\n\n\nplt.style.use('seaborn-talk')\nfig, ax1 =plt.subplots(1,1)\n\nplt.imshow(np.fliplr(np.rot90(regressed_model,3)),\n           origin='lower', aspect='equal',\n           extent=[-30, 30, -10, 60],\n           norm = mpl.colors.Normalize(vmin=0, vmax=1),\n           cmap='PiYG')\n#Add a \"colorbar\", a scale so people know what color represents what\ncbar = plt.colorbar()\ncbar.set_label(\"\\nMahomes Estimated Completion Probability\")\n\n#Set title, remove ticks and labels\nax1.set_title('Mahomes Estimated Completion Probability')\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-10,60)\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-10,60,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='k',ls='-',alpha=0.5, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='k',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nax1.annotate('Line of Scrimmage', (16,0),\n             textcoords=\"offset points\", # how to position the text\n                 xytext=(0,5), # distance from text to points (x,y)\n                 ha='center',fontsize = 9) # horizontal alignment can be left, right or center\n\n\nThis is definitely a different shape than the league-wide model we plotted. But exactly how does it differ? Once again we can answer this by subtracting our two models.\n\n\ndiff_gam = regressed_model - league_gam\n\nplt.style.use('seaborn-talk')\nfig, ax1 =plt.subplots(1,1)\n#Remember to change the min and max so again 0 is the midpoint, but the scale is more reasonable for the completion % data\nplt.imshow(np.fliplr(np.rot90(diff_gam,3)),\n           origin='lower', aspect='equal',\n           extent=[-30, 30, -10, 60],\n           norm = mpl.colors.Normalize(vmin=-0.5, vmax=0.5),\n           cmap='PiYG')\n\ncbar = plt.colorbar()\ncbar.set_label(\"\\n Completion Prob over Leage Avg\")\n\n#Set title, remove ticks and labels\nax1.set_title('Mahomes Estimated Completion Probability Over Avg')\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-10,60)\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-10,60,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='k',ls='-',alpha=0.5, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='k',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nax1.annotate('Line of Scrimmage', (16,0),\n             textcoords=\"offset points\", # how to position the text\n                 xytext=(0,5), # distance from text to points (x,y)\n                 ha='center',fontsize = 9) # horizontal alignment can be left, right or center\n\n\nMahomes has clearly had more success than most completing passes to the deep middle of the field.\nWe could go further here by utilizing the mask we used previously and only showing extreme differences (maybe greater or less than 10% above average), but I think this is a good place to stop for this post. I was incredibly pleased with the amount of people who played around with this data and code after the last post, and hopefully this inspires even more. But please remember to credit and cite Sarah Mallepalle and her team at CMU, since so much of this code and the original scraper came from them!\n\n\n",
    "preview": "posts/2020-08-29-player-density-and-completion-surface-estimates/player-density-and-completion-surface-estimates_files/figure-html5/plotting-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 6240,
    "preview_height": 4290
  },
  {
    "path": "posts/2020-08-28-fast-data-loading/",
    "title": "Fast Data Loading",
    "description": "Loading your nfl data at 10x speed!",
    "author": [
      {
        "name": "Analytics Darkweb",
        "url": "https://twitter.com/footballdaRkweb"
      }
    ],
    "date": "2020-08-28",
    "categories": [
      "Efficiency"
    ],
    "contents": "\nMost of the time data loading isn’t something we think about when doing public data analysis. Datasets such as nflfastR aren’t that large in the grand scheme of things. But what if you’re looking to compete in the next big data bowl? Or what if you just need certain portions of nflfastR?\nThis walkthrough inspired by this NYR post: https://enpiar.com/talks/nyr-2020/#19\nAnd Thomas Mock from the Rstudio team: https://gist.github.com/jthomasmock/b8a1c6e90a199cf72c6c888bd899e84e\n\n\nlibrary(tidyverse)\nlibrary(arrow)\n\nWell, we can read in our data more efficiently to save ourselfs not only time but also RAM by not storing huge datasets in memory.\nIn order to run the arrow package as I have here, you will need the nightly build. See the above nyR post for details.\nFirst, let’s take a look at how fast we can pull down the latest pbp data from nflfastR.\n\n\nseasons <- 2010:2019\nsystem.time(\n  pbp <- \n    purrr::map_df(seasons, function(x) {\n      readr::read_csv(\n        glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.csv.gz\")\n      )\n  })\n)\n\n   user  system elapsed \n  41.53   13.61   69.14 \n\nYour results may vary here, but for me? This process usually takes 30-45 seconds. Now, if you’re easily distracted like I am that’s just enough waiting around to be dangerous!\nOf course the alternative is to simply read in the data from a local copy, which is of course faster. We’ll use data.table’s fread here as it’s much faster than both the base read.csv and tidyverse’s read_csv.\n\n\nsystem.time(pbp <- data.table::fread(\"D:/Placeholder/nflfastR.csv\"))\n\n   user  system elapsed \n   7.38    0.53    2.98 \n\nBut what if we could do better? Or, what if we needed to load in much MUCH larger files? Perhaps gigabytes each? And what if we wanted to find a way to do some filtering as well?\nEnter the arrow package.\nArrow is a C++ backend that works across multiple languages to allow you incredibly fast load times and lets you conduct some of you first steps on disk. Meaning you’re not pulling the entire file into memory first.\nFirst we need to convert our data.frame into an arrow table. I found that using uncompressed made the process much faster.\n\n\nwrite_feather(pbp, \"D:/Placeholder/New/new_file\", compression = \"uncompressed\")\n\nds <- open_dataset(\"D:/Placeholder/New/\", format = \"feather\")\n\nsystem.time(open_dataset(\"D:/Placeholder/New/\", format = \"feather\") %>% collect())\n\n   user  system elapsed \n   2.14    3.04    2.14 \n\nWe can see that arrow loads our dataset pretty fast. A little faster than fread, but what if we could make it better?\nLets say I wanted to partitian the data by both season and play type. We can do this by converting our feather file to a dataset. We should choose ways to split the data that make the most sense given our usecase. For football, it may make sense to break things down by season and playtype since those are common splits to look at.\n\n\nfeather_dir <- \"D:/nflfastR/\"\nds %>%\n  group_by(season, play_type) %>%\n  write_dataset(feather_dir, format = \"feather\")\n\nNow for our last step, direct comparison!\nFor each test I am going to open a file, filter down to a particular season, play_type, then perform some summaries.\n\n\nsystem.time(\n  data.table::fread(\"D:/Placeholder/nflfastR.csv\") %>% \n    filter(season == 2019, play_type == \"pass\") %>% \n    group_by(posteam) %>% \n    summarise(epa = mean(epa, na.rm = TRUE))\n)\n\n   user  system elapsed \n   7.63    0.50    3.13 \n\nNotice below that I am using the collect() call between group_by and summarise!\n\n\nsystem.time(\n  open_dataset(\"D:/nflfastR/\", format = \"feather\") %>% \n    filter(season == 2019, play_type == \"pass\") %>% \n    group_by(posteam) %>% \n    collect() %>% \n    summarise(epa = mean(epa, na.rm = TRUE))\n)\n\n   user  system elapsed \n   0.25    0.05    0.29 \n\nThere you have it, to read in, filter, group, and summarise from data.table’s fread takes us significantly longer to read in than using arrow’s feather data type!\nWe’ve gone from loading online in about 60 seconds, to fread in 3-5 seconds, to feather around 2 seconds, but by saving our dataset in a novel way we can reduce our look ups to fractions of a second.\nThis 10x speed up might seem not worth the effort for this one file, but as these files get larger, as you merge more sources, these techniques can save a lot of time.\n\n\nmbm <- \n  microbenchmark::microbenchmark(\n  \"fread\" = {\n    data.table::fread(\"D:/Placeholder/nflfastR.csv\") %>% \n    filter(season == 2019, play_type == \"pass\") %>% \n    group_by(posteam) %>% \n    summarise(epa = mean(epa, na.rm = TRUE))\n  },\n  \"Naive Feather\" = {\n    open_dataset(\"D:/Placeholder/New/\", format = \"feather\") %>% \n    collect() %>% \n    filter(season == 2019, play_type == \"pass\") %>% \n    group_by(posteam) %>% \n    summarise(epa = mean(epa, na.rm = TRUE))\n  },\n  \"Custom Feather\" = {\n    open_dataset(\"D:/nflfastR/\", format = \"feather\") %>% \n    filter(season == 2019, play_type == \"pass\") %>% \n    group_by(posteam) %>% \n    collect() %>% \n    summarise(epa = mean(epa, na.rm = TRUE))\n  },\n  times = 5L\n  )\n\nHere is a plot showing the loading times for various methods.\n\n\nautoplot(mbm) + \n  labs(title = \"Data loading speed\")\n\n\n\n\n",
    "preview": "posts/2020-08-28-fast-data-loading/fast-data-loading_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-08-28-expected-completion-using-logistic-generalized-additive-mixed-models/",
    "title": "Individual Expected Completion using Logistic Generalized Additive Mixed Models",
    "description": "Case study how to leverage Generalized Additive Mixed Models (GAMM) to estimate the individual probability of completion per Quarterback as a random effect.",
    "author": [
      {
        "name": "Adrian Cadena",
        "url": "https://twitter.com/adrian_cadem"
      }
    ],
    "date": "2020-08-27",
    "categories": [
      "Logistic Generalized Additive Mixed Models",
      "Mixed Effects",
      "Completion Probability Intercept"
    ],
    "contents": "\nTable of Contents\nIntro\nPackages and Data Preparation\nModel\nSummary of Results\nRetrieving Estimates and Prepare Data for Plot\nPlotting\nInterpreting Results\nConclusion\n\nIntro\nMichael Lopez posted not long ago a great article explaining how Generalized Additive Models (GAMs) are a good way to measure non-linear effects of explanatory variables x on response variable y.\nLately, I’ve been playing around with linear and logistic mixed-effects models, so I thought about combining these with GAMs to estimate the probability of completion per Quarterback while accounting for non-linearities, especially on air yards.\nTo learn more about Logistic Mixed Effects I recommend https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/\nFor a football case application of GAMs, nothing like M.Lopez’ post itself https://statsbylopez.netlify.app/post/plotting-air-yards/\nPackages and Data Preparation\nWe will be using the gamm4 library to fit our model.\n\n\nlibrary(gamm4)\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(ggthemes)\nlibrary(scales)\n\nWe’ll be working with data from 2016 to 2019. Because the NFL started to matter in 2016 when Dak Prescott was drafted.\nGAMMs can take a while to run since they also perform cross-validation. So I’ll do my best to filter-out data as much as possible without affecting results. Hold on.\n\n\n# Some stuff to filter later on\nnpass <- pbp %>%\n  dplyr::filter(\n    play_type == \"pass\",\n    season_type == \"REG\"\n  ) %>%\n  mutate(\n    play_in_19 = if_else(season == 2019, 1, 0)\n  ) %>%\n  group_by(passer_player_id) %>%\n  dplyr::summarise(\n    num_plays = n(),\n    last_seas = max(season),\n    plays_in_19 = sum(play_in_19)\n  )\n\npbp2 <- merge(pbp, npass, by = \"passer_player_id\", all.x = T, no.dups = T)\n\n# Mutations/data prep\npbp_mut <- pbp2 %>%\n  dplyr::filter(\n    season_type == \"REG\",\n    wp <= .85,\n    wp >= .15,\n    play_type == \"pass\",\n    !is.na(complete_pass),\n    penalty == 0,\n    num_plays >= 200\n  ) %>%\n  dplyr::mutate(\n    ayard_is_zero = if_else(air_yards == 0, 1, 0),\n    era1 = if_else(season %in% 2014:2017, 1, 0),\n    away = if_else(home_team == posteam, 0, 1),\n    id = passer_player_id,\n    # fixing some weird bugs I found with names bugs don't affect model, but mess with plot\n    passer_player_name = if_else(passer_player_name == \"Jos.Allen\", \"J.Allen\",\n      if_else(passer_player_name == \"R.Griffin\", \"R.Griffin III\",\n        if_else(passer_player_name == \"Matt.Moore\", \"M.Moore\",\n          if_else(passer_player_name == \"G.Minshew II\", \"G.Minshew\", passer_player_name)\n        )\n      )\n    )\n  ) %>%\n  dplyr::select(\n    id, passer_player_name, era1, season, away, wind, temp, complete_pass,\n    air_yards, qb_hit, ayard_is_zero, yardline_100, ydstogo, down,\n    plays_in_19, yardline_100\n  )\n\n# To map id's and Quarterback names\nnames <- pbp_mut %>%\n  group_by(id) %>%\n  dplyr::summarise(\n    Quarterback = unique(passer_player_name),\n    last_seas = max(season),\n    plays_in_19 = unique(plays_in_19)\n  )\n\nModel\nIt would be a good idea to add non-linear components to ydstogo and yardline_100, but I don’t want to slow down the model too much. If you have the time to do it, go for it! just do s(ydstogo) and s(yardline_100).\nSet family = binomial(link='logit') to make it a logistic binomial regression.\nOur random effect will be id since we want to look at every QB intercept.\nWe do nACG = 0 to speed up the process, it technically sacrifices accuracy, but for this exercise is no big deal.\nThis will take around 5 minutes to run, depending on your computer.\n\n\ngam_model <- gamm4(\n  complete_pass ~\n  era1 +\n    ydstogo +\n    yardline_100 +\n    down +\n    away +\n    qb_hit +\n    ayard_is_zero +\n    s(air_yards),\n  random = ~ (1 | id),\n  data = pbp_mut,\n  nAGQ = 0,\n  control = glmerControl(optimizer = \"nloptwrap\"),\n  family = binomial(link = \"logit\")\n)\n\nSummary of Results\ngamm4 returns two different summaries, one for the Generalized Additive part and one for the Mixed-Effects part. Feel free to look at the coefficients. You can also map IDs to the player’s name. I’ll do that later on. If interested, just copy the code.\nRetrieving Estimates and Prepare Data for Plot\nFirst, we will use broom.mixed to retrieve the random intercepts as well as their confidence intervals. We will also create a function to transform log-ods into probabilities. Just to make the plot easier to read.\nThen we map ids with player names using merge() and sort the data frame by descending estimate.\nFinally, we mutate confidence intervals and transform log-odds to probabilities. I am adding a threshold on the number of plays in 2019 just to make the plot clearer.\n\n\n# Retreive estimates and standard errors\nest <- broom.mixed::tidy(gam_model$mer, effects = \"ran_vals\") %>%\n  dplyr::rename(\"id\" = \"level\") %>%\n  dplyr::filter(term == \"(Intercept)\")\n\n# Function to convert logit to prob\nlogit2prob <- function(logit) {\n  odds <- exp(logit)\n  prob <- odds / (1 + odds)\n  return(prob)\n}\n\n# Prepare data for plot\nplot <- merge(est, names, by = \"id\", all.x = T, no.dups = T) %>%\n  arrange(estimate) %>%\n  mutate(\n    lci = estimate - 1.96 * std.error,\n    uci = estimate + 1.96 * std.error,\n    prob = logit2prob(estimate),\n    prob_uci = logit2prob(uci),\n    prob_lci = logit2prob(lci),\n  ) %>%\n  dplyr::filter(\n    plays_in_19 >= 100\n  )\n\nPlotting\nThe first plot includes the intercept estimate as well as confidence intervals.\n\n\nplot %>%\n  filter(\n    last_seas == 2019\n  ) %>%\n  ggplot(aes(x = factor(Quarterback, level = Quarterback), prob)) +\n  geom_point(size = .7) +\n  geom_linerange(size = .5, aes(\n    ymin = prob_lci,\n    ymax = prob_uci\n  )) +\n  coord_flip() +\n  theme_bw() +\n  labs(\n    y = \"iProbability of Completion\",\n    title = \"Individual Probability of Completion per Quarterback\",\n    subtitle = \"How each QB increases probability completion, controlling for situation | GAMM\",\n    caption = \"Data: nflfastR | Analysis by Adrian Cadena @adrian_cadem\"\n  ) +\n  theme(\n    plot.title = element_text(size = 15, hjust = .5),\n    plot.subtitle = element_text(size = 10, hjust = .5),\n    axis.title.y = element_blank(),\n  ) #+ ggsave('plot_gamm1.png', dpi=1100,width = 20, height = 15, units = \"cm\") \n\n\nThe second plot is a little easier to read and looks nice in terms of aesthetics. We are only adding the estimated probability intercept, no confidence interval.\n\n\nplot %>%\n  filter(last_seas == 2019) %>%\n  ggplot(aes(x = factor(Quarterback, level = Quarterback), prob)) +\n  geom_col(fill = \"grey20\") +\n  geom_text(aes(label = Quarterback, y = (((prob - .43) * .5)) + .43), color = \"white\", hjust = 1, size = 2.6, vjust = 0.3) +\n  coord_flip() +\n  theme_bw() +\n  labs(\n    y = \"iProbability of Completion\",\n    title = \"Individual Probability of Completion per Quarterback\",\n    subtitle = \"How each QB increases probability completion, controlling for situation | GAMM\",\n    caption = \"Data: nflfastR | Analysis by Adrian Cadena @adrian_cadem\"\n  ) +\n  scale_y_continuous(limits = c(.43, .565), oob = rescale_none, labels = scales::percent_format(accuracy = 1)) +\n  theme(\n    plot.title = element_text(size = 14, hjust = .5),\n    plot.subtitle = element_text(size = 10, hjust = .5),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_blank(),\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n  ) +\n  ggsave(\"plot_gamm2.png\", dpi = 1100, width = 20, height = 15, units = \"cm\")\n\n\nInterpreting Results\nWhen trying to predict the probability of a pass being completed, the main drivers are factors such as air yards, yard line, yards to go, down, etc. Each one of these variables increases or decreases the chances of a pass being completed. However, what is the starting probability of a pass being completed before we account for any of those variables? In other words, what is the intercept?\nBy using the passer as a random variable, we can assign an intercept for each Quarterback. This measures the individual effect on pass completion that each QB has. The higher the intercept, the higher the probability of a pass being completed, solely because of who is throwing the pass. This is an interesting alternative to CPOE because just like Expected Completion, it accounts for situational variables. However, this is just an inferential exercise, CPOE has proven to be the better predictive measure.\nConclusion\nThe purpose of this post is to invite people to play around with GAMMs. I didn’t want to break anyone’s computer so I kept it simple. Many variables could be added to the model to make it better, while also adding smoothing factors to some of the existing variables.\nThere are many other applications for GAMMs in football analysis, and I’m excited to see what you guys come up with. Reach out to me with any questions!\n\n\n",
    "preview": "posts/2020-08-28-expected-completion-using-logistic-generalized-additive-mixed-models/expected-completion-using-logistic-generalized-additive-mixed-models_files/figure-html5/Plot-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2700
  },
  {
    "path": "posts/2020-08-25-open-source-fantasy-football-visualizing-trap-backs/",
    "title": "Open Source (Fantasy) Football: Visualizing TRAP Backs",
    "description": "Using nflfastR data to visualize where on the field running backs get their carries and how that translates to the Trivial Rush Attempt Percentage (TRAP) model.",
    "author": [
      {
        "name": "Sam Hoppen",
        "url": "https://twitter.com/SamHoppen"
      }
    ],
    "date": "2020-08-26",
    "categories": [
      "Figures",
      "nflfastR",
      "Fantasy Football"
    ],
    "contents": "\nTable of Contents\nIntro\nLoading Data and Packages\nAdding Player Positions\nVisualizing TB Touch Percent Based on Distance from the End Zone\nVisualizing High-Value Touches and the TRAP Model\nIntro\nIn this first post of mine, I am going to introduce the audience to open source fantasy football (a facet of football in which running backs DO matter), specifically the concept of TRAP backs.\nTRAP stands for Trivial Rush Attempt Percentage, which is a term popularized by Ben Gretch of CBS Sports. TRAP is meant to identify running backs who get the least-valuable touches in fantasy football by measuring a player’s percentage of total touches that are low-value rush attempts outside the 10-yard line.\nLoading Data and Packages\nThe first step in this analysis, as with many of these tutorials, is to load the data that we need. This includes the NFL play-by-play data, team colors and logos data (which will be used later), and NFL player positional data, along with the necessary libraries.\n\n\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggimage)\nlibrary(nflfastR)\n\nseasons <- 2019\n\npbp <- purrr::map_df(seasons, function(x) {\n  readRDS(\n    url(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\")\n    )\n  )\n})\n\nnfl_positions <- read_csv(url(\"https://raw.githubusercontent.com/samhoppen/NFL_Positions/master/nfl_positions_2011_2019.csv\"))\n\nAdding Player Positions\nIn order to get roster positions into nflfastR (which are not pre-populated), I built a repository that includes all players (from 2011-2019) and their respective position - that’s what the “nfl_positions” data frame is for. Since we’re only looking at running backs for this example, we want to filter out the other positions.\nTo add these to our pbp data, I used a sequence of left_join functions while adding in some fields that we’ll be using throughout this article. Additionally, because I’m doing this for fantasy football analysis, I want to filter out any non-fantasy-relevant plays, which is what the first filter is doing.\n\n\npbp <- pbp %>%\n  filter(season_type == \"REG\", down <= 4, play_type != \"no_play\") %>%\n  left_join(nfl_positions, by = c(\"passer_id\" = \"player_id\")) %>%\n  rename(\n    passer_full_name = full_player_name,\n    passer_position = position\n  ) %>%\n  left_join(nfl_positions, by = c(\"receiver_id\" = \"player_id\")) %>%\n  rename(\n    receiver_full_name = full_player_name,\n    receiver_position = position\n  ) %>%\n  left_join(nfl_positions, by = c(\"rusher_id\" = \"player_id\")) %>%\n  rename(\n    rusher_full_name = full_player_name,\n    rusher_position = position\n  ) %>%\n  select(-c(\"player.x\", \"player.y\", \"player\")) %>%\n  mutate(\n    ten_zone_rush = if_else(yardline_100 <= 10 & rush_attempt == 1, 1, 0),\n    ten_zone_pass = if_else(yardline_100 <= 10 & pass_attempt == 1 & sack == 0, 1, 0),\n    ten_zone_rec = if_else(yardline_100 <= 10 & complete_pass == 1, 1, 0),\n    field_touch = case_when(\n      yardline_100 <= 100 & yardline_100 >= 81 & (rush_attempt == 1 | complete_pass == 1) ~ \"touch_100_81\",\n      yardline_100 <= 80 & yardline_100 >= 61 & (rush_attempt == 1 | complete_pass == 1) ~ \"touch_80_61\",\n      yardline_100 <= 60 & yardline_100 >= 41 & (rush_attempt == 1 | complete_pass == 1) ~ \"touch_60_41\",\n      yardline_100 <= 40 & yardline_100 >= 21 & (rush_attempt == 1 | complete_pass == 1) ~ \"touch_40_21\",\n      yardline_100 <= 20 & yardline_100 >= 0 & (rush_attempt == 1 | complete_pass == 1) ~ \"touch_20_1\",\n      TRUE ~ \"other\"\n    )\n  )\n\nVisualizing TB Touch Percent Based on Distance from the End Zone\nNow that our play-by-play data has all of the information we need, we’re ready to start building new dataframes for our analysis.\nThe first piece of analysis is looking at the area of the field in which a running back’s rush attempts comes. This helps us get a high-level view of which running backs are getting touches closer to the goal line, which are the most valuable for fantasy football.\nIn this next block of code, we have a couple of things going on. First, as mentioned earlier, we’re filtering out only the running backs and grouping them in a way to get the total count of rushes for each area of the field, as defined above. Additionally, I’ve added an extra column in the second block of code to calculate the percent of rushes in each area of the field.\n\n\nrb_touches <- pbp %>%\n  filter(rusher_position == \"RB\") %>%\n  group_by(\n    rusher_full_name,\n    rusher_player_id,\n    field_touch\n  ) %>%\n  summarize(touches = n())\n\nrb_touches <- rb_touches %>%\n  group_by(rusher_full_name, rusher_player_id) %>%\n  mutate(\n    total_touches = sum(touches),\n    pct_touches = touches / total_touches\n  ) %>%\n  filter(total_touches >= 100)\n\nNow we have all of the data we need to build our first chart, but there are still a couple of small modifications to make in order to have our chart appear the way that we want it to.\nFirst, is creating a second dataframe that we’ll use to append to our primary dataframe - all I’m doing is pulling out each players’ red zone rush percent. I’m doing this because I eventually want to sort my chart by players’ red zone rushes as a percent of total touches, from highest to lowest. This may not be the most efficient way to add this data column, but it gets the job done.\n\n\nrb_touches_2 <- rb_touches %>%\n  filter(field_touch == \"touch_20_1\") %>%\n  select(rusher_full_name, rusher_player_id, pct_touches)\n\nrb_touches <- left_join(rb_touches,\n  rb_touches_2,\n  by = c(\n    \"rusher_full_name\" = \"rusher_full_name\",\n    \"rusher_player_id\" = \"rusher_player_id\"\n  )\n)\n\nSecond is a step I’m taking to use some custom colors from the Rcolorbrewer package, which will help us better visualize which running backs are getting the highest value touches (i.e. carries closer to the end zone). What I’m doing here is transforming our “field_touch” variable to a factor. We do this so that we can order the values in a way that aligns with the coloring we want, which is what we do in the second block of code below.\n\n\nlibrary(RColorBrewer)\nrb_touches$field_touch <- as.factor(rb_touches$field_touch)\nrb_touches$field_touch <- factor(rb_touches$field_touch, levels = c(\"touch_20_1\", \"touch_40_21\", \"touch_60_41\", \"touch_80_61\", \"touch_100_81\"))\n\ncolors <- brewer.pal(name = \"RdYlGn\", n = nlevels(rb_touches$field_touch))\nnames(colors) <- rev(levels(rb_touches$field_touch))\n\nNow that we have the data in the format that we want we’re ready to build our graph (using ggplot2, of course)!\n\n\nggplot() +\n  geom_col(\n    data = rb_touches,\n    aes(x = pct_touches.x, y = reorder(rusher_full_name, pct_touches.y), fill = field_touch)\n  ) +\n  scale_fill_manual(\n    values = colors,\n    limits = c(\"touch_100_81\", \"touch_80_61\", \"touch_60_41\", \"touch_40_21\", \"touch_20_1\"), labels = c(\"100 to 81 yds\", \"80 to 61 yds\", \"60 to 41 yds\", \"40 to 21 yds\", \"20 to 1 yds\")\n  ) +\n  labs(\n    x = \"Percent of plays\",\n    fill = \"Dist from end zone\",\n    title = \"RB touch % based on how far away from the goal line the touch was (min. 100 touches):\\nAlexander Mattison & Todd Gurley lead the league in % of touches in the red zone last year\",\n    caption = \"Figure: @SamHoppen | Data: @nflfastR\"\n  ) +\n  scale_x_continuous(\n    labels = scales::percent_format(accuracy = 1),\n    expand = c(0, 0.01)\n  ) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.x = element_blank(),\n    legend.position = \"bottom\"\n  )\n\n\nVisualizing High-Value Touches and the TRAP Model\nThis isn’t all that we can do, though. Taking the next step, we focus on one of the tenets of the TRAP model: high-value touches (HVT). A high-value touch is defined as a rush attempt inside the 10 yard line or a reception anywhere on the field. To calculate a running back’s TRAP, we take the percent of a player’s non-HVTs as a percent of his total touches.\nSo, how do we do this? Well, using some of the fields that we’ve already added to the play-by-play data!\nWe’re going to start by making some new dataframes, though, so as not to get stuff mixed up. You’ll also notice that I removed Kenyan Drake’s time with the Dolphins so we can get a representation of his role with Arizona (and it makes the data a little messy).\n\n\nrb_hvt <- pbp %>%\n  filter(rusher_position == \"RB\") %>%\n  group_by(\n    rusher_full_name,\n    rusher_player_id,\n    posteam\n  ) %>%\n  summarize(\n    rush_attempts = sum(rush_attempt),\n    ten_zone_rushes = sum(ten_zone_rush),\n    receptions = sum(complete_pass),\n    total_touches = rush_attempts + receptions,\n    hvts = receptions + ten_zone_rushes,\n    non_hvts = total_touches - hvts,\n    hvt_pct = hvts / total_touches,\n    non_hvt_pct = non_hvts / total_touches\n  )\n\nrb_hvt <- rb_hvt[!(rb_hvt$rusher_full_name == \"Kenyan Drake\" & rb_hvt$posteam == \"MIA\"), ]\n\nSince the data isn’t ready-made in the correct format needed for the ggplot that we’ll be building, there are a couple minor modifications to do. The first of these is using pivot_longer to get our values matched up in the right way. Additionally, I’ve created a lookup dataframe. This is done in order to add an extra field to sort our ggplot from high to low, as we did earlier.\n\n\nrb_hvt <- rb_hvt %>%\n  pivot_longer(cols = c(hvt_pct, non_hvt_pct), names_to = \"hvt_type\", values_to = \"touch_pct\")\n\nhvt_lookup <- rb_hvt %>%\n  filter(hvt_type == \"hvt_pct\") %>%\n  select(rusher_full_name, rusher_player_id, hvt_type, touch_pct)\n\nrb_hvt <- left_join(rb_hvt,\n  hvt_lookup,\n  by = c(\n    \"rusher_full_name\" = \"rusher_full_name\",\n    \"rusher_player_id\" = \"rusher_player_id\"\n  )\n)\n\nHere, we also add the teams_colors_logos dataframe (which we loaded up earlier) as we’ll be using that as part of our visualization in the plot.\n\n\nrb_hvt <- left_join(rb_hvt,\n  teams_colors_logos,\n  by = c(\"posteam\" = \"team_abbr\")\n) %>%\n  filter(total_touches >= 100, hvt_type.x == \"hvt_pct\")\n\nNow we’ve got our data ready for visualization and are good to plot!\n\n\nggplot() +\n  geom_col(\n    data = rb_hvt,\n    aes(x = touch_pct.x, y = reorder(rusher_full_name, touch_pct.x)), fill = rb_hvt$team_color\n  ) +\n  geom_text() +\n  labs(\n    x = \"Percent of plays\",\n    fill = \"Distance from goal line\",\n    title = \"Visualization of TRAP backs, displaying RB high value touches (carries inside the 10\\nand catches) as a % of total touches (min 100 touches)\",\n    caption = \"Figure: @SamHoppen | Data: @nflfastR\"\n  ) +\n  scale_x_continuous(\n    labels = scales::percent_format(accuracy = 1),\n    limits = c(0, 0.165),\n    expand = c(0, 0)\n  ) +\n  theme(\n    axis.title.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.title.x = element_blank()\n  )\n\n\nVoila, that’s your intro to open source (fantasy) football! Hope you all enjoyed!\n\n\n",
    "preview": "posts/2020-08-25-open-source-fantasy-football-visualizing-trap-backs/open-source-fantasy-football-visualizing-trap-backs_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 5700,
    "preview_height": 3900
  },
  {
    "path": "posts/2020-08-25-expected-turnovers/",
    "title": "Expected Turnovers for Quarterbacks",
    "description": "Building expected interceptions and expected fumbles models to find QBs likely to increase or decrease their \ninterceptions and/or turnovers per dropback from 2019 to 2020.",
    "author": [
      {
        "name": "Anthony Gadaleta",
        "url": "https://twitter.com/AG_8"
      }
    ],
    "date": "2020-08-25",
    "categories": [
      "Figures",
      "nflfastR",
      "turnovers",
      "quarterbacks"
    ],
    "contents": "\nTable of Contents\nIntro\nLoad Packages, Get the Data\nExpected Interceptions\nExpected Fumbles\nPredictive Power?\nVisuals\nConclusion\nIntro\nOutside of actually scoring points, few events can swing an NFL game the way an interception or fumble can. Over the course of a season, if your quarterback is continuously giving the football away to the other team, your odds of a successful season are likely quite low.\nWith that said, all turnovers, and specifically QB turnovers, are not created equal. The goal of expected interceptions, expected fumbles and (by adding them together) expected turnovers is to measure how likely an incomplete pass or fumble is to be converted to an interception or lost fumble.1\nLoad Packages, Get the Data\n\n\nlibrary(nflfastR)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(ggimage)\nlibrary(gt)\n\nDownload the latest play-by-play data for all plays from 2006-2019. We’ll be using 2006 as the start year because that’s the first year we have air yards data fully accessible.\n\n\nseasons <- 2006:2019\npbp <- purrr::map_df(seasons, function(x) {\n  readRDS(\n    url(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.rds\")\n    )\n  )\n})\n\nDownload all NFL roster data from 1999-2019.\n\n\nroster <- readRDS(url(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/roster-data/roster.rds\"))\n\nExpected Interceptions\nWe’ll start with building the model for expected interceptions. Our independent variables will be air yards, pass location, qb hits, number of pass defenders and season.\nStart by creating an incompletions dataframe, which filters out all plays that do not result in incomplete passes or interceptions. Additionally, create the pass broken up (pbu) variable based on the number of pass defenders listed. The assumption being if more defenders are listed as defending a given pass, the more congested the throwing lane was.\n\n\nincompletions <- pbp %>%\n  filter(season_type == \"REG\" & season >= 2006 & (incomplete_pass == 1 | interception == 1)) %>%\n  select(\n    incomplete_pass, air_yards, pass_defense_1_player_id, pass_defense_2_player_id,\n    season, posteam, interception, qb_hit, week, defteam, passer, posteam, pass_location, desc\n  ) %>%\n  mutate(\n    pbu = case_when(\n      !is.na(pass_defense_1_player_id) & !is.na(pass_defense_2_player_id) &\n        (incomplete_pass == 1 | interception == 1) ~ 2,\n      !is.na(pass_defense_1_player_id) & is.na(pass_defense_2_player_id) &\n        (incomplete_pass == 1 | interception == 1) ~ 1,\n      TRUE ~ 0\n    ),\n  )\nincompletions$air_yards[is.na(incompletions$air_yards)] <- 0\nincompletions$pass_location[is.na(incompletions$pass_location)] <- \"None\"\n\nSplit into training and testing dataframes. I used 2006-2016 to train the model and 2017-2019 to test. The split comes out to approximately 79% training, 21% testing.\n\n\nint_train <- incompletions %>%\n  filter(season < 2017) %>%\n  select(-c(\n    pass_defense_1_player_id, pass_defense_2_player_id, incomplete_pass, posteam, week, defteam,\n    passer, posteam, desc\n  )) %>%\n  mutate(\n    interception = if_else(interception == 1, \"int\", \"no.int\"),\n  )\n\nint_test <- incompletions %>%\n  filter(season >= 2017)\n\nTrain the model using logistic regression, then add expected interceptions to the int_test dataframe.\n\n\nfitControl <- trainControl(\n  method = \"repeatedcv\",\n  number = 10,\n  repeats = 10,\n  classProbs = TRUE,\n  summaryFunction = twoClassSummary\n)\n\nset.seed(69) # nice\nint_model <- train(interception ~ .,\n  data = int_train,\n  method = \"glm\", preProcess = c(\"scale\", \"center\"),\n  metric = \"ROC\", trControl = fitControl\n)\nint_model\n\nGeneralized Linear Model \n\n74261 samples\n    5 predictor\n    2 classes: 'int', 'no.int' \n\nPre-processing: scaled (7), centered (7) \nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 66835, 66835, 66835, 66835, 66834, 66834, ... \nResampling results:\n\n  ROC        Sens       Spec     \n  0.9067668  0.1193285  0.9968662\n\nint_test$exp_int <- predict(int_model, int_test, type = \"prob\")\n\nLet’s take a look at the incompletions most likely have been intercepted from 2017-2019:\n\n\nint_test %>%\n  arrange(-exp_int) %>%\n  select(desc, exp_int, passer, posteam, defteam, season, week) %>%\n  head(5)\n\n# A tibble: 5 x 7\n  desc         exp_int$int $no.int passer posteam defteam season  week\n  <chr>              <dbl>   <dbl> <chr>  <chr>   <chr>    <int> <int>\n1 (:03) (Shot~       0.986  0.0141 D.Wat~ HOU     NE        2017     3\n2 (:03) (Shot~       0.985  0.0153 J.All~ BUF     JAX       2018    12\n3 (2:09) 73-C~       0.984  0.0158 M.Tru~ CHI     NO        2019     7\n4 (2:41) 5-J.~       0.983  0.0167 J.Fla~ BAL     TEN       2017     9\n5 (4:45) (Sho~       0.983  0.0175 M.Sta~ DET     NYG       2017     2\n\nExpected Fumbles\nNext up, we’ll build the expected fumbles model, using fumble forced vs not forced, fumble out of bounds, yards gained, sacks and aborted snaps as our independent variables.\nStart by creating a fumbles dataframe, which includes all plays where the ball hits the turf, regardless of which team recovers.\n\n\nfumbles <- pbp %>%\n  filter(season_type == \"REG\", season >= 2006, fumble == 1) %>%\n  select(\n    fumble_forced, fumble_not_forced, fumble_out_of_bounds, fumble_lost, fumble, yards_gained, sack,\n    aborted_play, season, posteam, week, defteam, fumbled_1_player_name, desc\n  )\n\nSplitting the same way as the incompletions, this time for an 80-20 train-test split.\n\n\nfumble_train <- fumbles %>%\n  filter(season < 2017) %>%\n  select(-c(season, posteam, week, fumble, defteam, fumbled_1_player_name, desc)) %>%\n  mutate(\n    fumble_lost = if_else(fumble_lost == 1, \"lost\", \"recovered\")\n  )\nfumble_test <- fumbles %>%\n  filter(season >= 2017)\n\nTrain the model using logistic regression (we can reuse the trControl from above) and then add expected fumbles to the fumble_test dataframe.\n\n\nset.seed(69) # nice\nfumble_model <- train(fumble_lost ~ .,\n  data = fumble_train,\n  method = \"glm\", preProcess = c(\"scale\", \"center\"),\n  trControl = fitControl, metric = \"ROC\"\n)\nfumble_model\n\nGeneralized Linear Model \n\n7642 samples\n   6 predictor\n   2 classes: 'lost', 'recovered' \n\nPre-processing: scaled (6), centered (6) \nResampling: Cross-Validated (10 fold, repeated 10 times) \nSummary of sample sizes: 6878, 6877, 6878, 6878, 6878, 6878, ... \nResampling results:\n\n  ROC        Sens       Spec     \n  0.7115302  0.7990853  0.5359961\n\nfumble_test$exp_fl <- predict(fumble_model, fumble_test, type = \"prob\")\n\nLet’s take a look at the fumbles most likely to have been lost from 2017-2019:\n\n\nfumble_test %>%\n  arrange(-exp_fl) %>%\n  select(desc, exp_fl, fumbled_1_player_name, posteam, defteam, season, week) %>%\n  head(5)\n\n# A tibble: 5 x 7\n  desc  exp_fl$lost $recovered fumbled_1_playe~ posteam defteam season\n  <chr>       <dbl>      <dbl> <chr>            <chr>   <chr>    <int>\n1 (:01~       0.852      0.148 T.Cohen          CHI     GB        2019\n2 (:05~       0.834      0.166 J.Witten         DAL     GB        2017\n3 (10:~       0.834      0.166 J.Jones          KC      LAC       2018\n4 (:02~       0.834      0.166 J.Edelman        NE      MIA       2019\n5 (:04~       0.824      0.176 J.Crowder        WAS     KC        2017\n# ... with 1 more variable: week <int>\n\nPredictive Power?\nNow it’s time to see if these new stats are actually useful for predicting future turnovers.\nFirst, modify roster names to allow us to use merge roster data with the dataframes we created above.\n\n\nroster$name <- paste0(substr(roster$teamPlayers.firstName, 1, 1), \".\", roster$teamPlayers.lastName)\n\nMerge roster data and group by passer and season to get total interceptions and expected interceptions and then total fumbles lost and expected fumbles lost for each passer in each season.\n\n\nxInt <- int_test %>%\n  filter(!is.na(passer)) %>%\n  left_join(roster[, c(\n    \"team.season\", \"name\", \"teamPlayers.position\", \"team.abbr\",\n    \"teamPlayers.headshot_url\"\n  )],\n  by = c(\"passer\" = \"name\", \"season\" = \"team.season\", \"posteam\" = \"team.abbr\")\n  ) %>%\n  rename(\n    position = teamPlayers.position,\n    player = passer,\n  ) %>%\n  filter(position == \"QB\") %>%\n  group_by(player, posteam, season, teamPlayers.headshot_url) %>%\n  summarise(Interceptions = sum(interception), xInt = sum(exp_int$int)) %>%\n  mutate(diff = Interceptions - xInt)\n\nxFmb <- fumble_test %>%\n  filter(!is.na(fumbled_1_player_name)) %>%\n  left_join(roster[, c(\n    \"team.season\", \"name\", \"teamPlayers.position\", \"team.abbr\",\n    \"teamPlayers.headshot_url\"\n  )],\n  by = c(\"fumbled_1_player_name\" = \"name\", \"season\" = \"team.season\", \"posteam\" = \"team.abbr\")\n  ) %>%\n  rename(\n    position = teamPlayers.position,\n    player = fumbled_1_player_name,\n  ) %>%\n  filter(position == \"QB\") %>%\n  group_by(player, posteam, season, teamPlayers.headshot_url) %>%\n  summarise(Fumbles_Lost = sum(fumble_lost), xFmb = sum(exp_fl$lost)) %>%\n  mutate(diff = Fumbles_Lost - xFmb)\n\nFind total dropbacks, epa per dropback and success rate on dropbacks for each passer. The latter two stats really aren’t necessary, but I thought it could be useful to show how well certain quarterbacks performed overall.\n\n\ndropbacks <- pbp %>%\n  filter(season_type == \"REG\" & season > 2016 & !is.na(passer)) %>%\n  group_by(passer, season) %>%\n  summarise(dropbacks = n(), epa = mean(epa, na.rm = TRUE), sr = mean(success, na.rm = TRUE))\n\nMerge the dropbacks dataframe with the xInt and xFmb dataframes. Then calc total turnovers, expected turnovers, turnovers per dropback, interceptions per dropback, differences between all of the actual and expected stats and the next season’s turnovers, interceptions and fumbles.\n\n\nxTO <- dropbacks %>%\n  inner_join(xInt, by = c(\"passer\" = \"player\", \"season\")) %>%\n  left_join(xFmb, by = c(\"passer\" = \"player\", \"posteam\", \"season\", \"teamPlayers.headshot_url\"))\n\nxTO$Fumbles_Lost[is.na(xTO$Fumbles_Lost)] <- 0\nxTO$xFmb[is.na(xTO$xFmb)] <- 0\nxTO$diff.y[is.na(xTO$diff.y)] <- 0\n\nxTO <- xTO %>%\n  mutate(\n    Turnovers = Interceptions + Fumbles_Lost,\n    xTO = xInt + xFmb,\n    diff = diff.x + diff.y,\n    to_pct = Turnovers / dropbacks,\n    int_pct = Interceptions / dropbacks,\n    xto_pct = xTO / dropbacks,\n    xint_pct = xInt / dropbacks,\n    to_pct_diff = xto_pct - to_pct,\n    int_pct_diff = xint_pct - int_pct,\n  ) %>%\n  filter(dropbacks >= 250) %>%\n  group_by(passer) %>%\n  mutate(\n    next_to = lead(Turnovers, 1),\n    next_int = lead(Interceptions, 1),\n    next_fmb = lead(Fumbles_Lost, 1),\n    next_db = lead(dropbacks, 1)\n  )\n\nFinally, let’s evaluate how predictive our new expected statistics are compared to their standard counterparts.\n\n\n[1] \"R2 of current TOs to next season's TOs: 0.187481148618913\"\n\n[1] \"R2 of current xTO to next season's TOs: 0.118063100859092\"\n\n[1] \"R2 of current Ints to next season's Ints: 0.178815133484698\"\n\n[1] \"R2 of current xInts to next season's Ints: 0.144854426175095\"\n\n[1] \"R2 of current Fumbles to next season's Fumbles: 0.204064081434686\"\n\n[1] \"R2 of current xFmb to next season's Fumbles: 0.106391377897902\"\n\nAs we can see, the “regular” stats outperform all of the expected turnover statistics. But what if we look at these as rate stats per dropback?\n\n\n[1] \"R2 of current TOs per dropback to next season's TOs per dropback: 0.259161533025725\"\n\n[1] \"R2 of current xTOs per dropback to next season's TOs per dropback: 0.315618037803437\"\n\n[1] \"R2 of current Ints per dropback to next season's Ints per dropback: 0.211910004427118\"\n\n[1] \"R2 of current xInts per dropback to next season's Ints per dropback: 0.297221826811028\"\n\nNow we’ve got some winners! We can predict next season’s turnovers and interceptions per dropback more effectively using xTOs and xInts per dropback than we can using the ordinary rate stats.\nVisuals\nFinally, we can plot the results to help us visualize who is most likely to turn the ball over at a higher or lower rate next season.\n\n\nggplot(subset(xTO, season == 2019), aes(x = Turnovers / dropbacks, y = xTO / dropbacks)) +\n  geom_image(aes(image = teamPlayers.headshot_url), size = 0.05, asp = 16 / 9) +\n  labs(\n    title = \"QB Turnovers 2019\",\n    subtitle = \"Regular Season | Min. 250 Dropbacks\",\n    x = \"Actual Turnovers per Dropback\",\n    y = \"Expected Turnovers per Dropback\",\n    caption = \"@AG_8 | Data: @nflfastR\"\n  ) +\n  theme_bw() +\n  theme(\n    aspect.ratio = 9 / 16,\n    plot.title = element_text(size = 12, hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, hjust = 0.5),\n  ) +\n  geom_abline(slope = 1, intercept = 0)\n\n\n\n\nggplot(subset(xTO, season == 2019), aes(x = Interceptions / dropbacks, y = xInt / dropbacks)) +\n  geom_image(aes(image = teamPlayers.headshot_url), size = 0.05, asp = 16 / 9) +\n  labs(\n    title = \"QB Interceptions 2019\",\n    subtitle = \"Regular Season | Min. 250 Dropbacks\",\n    x = \"Actual Interceptions per Dropback\",\n    y = \"Expected Interceptions per Dropback\",\n    caption = \"@AG_8 | Data: @nflfastR\"\n  ) +\n  theme_bw() +\n  theme(\n    aspect.ratio = 9 / 16,\n    plot.title = element_text(size = 12, hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, hjust = 0.5),\n  ) +\n  geom_abline(slope = 1, intercept = 0)\n\n\nLastly, we’ll use the gt package to make a really cool looking table of the 2019 data.\n\n\nxTO %>%\n  ungroup() %>%\n  filter(season == 2019) %>%\n  select(c(passer, posteam, dropbacks, to_pct, xto_pct, to_pct_diff, int_pct, xint_pct, int_pct_diff)) %>%\n  mutate(\n    to_pct_diff = to_pct_diff * 100,\n    int_pct_diff = int_pct_diff * 100\n  ) %>%\n  gt() %>%\n  tab_header(\n    title = \"Expected QB Turnovers\",\n    subtitle = \"Regular Season 2019 | Min. 250 Dropbacks\"\n  ) %>%\n  cols_label(\n    passer = \"QB\",\n    posteam = \"Team\",\n    dropbacks = \"Dropbacks\",\n    to_pct = \"TOs per Dropback\",\n    xto_pct = \"xTOs per Dropback\",\n    to_pct_diff = \"xTOs/DB - TOs/DB\",\n    int_pct = \"Ints per Dropback\",\n    xint_pct = \"xInts per Dropback\",\n    int_pct_diff = \"xInts/DB - Ints/DB\"\n  ) %>%\n  fmt_number(\n    columns = c(\"to_pct\", \"xto_pct\", \"to_pct_diff\", \"int_pct\", \"xint_pct\", \"int_pct_diff\"),\n    decimals = 2\n  ) %>%\n  fmt_percent(\n    columns = c(\"to_pct\", \"xto_pct\", \"int_pct\", \"xint_pct\")\n  ) %>%\n  tab_source_note(\"@AG_8 | Data: @nflfastR\") %>%\n  data_color(\n    columns = c(\"to_pct\", \"xto_pct\", \"to_pct_diff\", \"int_pct\", \"xint_pct\", \"int_pct_diff\"),\n    colors = scales::col_numeric(palette = \"Reds\", domain = NULL)\n  ) %>%\n  cols_align(align = \"center\") %>%\n  cols_width(\n    everything() ~ px(90)\n  )\nExpected QB Turnovers\n    Regular Season 2019 | Min. 250 Dropbacks\n    QB\n      Team\n      Dropbacks\n      TOs per Dropback\n      xTOs per Dropback\n      xTOs/DB - TOs/DB\n      Ints per Dropback\n      xInts per Dropback\n      xInts/DB - Ints/DB\n    A.Dalton\n      CIN\n      601\n      3.00&percnt;\n      2.64&percnt;\n      −0.35\n      2.33&percnt;\n      2.15&percnt;\n      −0.18\n    A.Rodgers\n      GB\n      677\n      1.18&percnt;\n      1.88&percnt;\n      0.70\n      0.59&percnt;\n      1.61&percnt;\n      1.02\n    B.Mayfield\n      CLE\n      642\n      3.58&percnt;\n      3.01&percnt;\n      −0.57\n      3.27&percnt;\n      2.54&percnt;\n      −0.73\n    C.Keenum\n      WAS\n      285\n      2.81&percnt;\n      3.02&percnt;\n      0.21\n      1.75&percnt;\n      1.96&percnt;\n      0.21\n    C.Wentz\n      PHI\n      714\n      1.96&percnt;\n      2.92&percnt;\n      0.96\n      0.98&percnt;\n      1.94&percnt;\n      0.96\n    D.Brees\n      NO\n      410\n      0.98&percnt;\n      1.60&percnt;\n      0.62\n      0.98&percnt;\n      1.60&percnt;\n      0.62\n    D.Haskins\n      WAS\n      256\n      3.52&percnt;\n      2.81&percnt;\n      −0.70\n      2.73&percnt;\n      1.88&percnt;\n      −0.85\n    D.Jones\n      NYG\n      561\n      4.10&percnt;\n      3.89&percnt;\n      −0.21\n      2.14&percnt;\n      2.29&percnt;\n      0.15\n    D.Prescott\n      DAL\n      675\n      1.93&percnt;\n      2.22&percnt;\n      0.30\n      1.63&percnt;\n      1.81&percnt;\n      0.19\n    D.Watson\n      HOU\n      615\n      2.44&percnt;\n      2.53&percnt;\n      0.09\n      1.95&percnt;\n      1.95&percnt;\n      −0.01\n    J.Allen\n      BUF\n      566\n      2.30&percnt;\n      2.75&percnt;\n      0.46\n      1.59&percnt;\n      1.89&percnt;\n      0.30\n    J.Brissett\n      IND\n      540\n      2.04&percnt;\n      2.34&percnt;\n      0.30\n      1.11&percnt;\n      1.78&percnt;\n      0.67\n    J.Flacco\n      DEN\n      313\n      2.56&percnt;\n      2.74&percnt;\n      0.18\n      1.60&percnt;\n      1.43&percnt;\n      −0.16\n    J.Garoppolo\n      SF\n      561\n      3.21&percnt;\n      2.59&percnt;\n      −0.62\n      2.32&percnt;\n      1.89&percnt;\n      −0.43\n    J.Goff\n      LA\n      704\n      2.98&percnt;\n      2.41&percnt;\n      −0.57\n      2.27&percnt;\n      1.80&percnt;\n      −0.47\n    J.Winston\n      TB\n      743\n      4.71&percnt;\n      3.38&percnt;\n      −1.33\n      4.04&percnt;\n      2.62&percnt;\n      −1.42\n    K.Allen\n      CAR\n      575\n      4.00&percnt;\n      3.72&percnt;\n      −0.28\n      2.78&percnt;\n      2.67&percnt;\n      −0.11\n    K.Cousins\n      MIN\n      514\n      1.75&percnt;\n      1.97&percnt;\n      0.22\n      1.17&percnt;\n      1.12&percnt;\n      −0.04\n    K.Murray\n      ARI\n      654\n      2.14&percnt;\n      1.90&percnt;\n      −0.24\n      1.83&percnt;\n      1.70&percnt;\n      −0.13\n    L.Jackson\n      BAL\n      482\n      1.66&percnt;\n      2.16&percnt;\n      0.50\n      1.24&percnt;\n      1.60&percnt;\n      0.35\n    M.Rudolph\n      PIT\n      331\n      2.72&percnt;\n      2.94&percnt;\n      0.22\n      2.72&percnt;\n      2.30&percnt;\n      −0.42\n    M.Ryan\n      ATL\n      731\n      2.60&percnt;\n      2.59&percnt;\n      −0.01\n      1.92&percnt;\n      1.91&percnt;\n      −0.00\n    M.Stafford\n      DET\n      331\n      2.42&percnt;\n      2.91&percnt;\n      0.49\n      1.51&percnt;\n      2.26&percnt;\n      0.75\n    M.Trubisky\n      CHI\n      606\n      1.98&percnt;\n      2.27&percnt;\n      0.29\n      1.65&percnt;\n      1.96&percnt;\n      0.31\n    P.Mahomes\n      KC\n      581\n      1.20&percnt;\n      1.37&percnt;\n      0.16\n      0.86&percnt;\n      1.18&percnt;\n      0.32\n    P.Rivers\n      LAC\n      676\n      3.40&percnt;\n      2.81&percnt;\n      −0.59\n      2.96&percnt;\n      2.30&percnt;\n      −0.66\n    R.Fitzpatrick\n      MIA\n      613\n      2.45&percnt;\n      2.84&percnt;\n      0.39\n      2.12&percnt;\n      2.14&percnt;\n      0.02\n    R.Tannehill\n      TEN\n      349\n      2.58&percnt;\n      3.29&percnt;\n      0.71\n      1.72&percnt;\n      2.45&percnt;\n      0.73\n    R.Wilson\n      SEA\n      651\n      1.23&percnt;\n      2.01&percnt;\n      0.78\n      0.77&percnt;\n      1.48&percnt;\n      0.72\n    S.Darnold\n      NYJ\n      516\n      3.10&percnt;\n      2.54&percnt;\n      −0.56\n      2.52&percnt;\n      1.86&percnt;\n      −0.65\n    T.Brady\n      NE\n      685\n      1.31&percnt;\n      1.63&percnt;\n      0.32\n      1.17&percnt;\n      1.38&percnt;\n      0.21\n    @AG_8 | Data: @nflfastR\n    \n\nConclusion\nBased on the table and plots above, we can see that Jameis had, by far, the largest difference between expected and actual turnovers. This really isn’t much of a shock since you of course need some bad luck to have as high of a turnover rate as he did.\nAdditionally, I found it interesting that although Rodgers and Wentz were two of “luckiest” with turnovers from last year, if they regress to their xTO and xInt numbers they would still be average or maybe even slightly above average, just in terms of turnovers.\nFinally, although Daniel Jones and Kyle Allen were terrible in terms of hanging onto the rock last season, there isn’t a ton of hope for improvement for either. Both had xTO rates just slightly below their actual TO rates.\nWithout tracking data, it really doesn’t make sense to calculate the likelihood of an interception or fumble on all plays.↩\n",
    "preview": "posts/2020-08-25-expected-turnovers/expected-turnovers_files/figure-html5/unnamed-chunk-15-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3300
  },
  {
    "path": "posts/2020-08-24-getting-into-sports-analytics/",
    "title": "Getting into sports analytics",
    "description": "Collection of short answers to common questions.",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin"
      }
    ],
    "date": "2020-08-24",
    "categories": [
      "Getting started"
    ],
    "contents": "\nTable of Contents\nIntroduction\nLearning R\nShould I learn R or python?\nBig Data Bowl / Machine Learning\nWhat degree should I get?\nI’m in college, what should I do?\nAdvice from people in the industry\nIntroduction\nI get a lot of emails asking about how to start learning things. Rather than being useful to one person, I’m going to start collecting the questions (anonymized) and answers here.\nLearning R\nQ: How can I get started learning R?\nI wrote a beginner’s guide for working with NFL data in R that people seem to find useful\nThis book “R for Data Science” is the go-to for getting started\nFor later on: Advanced R by Hadley Wickham\nShould I learn R or python?\nShort answer: it doesn’t matter, just pick one and get good at it.\nLonger answer: More people in the football public analytics community use R, so there are more resources for getting up and running faster. In addition, if there is one thing that R is better at python at, it is cleaning and manipulating data, so if all you care about is working with data, R might be a better choice to start. At the same time, python is also great, and in the long run if you end up doing data analysis for a career, you’re probably going to end up learning both at some point anyway. And if you start doing machine learning (ML) stuff (see next question), python generally has more packages and tools available, and most ML courses are taught using python.\nBig Data Bowl / Machine Learning\nQ: I know about the NFL big data bowl and that the papers of many finalists are available on the internet. But I believe that I lack the skills needed to understand those papers and use them to answer my questions. What statistics and machine learning resources do you recommend I use to learn the necessary machine learning that can be applied to football, when appropriate, to answer my questions?\nUnfortunately, economics doesn’t give much training in machine learning. I too couldn’t really understand the Big Data Bowl stuff until I took this course (all the videos and homeworks are free and posted online), which was pretty challenging but gives a great foundation for what ML means and how to think about it.\nLecture videos\nLecture slides\nHomework assignments: bottom of page. Completed in python\nThis course is very similar to the famous Stanford 231n course – the instructor used to teach 231n – but the homeworks are in Colab so there’s much less setup involved with getting python up and running. I did the first four assignments and finally could understand the Big Data Bowl winning solution.\nI also highly recommend An Introduction to Statistical Learning, and I have been recommended this book, Bayesian Data Analysis.\nWhat degree should I get?\nQ: Do I need a PhD to get into football analytics?\nNo, definitely not. Getting a PhD is not required and certainly not even expected for doing this kind of stuff, although there are certainly benefits to having one if you enjoy research (note: maximizing earnings is not one of those benefits). The big question is what you want to do. If it’s work for a team, you’d want to beef up your technical skills, perhaps through a Master’s program, and do stuff like compete in the Big Data Bowls, conduct research and get it out to the public, etc. If you want to go to grad school in econ, you’d probably want to do something like gaining research experience working under a professor and, if needed, taking the math/stats classes needed to be a good candidate for grad school. Ultimately this comes down to what you value so there’s no right answer imo, but earning a PhD is way, way overkill if what you want to do is work for a team. And finally, getting into sports (especially with a team) is hard so thinking about what you’d want to do if you don’t is also useful- i.e. ideally one would pick a field that is employable and inherently interesting to them.\nQ: What field should I choose?\nMy background was in econ and that’s not the best preparation for getting into sports analytics (something like statistics or other fields with more exposure to data science / machine learning tools gives better training), with the caveat that I was in school a long time ago so maybe what is taught has chagned since then. With that said, here’s an example program- this is what Sean Clement did prior to getting hired by the Ravens (see in particular the Data Science track). Derrick Yam (Ravens) has a Master’s in biostatistics, Sarah Bailey (Rams) a Master’s in statistics, Sarah Mallepalle (Ravens) a B.S. in Statistics and Machine Learning, etc. These programs are a lot more technical than what you’d get in an MBA (which don’t make them better or worse, just more aligned with what the people getting these jobs are doing). Finally, I’ve heard good things about Coursera but haven’t personally used it.\nI’m in college, what should I do?\nThere’s no one path, but some good answers to this when I posed this question on twitter:\nEthan: “learn to code (R/Python), learn stats, start doing projects, ideally publicly, focus on communicating your results, get domain knowledge, including coach vocabulary”\nFrom NESSIS talks: “Do analytics and publish it”\nCanzhi: “dont major in one of those sports management programs or whatever. learn math / stats. learn to write code. learn how web technologies work so you can scrape your own data. then build stuff and show people :)”\nAdvice from people in the industry\nSee Namita Nandakumar’s excellent thread here. To highlight two tweets:\n\n\n\n\nMatthew Barlowe:\n\n\n\nCaio Brighenti:\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-24-visualizing-epsns-total-qbr-using-interactive-plots/",
    "title": "Visualizing EPSN's Total QBR Using Interactive Plots",
    "description": "How to get ESPN data and create interactive plots using the plotly ggplot2 library.",
    "author": [
      {
        "name": "Sebastian Carl",
        "url": "https://twitter.com/mrcaseb"
      }
    ],
    "date": "2020-08-24",
    "categories": [
      "Scraping",
      "espnscrapeR",
      "Interactive plots",
      "Total QBR",
      "Figures"
    ],
    "contents": "\n\nContents\nPreface\nHow to get the data\nCreate some static weekly QBR plots\nLet’s make it interactive\n\nPreface\nWhen I first started working with ESPN’s Total QBR data I developed my own code to scrape the website. For this post I dug out the old (bad) code and wanted to make it prettier. But at the last minute I remembered something: there is this undervalued R package espnscrapeR developed and maintained by Thomas Mock. So instead of implementing the scraper by myself in this post I’ll show off, how to get all the data and save it locally for later reuse.\nAnd as the title suggests we want to try out something new here: interactive plots!\nHow to get the data\nWe want to use the espnscrapeR in this section. So let’s install it by running the following code block.\n\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"jthomasmock/espnscrapeR\")\n\n\n\nNow that espnscrapeR is installed we will load weekly Total QBR.\nWe’ll do this by writing the function get_week_qbr which loads the QBR data for a given week and season.\n\n\nget_week_qbr <- function(s, wk) {\n  if (wk <= 17) {\n    out <- espnscrapeR::get_nfl_qbr(season = s, season_type = \"Regular\", week = wk)\n  } else if (wk > 17 & s != 2017) {\n    out <- espnscrapeR::get_nfl_qbr(season = s, season_type = \"Playoffs\", week = wk - 17)\n  }\n}\n\n\n\nThen we call this function to actually do the job and save the output. Note: We call the function for the weeks 1 to 21 although the SB is week 22 (in ESPN’s convention) because of the Pro Bowl. The package automatically converts to week 22 for us.\nThe code is somewhat advanced and you may need to do some more background reading to understand it. The short version: we call the above function for all seasons between 2006 and 2019 and all available weeks in parallel running processes and bind everything together in a dataframe which get’s saved to disc for further usage.\n\n\nlibrary(dplyr)\nfuture::plan(\"multiprocess\")\n\n# Total QBR data as available beginning in 2006\n# Note: Playoffs 2017 are missing\nseasons <- 2006:2019\n\nall_qbr <-\n  furrr::future_pmap_dfr(purrr::transpose(purrr::cross2(seasons, 1:21)), get_week_qbr) %>%\n  dplyr::mutate(\n    game_week = as.numeric(game_week),\n    game_week = dplyr::if_else(season_type == \"Playoffs\", game_week + 17, game_week)\n  ) %>%\n  dplyr::arrange(season, game_week)\n\n# save to disk------------------------------------------------------------------\n# binary\nsaveRDS(all_qbr, file = \"all_qbr.rds\")\n\n# ASCII\nreadr::write_csv(all_qbr, \"all_qbr.csv\")\n\n\n\nCreate some static weekly QBR plots\nTo better demonstrate the advantages of an interactive plot I want to start with some static charts by plotting weekly QBR for some selected QBs (the insane numbers). We want to plot multiple QBs so let’s create a function again.\nBefore we do this please note there are some Quarterbacks appearing with special team names (e.g. \"DEN/KC\") in the data because they changed their team within a season. Our new function won’t work for those QBs but I was too lazy to rewrite it because I honestly don’t care about those QBs. So please be careful when trying to run the code with one of the following names and corresponding seasons.\n\n\nlibrary(tidyverse)\nreadRDS(\"all_qbr.rds\") %>%\n  dplyr::filter(stringr::str_length(team) > 3) %>%\n  dplyr::group_by(season, short_name, team) %>%\n  dplyr::summarise() %>%\n  knitr::kable(\"simple\", align = \"c\")\n\n\nseason\nshort_name\nteam\n2011\nK. Orton\nDEN/KC\n2013\nJ. Freeman\nMIN/TB\n2013\nM. Flynn\nGB/OAK\n2014\nC. Keenum\nSTL/HOU\n2015\nM. Cassel\nBUF/DAL\n2015\nR. Mallett\nBAL/HOU\n2016\nM. Barkley\nCHI/ARI\n2016\nM. Sanchez\nDAL/DEN\n2017\nT.J. Yates\nBUF/HOU\n2018\nM. Barkley\nBUF/CIN\n2019\nJ. Driskel\nCIN/DET\n\nNow let’s write the plotting function plot_weekly_qbr. To call it we have to pass a year (season), the Quarterback name as listed in the ESPN data and our above saved QBR data.\n\n\nplot_weekly_qbr <- function(year, QB, all_qbr_file) {\n\n  # Filter the QB we want to look at, modify team names and week numbers\n  # to fit nflfastR convention and add the team color of the QB\n  single_qbr <- all_qbr_file %>%\n    dplyr::filter(season == year & name == QB) %>%\n    dplyr::mutate(\n      team = dplyr::case_when(\n        team == \"LAR\" ~ \"LA\",\n        team == \"WSH\" ~ \"WAS\",\n        TRUE ~ team\n      ),\n      game_week = dplyr::if_else(game_week > 21, 21, game_week)\n    ) %>%\n    dplyr::left_join(\n      nflfastR::teams_colors_logos %>% select(team_abbr, team_color),\n      by = c(\"team\" = \"team_abbr\")\n    )\n\n  # Search for the QBs opponents using the schedule function of nflfastR\n  # and add logos of the opponents\n  opponents <- nflfastR::fast_scraper_schedules(year) %>%\n    dplyr::filter(home_team == single_qbr$team | away_team == single_qbr$team) %>%\n    dplyr::mutate(\n      opp = dplyr::if_else(home_team == single_qbr$team, away_team, home_team)\n    ) %>%\n    dplyr::left_join(\n      nflfastR::teams_colors_logos %>% select(team_abbr, team_logo_espn),\n      by = c(\"opp\" = \"team_abbr\")\n    ) %>%\n    dplyr::mutate(\n      grob = purrr::map(seq_along(team_logo_espn), function(x) {\n        grid::rasterGrob(magick::image_read(team_logo_espn[[x]]))\n      })\n    )\n\n  # Combine the QBR data of the chosen QB with the game data\n  chart <- single_qbr %>%\n    dplyr::left_join(opponents, by = c(\"game_week\" = \"week\"))\n\n  # Set title string for later usage\n  if (max(chart$game_week) > 17) {\n    title_string <- glue::glue(\"{QB} Weekly Total QBR {year} including Playoffs\")\n  } else {\n    title_string <- glue::glue(\"{QB} Weekly Total QBR {year} Regular Season\")\n  }\n\n  # going to draw some quantile lines and combine them here\n  quantiles <- c(\n    quantile(all_qbr_file$qbr_total, 0.10),\n    quantile(all_qbr_file$qbr_total, 0.25),\n    quantile(all_qbr_file$qbr_total, 0.75),\n    quantile(all_qbr_file$qbr_total, 0.90),\n    quantile(all_qbr_file$qbr_total, 0.98)\n  )\n\n  chart %>%\n    ggplot(aes(x = game_week, y = qbr_total)) +\n    geom_hline(yintercept = quantiles, color = \"black\", linetype = \"dashed\", alpha = 0.7) +\n    geom_hline(yintercept = quantile(all_qbr_file$qbr_total, 0.50), color = \"black\", linetype = \"solid\", alpha = 0.7) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.10), label = \"10th Percentile\", hjust = 1, size = 2) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.25), label = \"25th Percentile\", hjust = 1, size = 2) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.50), label = \"50th Percentile\", hjust = 1, size = 2) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.75), label = \"75th Percentile\", hjust = 1, size = 2) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.90), label = \"90th Percentile\", hjust = 1, size = 2) +\n    geom_text(x = 0, y = 2 + quantile(all_qbr_file$qbr_total, 0.98), label = \"98th Percentile\", hjust = 1, size = 2) +\n    geom_line(colour = chart$team_color) +\n    scale_x_continuous(\n      limits = c(-1.4, NA),\n      breaks = scales::breaks_pretty()\n    ) +\n    ggpmisc::geom_grob(aes(x = game_week, y = qbr_total, label = grob), vp.width = 0.05) +\n    labs(\n      x = \"Game Week\",\n      y = \"ESPN Total QBR\",\n      caption = \"Figure: @mrcaseb | Data: espnscrapeR by @thomas_mock\",\n      title = title_string,\n      subtitle = \"Percentiles of the whole NFL 2006 - 2019 are drawn for orientation\\nTo qualify, a player must play a minimum of 20 action plays per team game\"\n    ) +\n    ggthemes::theme_stata(scheme = \"sj\", base_size = 8) +\n    theme(\n      plot.title = element_text(face = \"bold\"),\n      plot.caption = element_text(hjust = 1),\n      axis.text.y = element_text(angle = 0, vjust = 0.5),\n      legend.title = element_text(size = 8, hjust = 0, vjust = 0.5, face = \"bold\"),\n      legend.position = \"top\",\n      aspect.ratio = 1 / 1.618\n    ) +\n    NULL\n}\n\n\n\nWell, that’s a lot of code. Let’s see if it is working properly by choosing the insane 2007 season of Tom Brady (in terms of Total QBR the all-time best season).\n\n\nall_qbr <- readRDS(\"all_qbr.rds\")\nplot_weekly_qbr(2007, \"Tom Brady\", all_qbr)\n\n\n\n\nThe other two in the all-time best seasons in terms of Total QBR are Peyton Manning in 2006 and Aaron Rodgers in 2011.\n\n\nplot_weekly_qbr(2006, \"Peyton Manning\", all_qbr)\n\n\n\n\n\n\nplot_weekly_qbr(2011, \"Aaron Rodgers\", all_qbr)\n\n\n\n\nAnd what does it look like if the season wasn’t that good?\n\n\nplot_weekly_qbr(2019, \"Mitchell Trubisky\", all_qbr)\n\n\n\n\nLet’s make it interactive\nThe above plots are nice and all but actually we may miss a lot of information when looking at them. For example, we might want to know\nif the game was at home or on the road,\nwhat the result was or\nhow many QB plays were counted for Total QBR.\nAdditionally we are only looking at a single season because plotting more will be a little overwhelming. Instead, it would be cool if you could watch a range of games from several seasons.\nThis is what we are going to do now. We will use the Plotly ggplot2 Library1 so you’ll have to install it if it’s not installed yet.\nI will use a function again because I want to compare two Quarterbacks. Please note that the code looks similar to the above example but it differs in some points.\nThe additional information will be prompted if you hover with the mouse over an opponents logo. We have to create the shown text in this code block as well.\n\n\ninteractive_weekly_qbr <- function(first_year, last_year, QB, all_qbr_file) {\n\n  # Filter the QB we want to look at, modify team names and week numbers\n  # to fit nflfastR convention and add the team color of the QB\n  single_qbr <- all_qbr_file %>%\n    dplyr::filter(dplyr::between(season, first_year, last_year) & name == QB) %>%\n    dplyr::mutate(\n      team = dplyr::case_when(\n        team == \"LAR\" ~ \"LA\",\n        team == \"WSH\" ~ \"WAS\",\n        team == \"OAK\" ~ \"LV\",\n        TRUE ~ team\n      ),\n      game_week = dplyr::if_else(game_week > 21, 21, game_week),\n      gm = 1:dplyr::n() # this is a running game number within the chosen era. We'll use it to plot\n    ) %>%\n    dplyr::left_join(\n      nflfastR::teams_colors_logos %>% select(team_abbr, team_color),\n      by = c(\"team\" = \"team_abbr\")\n    )\n\n  # Search for the QBs opponents using the schedule function of nflfastR\n  # and add logos of the opponents\n  opponents <- nflfastR::fast_scraper_schedules(first_year:last_year) %>%\n    dplyr::filter(home_team == single_qbr$team | away_team == single_qbr$team) %>%\n    dplyr::mutate(\n      opp = dplyr::if_else(home_team == single_qbr$team, away_team, home_team),\n      opp = dplyr::case_when(\n        opp == \"OAK\" ~ \"LV\",\n        TRUE ~ opp\n      ),\n\n      # create string for game location\n      loc_desc = dplyr::if_else(\n        home_team == single_qbr$team,\n        glue::glue(\"vs. {away_team}\"),\n        glue::glue(\"@ {home_team}\")\n      ),\n\n      # create string for game description\n      game_desc = dplyr::case_when(\n        week == 18 ~ \"Wild Card\",\n        week == 19 ~ \"Divisional Round\",\n        week == 20 ~ \"Conference Championship\",\n        week == 21 ~ \"Super Bowl\",\n        TRUE ~ \"Regular Season\"\n      )\n    ) %>%\n    dplyr::left_join(\n      nflfastR::teams_colors_logos %>% select(team_abbr, opp_color = team_color, team_logo_espn),\n      by = c(\"opp\" = \"team_abbr\")\n    )\n\n  # Combine the QBR data of the chosen QB with the game data\n  chart <- single_qbr %>%\n    dplyr::left_join(opponents, by = c(\"season\", \"game_week\" = \"week\")) %>%\n    dplyr::mutate(\n\n      # create string for game outcome\n      outcome = dplyr::case_when(\n        team == home_team & home_result > 0 ~ \"Won\",\n        team == home_team & home_result < 0 ~ \"Lost\",\n        team == away_team & home_result > 0 ~ \"Lost\",\n        team == away_team & home_result < 0 ~ \"Won\",\n        home_result == 0 ~ \"Tie\",\n        TRUE ~ NA_character_\n      )\n    )\n\n  # Set title string for later usage\n  if (max(chart$game_week) > 17) {\n    title_string <- glue::glue(\"{QB} Weekly Total QBR from {first_year} to {last_year} including Playoffs\")\n  } else {\n    title_string <- glue::glue(\"{QB} Weekly Total QBR from {first_year} to {last_year} Regular Season\")\n  }\n\n  # going to draw some quantile lines and combine them here\n  quantiles <- c(\n    quantile(all_qbr_file$qbr_total, 0.10),\n    quantile(all_qbr_file$qbr_total, 0.25),\n    quantile(all_qbr_file$qbr_total, 0.75),\n    quantile(all_qbr_file$qbr_total, 0.90),\n    quantile(all_qbr_file$qbr_total, 0.98)\n  )\n\n  # Adding the logos to the interactive plots needs to be done in a different way\n  # Here we compute the list needed to add the later\n  image_list <- chart %>%\n    dplyr::transmute(\n      source = team_logo_espn,\n      xref = \"x\",\n      yref = \"y\",\n      x = gm,\n      y = qbr_total,\n      sizex = 7,\n      sizey = 7,\n      opacity = 0.9,\n      xanchor = \"center\",\n      yanchor = \"middle\"\n    ) %>%\n    purrr::transpose()\n\n  plot <-\n    chart %>%\n    ggplot(aes(x = gm, y = qbr_total)) +\n    geom_hline(yintercept = quantiles, color = \"black\", linetype = \"dashed\", alpha = 0.7) +\n    geom_hline(yintercept = quantile(all_qbr_file$qbr_total, 0.50), color = \"black\", linetype = \"solid\", alpha = 0.7) +\n    geom_vline(xintercept = chart$gm[chart$game_week == 1] - 0.5, color = \"black\", linetype = \"dotted\", alpha = 0.7) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.10), label = \"10th Percentile\", size = 3.5) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.25), label = \"25th Percentile\", size = 3.5) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.50), label = \"50th Percentile\", size = 3.5) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.75), label = \"75th Percentile\", size = 3.5) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.90), label = \"90th Percentile\", size = 3.5) +\n    geom_text(x = -0.5, y = 2 + quantile(all_qbr_file$qbr_total, 0.98), label = \"98th Percentile\", size = 3.5) +\n    geom_line(color = chart$team_color) +\n    geom_point(\n      # The text aesthetics in here are used for the tooltip text in the plotly object\n      aes(text = glue::glue(\n        \"Season: {season}\\nWeek: {game_week} ({game_desc})\\n{outcome} {away_score}:{home_score} {loc_desc}\\nTotal QBR: {qbr_total}\\nNumber of QB plays: {qb_plays}\"\n      )),\n      color = chart$opp_color,\n      size = 0.05\n    ) +\n    scale_x_continuous(\n      limits = c(-1.4, NA),\n      breaks = scales::breaks_pretty()\n    ) +\n    # Subtitle and caption labs don't work in the plotly object so I have removed them\n    # and added them manually to the object using plotly::layout()\n    labs(\n      x = glue::glue(\"Game Number in Given Era ({first_year}-{last_year})\"),\n      y = \"ESPN Total QBR\"\n    ) +\n    theme_bw() +\n    NULL\n\n  # The interactive part is done with plotly\n  plotly::ggplotly(plot, tooltip = \"text\") %>%\n    plotly::layout(\n      title = list(text = glue::glue(\"{title_string}<br><sup>Percentiles of the whole NFL 2006 - 2019 are drawn for orientation.<\/sup>\")),\n      margin = list(t = 50),\n      images = image_list\n    ) %>%\n    plotly::rangeslider(start = -3, end = min(30.5, max(chart$gm) + 0.5), bgcolor = \"#D3D3D3\", thickness = 0.08)\n}\n\n\n\nNow we are ready to call the function for specific Quarterbacks and seasons. I don’t want to do overkill so I am just doing three seasons for two Quarterbacks from the 2017 Draft.\n\n\nlibrary(plotly)\ninteractive_weekly_qbr(2017, 2019, \"Patrick Mahomes\", all_qbr)\n\n\n\n\n\n\ninteractive_weekly_qbr(2017, 2019, \"Mitchell Trubisky\", all_qbr)\n\n\n\n\n\nView source code on GitHub \n\n\nThere are multiple possible packages usable for this website. I suggest starting here.↩︎\n",
    "preview": "posts/2020-08-24-visualizing-epsns-total-qbr-using-interactive-plots/visualizing-epsns-total-qbr-using-interactive-plots_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3000
  },
  {
    "path": "posts/2020-08-23-exploring-wins-with-nflfastr/",
    "title": "Exploring Wins with nflfastR",
    "description": "Looking at what metrics are important for predicting wins. Creating expected season win totals and comparing to reality.",
    "author": [
      {
        "name": "Austin Ryan",
        "url": "https://twitter.com/packeRanalytics"
      }
    ],
    "date": "2020-08-23",
    "categories": [
      "Tidymodels",
      "Figures",
      "nflfastR"
    ],
    "contents": "\nTable of Contents\nSimple Linear Regression\nRandom Forest Variable Importance\nMultiple Linear Regression\nHow did expected and actual wins look in 2019?\nWhat does this mean for the 2020 season?\nOther Findings\nWhat can two decades worth of play-by-play data and some math tell us about what wins games in the NFL? Let’s look at some simple linear regressions using metrics we can easily compute with nflfastR data.\nPlease note code chunks have been intentionally hidden in this post for readability. See the rmd file at https://github.com/mrcaseb/open-source-football/ if you would like to see the underlying code.\nSimple Linear Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see passing efficiency metrics have the strongest relationships with wins. Furthermore, offensive passing efficiency metrics have stronger relationships than defensive passing metrics do.\nA team’s expected points added per dropback explains nearly half of the variation in their season win total. Whereas defensive expected points added per dropback explains about 32% of the variation in wins. Offensive and defensive rushing efficiency metrics only explain about 18 and 9% of the variation in wins respectively.\n\n\n\n\n\n\nRandom Forest Variable Importance\nWe can also build a random forest model and let the model tell us what features yield the most information gain. Again, passing efficiency is the largest driver of wins and it is not particularly close.\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression\nWe know offensive and defensive EPA per dropback metrics are useful for explaining season win totals. Just for fun make a linear regression model that uses EPA per dropback and per rush for both sides of the ball. This regression explains 78% of the variation in season wins.\nWe can use the regression formula to develop expected wins based on EPA per play metrics. The distribution of actual wins minus expected wins is normally distributed with a mean of 0 and a standard deviation of 1.4 wins.\nThis means 68% of the season win totals from 2009-2019 are plus or minus 1.4 wins from what our expected wins formula predicts. Furthermore, 95% of the season win totals are within 3 games of what we would predict. Put another way, it is rare for a team to out or underperform their expected wins by more than 3 games.\n\n\n\n\n\n\n\n\n\nHow did expected and actual wins look in 2019?\n\n\n\n\n\n\n\n\n\nBased on our expected wins formula the NFC North champs were predicted to have 10 wins while they actual won 13. Additionally, the team they beat to get to the NFC Championship looked more like an 8 win team rather than an 11 win team. On the other end of the spectrum the Cowboys produced EPA per play metrics that predicted an 11 win team, however, they ended up 3 wins short.\n\n\n\n\n\n\n\n\n\nWhat does this mean for the 2020 season?\nLooking at the 25 teams in the right tail (those who over performed by more than 2.5 wins) from 1999 to 2018 we find that on average their wins dropped by 2.3 games in the next season. Not great news for Packers or Seahawks fans in 2020.\nThe 29 teams n the left tail we see that teams who under performed by more than 2.5 wins increased their wins by 2.7 games the next season. The 2019 Cowboys, Chargers, and Buccaneers also fall into this tail.\n\n\n\n\n\n\nIf we look at teams who over performed by more than 2 games (56 from 1999 to 2018) we see their wins drop on average by 2.6 games the next season. Conversely, teams who under perform by more than 2 games (50 from 1999 to 2018) increase their wins the next season by 2.6 games on average.\nOther Findings\nThe difference between actual and expected wins is largely a function of how a team performs in one score games and on special teams performance. Record in one score games isn’t very stable year over year for the most part, however, a few teams did consistently out or over perform their expected wins.\nOf the 669 season long performances in the data only 38 teams under performed by more than 2.35 wins. The Chargers account for over a fifth of those seasons.\n\n\n\n\n\n\nThe Browns have not over performed since 2009 when they won 5 games but this model saw them as more of a 2 win team.\n\n\n\nOn the other end of the spectrum the Patriots have only under performed by more than half a game two times.\n\n\n\n\n\n",
    "preview": "posts/2020-08-23-exploring-wins-with-nflfastr/exploring-wins-with-nflfastr_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-08-22-ranking-qbs-using-era-adjusted-elo/",
    "title": "Ranking QBs Using Era Adjusted Elo",
    "description": "Use 538's QB Elo value, a highly predictive measurement of QB impact, to compare QB careers across era",
    "author": [
      {
        "name": "Robby Greer",
        "url": "https://twitter.com/greerreNFL"
      }
    ],
    "date": "2020-08-22",
    "categories": [
      "Elo",
      "python"
    ],
    "contents": "\nTable of Contents\nPart 0: Background and summary\nPart 1: Importing and cleaning data\nPart 2: Adjusting for era\nPart 3: Adding stats and compiling careers\nPart 4: Graphing careers\nPart 0: Background and summary\nElo is a ranking and prediction framework that 538 has successfully applied to the NFL. Because QB performance plays such a strong role in overall team performance, 538’s Elo framework models QB contributions separately before adding them back to the overall team grade.\nThese QB rankings significantly improve the overall predictive power of the framework, making them a fairly accurate measure of a QB’s value. Every 25 points of Elo are equivalent to roughly 1 point of expected game margin. For instance, a QB with an Elo of 100 would be worth roughly 4 points more per game than a replacement level QB.\nMeasuring the cumulative Elo added by a QB over the course of their career is akin to measuring the total points added above a replacement level player. In this post, QB Elo values are pulled from 538 and normalized by era, allowing for an interesting comparison of QB careers throughout the history of the NFL. As QB rankings can be a touchy subject, it is worth noting that these rankings are just one quantitative view of a QBs overall performance.\nPart 1: Importing and cleaning data\nFirst, import packages:\n\n\nimport pandas as pd\nimport numpy\nimport requests\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nNext, pull Elo data from 538 and load it into a pandas data frame:\n\n\ndata_link = 'https://projects.fivethirtyeight.com/nfl-api/nfl_elo.csv'\ndata_df = pd.read_csv(data_link)\n\nWe only want data with QB grades, and we’ll exclude playoffs:\n\n\ndata_df = data_df[(~numpy.isnan(data_df['qbelo1_pre'])) & (~numpy.isnan(data_df['qbelo2_pre']))]\ndata_df = data_df[data_df['playoff'].isna()]\n\nElo game data comes with game dates, not weeks:\n\n\ndata_df['date'].sample(5)\n\n12007    2003-09-07\n12393    2004-11-07\n9108     1991-10-20\n4180     1968-09-15\n3363     1963-09-15\nName: date, dtype: object\n\nWhich we’ll want to convert to weeks to make them easier to group:\n\n\n## create a datetime series from the date ##\ndata_df['date_time'] = pd.to_datetime(data_df['date'])\n\n## mondays are new weeks, so subtract a day and then trunc to get an NFL week ##\ndata_df['date_time'] = data_df['date_time'] - pd.Timedelta(days=1)\ndata_df['week_of'] = data_df['date_time'].dt.week\n\nNext, separate Home and Away data and merge it into a single flat file, filtering out unnecessary fields and renaming columns in the process:\n\n\n## create a flat file ##\nhome_data_df = data_df.copy()[[\n    'season',\n    'week_of',\n    'qb1',\n    'qb1_value_post',\n    'score1',\n    'score2'\n]].rename(columns={\n    'qb1' : 'qb_name',\n    'qb1_value_post' : 'qb_elo_value',\n    'score1' : 'points_for',\n    'score2' : 'points_against',\n})\n\naway_data_df = data_df.copy()[[\n    'season',\n    'week_of',\n    'qb2',\n    'qb2_value_post',\n    'score2',\n    'score1'\n]].rename(columns={\n    'qb2' : 'qb_name',\n    'qb2_value_post' : 'qb_elo_value',\n    'score2' : 'points_for',\n    'score1' : 'points_against',\n})\n\nflat_df = pd.concat([home_data_df,away_data_df])\nflat_df = flat_df.sort_values(by=['season','week_of'])\n\nThis yields a data frame with individual QB games:\n\n\nflat_df.sample(5)\n\n       season  week_of  ... points_for  points_against\n11821    2002       41  ...         10              17\n2500     1954       44  ...         30               6\n5913     1977       38  ...         30              27\n12046    2003       38  ...         12              13\n5502     1975       38  ...         14              42\n\n[5 rows x 6 columns]\n\nNote that the above data are Elo values. To convert Elo values to Elo, you’d need to multiply by 3.3 per 538’s methodology. Add some addition stats:\n\n\nflat_df['point_margin'] = flat_df['points_for'] - flat_df['points_against']\nflat_df['win'] = numpy.where(flat_df['point_margin'] > 0,1,0)\n\nPart 2: Adjusting for era\n538’s QB ratings are based on stats that have increased overtime alongside improved QB play. This can be seen by looking at the median QB Elo value overtime:\n\n\n## calculate median QB values by season week ##\nmedian_df = flat_df.groupby(['season','week_of']).agg(\n    qb_elo_value_median = ('qb_elo_value', 'median'),\n    qb_elo_value_min = ('qb_elo_value', 'min'),\n    qb_elo_value_max = ('qb_elo_value', 'max')\n).reset_index()\n\n## plot ##\nmedian_line = median_df['qb_elo_value_median'].plot.line()\nplt.show()\n\n\nTo make 538’s Elo values comparable across era, this stat inflation needs to be removed:\n\n\n## add weekly median to flat file ##\nflat_df = pd.merge(\n    flat_df,\n    median_df,\n    on=['season','week_of'],\n    how='left'\n)\n\n## calculate an adjusted stat that removes the median\nflat_df['qb_elo_value_era_adjusted'] = flat_df['qb_elo_value'] - flat_df['qb_elo_value_median']\n\nPart 3: Adding stats and compiling careers\nTo compare quarterbacks, we’ll need to aggregate all of their QB values, but first, some additional stats can be added to make the ultimate comparisons more interesting. Namely, Elo ranking relative to other starters at a point in time, total starts, and win percentages:\n\n\n## add weekly ranking ##\nflat_df['qb_rank'] = flat_df.groupby(['season','week_of'])['qb_elo_value'].rank(method='max', ascending=False)\nflat_df['top_1_qb'] = numpy.where(flat_df['qb_rank']<=1, 1,0)\nflat_df['top_3_qb'] = numpy.where(flat_df['qb_rank']<=3, 1,0)\nflat_df['top_5_qb'] = numpy.where(flat_df['qb_rank']<=5, 1,0)\n\n## add cumulative count ##\nflat_df['game_number'] = flat_df.groupby(['qb_name']).cumcount() + 1\nflat_df['cumulative_era_adjusted_value'] = flat_df.groupby('qb_name')['qb_elo_value_era_adjusted'].transform(pd.Series.cumsum)\nflat_df['cumulative_wins'] = flat_df.groupby('qb_name')['win'].transform(pd.Series.cumsum)\nflat_df['cumulative_best_starter'] = flat_df.groupby('qb_name')['top_1_qb'].transform(pd.Series.cumsum)\nflat_df['cumulative_top_3_starts'] = flat_df.groupby('qb_name')['top_3_qb'].transform(pd.Series.cumsum)\nflat_df['cumulative_top_5_starts'] = flat_df.groupby('qb_name')['top_5_qb'].transform(pd.Series.cumsum)\n\nAfter adding stats, compile at the QB level to get a look at their career:\n\n\n## aggregate ##\nagg_df = flat_df.groupby('qb_name').agg(\n    total_starts = ('game_number', 'max'),\n    cumulative_era_adjusted_elo_value = ('qb_elo_value_era_adjusted', 'sum'),\n    winning_percentage = ('win', 'mean'),\n    pct_of_starts_as_best_qb = ('top_1_qb', 'mean'),\n    pct_of_starts_as_top3_qb = ('top_3_qb', 'mean'),\n    pct_of_starts_as_top5_qb = ('top_5_qb', 'mean')\n).reset_index()\n\nagg_df['average_era_adjusted_elo_value'] = agg_df['cumulative_era_adjusted_elo_value'] / agg_df['total_starts']\n\nSort by total Elo value to see the era adjusted rankings:\n\n\n## sort ##\nagg_df = agg_df.sort_values(by=['cumulative_era_adjusted_elo_value'],ascending=[False])[[\n    'qb_name',\n    'total_starts',\n    'cumulative_era_adjusted_elo_value',\n    'average_era_adjusted_elo_value',\n    'winning_percentage',\n    'pct_of_starts_as_best_qb',\n    'pct_of_starts_as_top3_qb',\n    'pct_of_starts_as_top5_qb'\n]]\n\nagg_df[['qb_name','total_starts','cumulative_era_adjusted_elo_value']].head(15)\n\n            qb_name  total_starts  cumulative_era_adjusted_elo_value\n493  Peyton Manning           265                       26065.054253\n204      Drew Brees           274                       23763.826759\n616       Tom Brady           283                       19225.832143\n146      Dan Marino           240                       17160.155283\n226  Fran Tarkenton           239                       14498.753253\n338     Joe Montana           164                       14043.230167\n73      Brett Favre           298                       12764.784141\n588     Steve Young           143                       12689.178178\n4     Aaron Rodgers           174                       12132.510866\n364   Johnny Unitas           185                        9796.247620\n144       Dan Fouts           171                        9767.284528\n441       Matt Ryan           189                        8398.344958\n380    Ken Anderson           172                        7866.688036\n495   Philip Rivers           224                        7385.531378\n347      John Elway           231                        7250.838144\n\nPart 4: Graphing careers\nThough simple, era adjusted QB Elo appears to provide a fairly good ranking of QBs across era. One interesting way to leverage this measure further is by comparing cumulative QB Elo gained over the course of a QB’s career.\nCreate a function for graphing career Elo based on a list of QBs:\n\n\ndef create_qb_chart(qbs_to_plot):\n    ## plot career cumulative Elo value based on a list of QBs ##\n    ## make a copy of the flat file ##\n    chart_df = flat_df.copy()\n    ## filter to just relevant fields ##\n    chart_df = chart_df[[\n        'qb_name',\n        'game_number',\n        'cumulative_era_adjusted_value'\n    ]]\n    ## create sub selection of QBs\n    chart_df = chart_df[numpy.isin(\n        chart_df['qb_name'],\n        qbs_to_plot\n    )]\n    ## set up plot ##\n    sns.lineplot(\n        'game_number',\n        'cumulative_era_adjusted_value',\n        hue='qb_name',\n        ci=None,\n        palette='RdPu',\n        data=chart_df\n    )\n    sns.despine()\n    ## set axis titles and sizes ##\n    plt.xlabel('Games Played', labelpad=10, fontsize='small', weight='bold')\n    plt.ylabel('Cumulative Elo Value Added', labelpad=10, fontsize='small', weight='bold')\n    plt.rc('xtick',labelsize='x-small')\n    plt.rc('ytick',labelsize='x-small')\n    ## define plot ranges, leaving a little room for padding ##\n    xmin = 0\n    xmax = chart_df['game_number'].max() * 1.2\n    ymin = chart_df['cumulative_era_adjusted_value'].min() * 1.15\n    ymax = chart_df['cumulative_era_adjusted_value'].max() * 1.15\n    plt.xlim(xmin,xmax)\n    plt.ylim(ymin,ymax)\n    ## add darker axis ##\n    plt.axhline(y = ymin, color = 'black', linewidth = 1.75)\n    plt.axvline(x = xmin, color = 'black', linewidth = 1.75)\n    ## and a line at zero\n    plt.axhline(y = 0, color = 'black', linewidth = 0.75)\n    ## add labels at the end of each line ##\n    for i in qbs_to_plot:\n        plt.text(\n            x = chart_df[chart_df['qb_name'] == i]['game_number'].iloc[-1] + 1,\n            y = chart_df[chart_df['qb_name'] == i]['cumulative_era_adjusted_value'].iloc[-1] + 5,\n            s = i,\n            weight = 'bold',\n            fontsize = 'small',\n            backgroundcolor = '#ffffff'\n        )\n    ## remove legend ##\n    plt.legend([],[], frameon=False)\n    plt.tight_layout()\n\nMake your comparisons…\nManning, Brady, and Brees:\n\n\nqb_list = ['Tom Brady', 'Peyton Manning','Drew Brees']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\nRomo > Dak > Aikman?:\n\n\nqb_list = ['Tony Romo', 'Troy Aikman', 'Dak Prescott']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\nJaMarcus Russell, it could have been worse:\n\n\nqb_list = ['JaMarcus Russell', 'Johnny Manziel', 'Ryan Leaf']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\nMaybe Leaf just needed more time and a better defense:\n\n\nqb_list = ['Ryan Leaf', 'Trent Dilfer']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\nMahomes, off to one of the best starts ever:\n\n\nqb_list = ['Patrick Mahomes', 'Aaron Rodgers', 'Dan Marino']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\nJosh Allen, not so much …\n\n\nqb_list = ['Josh Allen', 'Sam Darnold', 'Mitchell Trubisky', 'Baker Mayfield']\ncreate_qb_chart(qb_list)\nplt.show()\n\n\n\n\n",
    "preview": "posts/2020-08-22-ranking-qbs-using-era-adjusted-elo/ranking-qbs-using-era-adjusted-elo_files/figure-html5/all_time_greats-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2700
  },
  {
    "path": "posts/2020-08-21-game-excitement-and-win-probability-in-the-nfl/",
    "title": "Game Excitement and Win Probability in the NFL",
    "description": "Game excitement calculation and a win probability figure.",
    "author": [
      {
        "name": "Max Bolger",
        "url": "https://twitter.com/mnpykings"
      }
    ],
    "date": "2020-08-21",
    "categories": [
      "nflfastR",
      "python"
    ],
    "contents": "\nTable of Contents\nPart 1: Importing and Preprocessing\nPart 2: Game Excitement Index\nPart 3: Win Probability Chart\nPart 1: Importing and Preprocessing\nFirst we need to import our dependencies. These pacakges are what make this analysis possible.\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nNext we will read in our data from the nflfastR data repo.\n\n\n# Read in data\nYEAR = 2019\n\ndata = pd.read_csv('https://github.com/guga31bb/nflfastR-data/blob/master/data/' \\\n                         'play_by_play_' + str(YEAR) + '.csv.gz?raw=True',\n                         compression='gzip', low_memory=False)\n\nPerfect! Our data and notebook are set up and ready to go. The next step is to filter our df to include only the game we would like to work with. We will subset by game_id (which we will need later). The new nflfastR game ids are very convenient and use the following format:\nYEAR_WEEK_AWAY_HOME\nNote, the year needs to be in YYYY format and single digit weeks must lead with a 0.\n\n\n#Subset the game of interest\ngame_df = data[\n             (data.game_id== '2019_09_MIN_KC')\n             ]\n\n#View a random sample of our df to ensure everything is correct          \ngame_df.sample(3)\n\n       play_id         game_id  ...  xyac_success   xyac_fd\n23013     1294  2019_09_MIN_KC  ...      1.000000  1.000000\n23080     3077  2019_09_MIN_KC  ...      0.140994  0.107368\n23077     2992  2019_09_MIN_KC  ...           NaN       NaN\n\n[3 rows x 340 columns]\n\nThe last step in preprocessing for this particular analysis is dropping null values to avoid jumps in our WP chart. To clean things up, we can filter the columns to show only those that are of importance to us.\n\n\ncols = ['home_wp','away_wp','game_seconds_remaining']\ngame_df = game_df[cols].dropna()\n\n#View new df to again ensure everything is correct\ngame_df\n\n        home_wp   away_wp  game_seconds_remaining\n22960  0.560850  0.439150                  3600.0\n22961  0.560850  0.439150                  3600.0\n22962  0.599848  0.400152                  3596.0\n22963  0.612526  0.387474                  3590.0\n22964  0.629503  0.370497                  3584.0\n...         ...       ...                     ...\n23132  0.697633  0.302367                    59.0\n23134  0.806030  0.193970                    24.0\n23135  0.910061  0.089939                     4.0\n23136  0.927525  0.072475                     3.0\n23137  1.000000  0.000000                     0.0\n\n[166 rows x 3 columns]\n\nEverything looks good to go! Before we use this data to create the WP chart, we are going to calculate the game’s excitement index.\nPart 2: Game Excitement Index\nWe are using Luke Benz’ formula for GEI which can be found here. It’s simple yet effective which is why I like it so much. As Luke notes, “the formula sums the absolute value of the win probability change from each play”. Here, we are creating a function (inspired by ChiefsAnalytics) that follows his formula. This function requires a single parameter game_id. The new version of nflfastR’s game id must be used here.\n\n\n#Calculate average length of 2019 games for use in our function\navg_length = data.groupby(by=['game_id'])['epa'].count().mean()\n\ndef calc_gei(game_id):\n  game = data[(data['game_id']==game_id)]\n  #Length of game\n  length = len(game)\n  #Adjusting for game length\n  normalize = avg_length / length\n  #Get win probability differences for each play\n  win_prob_change = game['home_wp'].diff().abs()\n  #Normalization\n  gei = normalize * win_prob_change.sum()\n  return gei\n\nLet’s run the function by passing in our game id from earlier.\n\n\nprint(f\"Vikings @ Chiefs GEI: {calc_gei('2019_09_MIN_KC')}\")\n\nVikings @ Chiefs GEI: 4.652632439280925\n\nThis seemed to be a pretty exciting game. Let’s compare it to other notable games from last season.\n\n\n# Week 1 blowout between the Ravens and Dolphins\nprint(f\"Ravens @ Dolphins GEI: {calc_gei('2019_01_BAL_MIA')}\")\n\n# Week 14 thriller between the 49ers and Saints\n\nRavens @ Dolphins GEI: 0.9723172478637379\n\nprint(f\"49ers @ Saints GEI: {calc_gei('2019_14_SF_NO')}\")\n\n49ers @ Saints GEI: 5.190375267367869\n\nYep, the Vikings vs Chiefs game was definitely one of the more exciting regular season games of last season. Let’s see how it looks visually with a WP chart!\nPart 3: Win Probability Chart\nMatplotlib and Seaborn can be used together to create some beautiful plots. Before we start, below is a useful line of code that prints out all usable matplotlib styles. You can also see how each of them look by checking out the documentation.\n\n\n#Print all matplotlib styles\nprint(plt.style.available)\n\n['Solarize_Light2', '_classic_test_patch', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n\nSince we already have all of our data set up from Step 1, we can jump straight to the plot!\n\n\n#Set style\nplt.style.use('dark_background')\n\n#Create a figure\nfig, ax = plt.subplots(figsize=(16,8))\n\n#Generate lineplots\nsns.lineplot('game_seconds_remaining', 'away_wp', \n             data=game_df, color='#4F2683',linewidth=2)\n\nsns.lineplot('game_seconds_remaining', 'home_wp', \n             data=game_df, color='#E31837',linewidth=2)\n\n#Generate fills for the favored team at any given time\n\n<AxesSubplot:xlabel='game_seconds_remaining', ylabel='home_wp'>\n\nax.fill_between(game_df['game_seconds_remaining'], 0.5, game_df['away_wp'], \n                where=game_df['away_wp']>.5, color = '#4F2683',alpha=0.3)\n\nax.fill_between(game_df['game_seconds_remaining'], 0.5, game_df['home_wp'], \n                where=game_df['home_wp']>.5, color = '#E31837',alpha=0.3)\n\n#Labels\nplt.ylabel('Win Probability %', fontsize=16)\nplt.xlabel('', fontsize=16)\n\n#Divider lines for aesthetics\nplt.axvline(x=900, color='white', alpha=0.7)\nplt.axvline(x=1800, color='white', alpha=0.7)\nplt.axvline(x=2700, color='white', alpha=0.7)\nplt.axhline(y=.50, color='white', alpha=0.7)\n\n#Format and rename xticks\nax.set_xticks(np.arange(0, 3601,900))\n\n[<matplotlib.axis.XTick object at 0x000000002F30CF60>, <matplotlib.axis.XTick object at 0x000000002F30CB00>, <matplotlib.axis.XTick object at 0x000000002F33FD30>, <matplotlib.axis.XTick object at 0x000000002F3D0438>, <matplotlib.axis.XTick object at 0x000000002F3D08D0>]\n\nplt.gca().invert_xaxis()\nx_ticks_labels = ['End','End Q3','Half','End Q1','Kickoff']\nax.set_xticklabels(x_ticks_labels, fontsize=12)\n\n#Titles\n\n[Text(0, 0, 'End'), Text(900, 0, 'End Q3'), Text(1800, 0, 'Half'), Text(2700, 0, 'End Q1'), Text(3600, 0, 'Kickoff')]\n\nplt.suptitle('Minnesota Vikings @ Kansas City Chiefs', \n             fontsize=20, style='italic',weight='bold')\n\nplt.title('KC 26, MIN 23 - Week 9 ', fontsize=16, \n          style='italic', weight='semibold')\n\n#Creating a textbox with GEI score\nprops = dict(boxstyle='round', facecolor='black', alpha=0.6)\nplt.figtext(.133,.85,'Game Excitement Index (GEI): 4.65',style='italic',bbox=props)\n\n#Citations\nplt.figtext(0.131,0.137,'Graph: @mnpykings | Data: @nflfastR')\n\n#Save figure if you wish\n#plt.savefig('winprobchart.png', dpi=300)\n\n\nWow, this game had a ton of WP changes. No wonder it had a high GEI!\nThings to be aware of:\nSometimes the plot generates small gaps in the fill. This only occurs when the previous data point is on the opposite side of the 50% threshold compared to the current data point or vice versa (this happens twice to the Chiefs’ WP line towards the end of the game). The .fill_between() function only checks to fill at each new data point and not inbetween. This is very minor and the dark background makes it hardly noticeable, but I wanted to address it to make sure nobody gets confused if this happens to them.\nThe nflfastR win probability model is a little wonky in OT due to it not accounting for ties as Sebastian mentions here. Be mindful of this when calculating GEI or creating WP charts with OT games.\nThat concludes this tutorial. Thanks for reading, I hope you learned some python in the process! Big thanks to Sebastian Carl and Ben Baldwin for everything they do; I’m looking forward to watching this platform grow! The future of sports analytics has never looked brighter.\n\n\n",
    "preview": "posts/2020-08-21-game-excitement-and-win-probability-in-the-nfl/game-excitement-and-win-probability-in-the-nfl_files/figure-html5/plot-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 9600,
    "preview_height": 4800
  },
  {
    "path": "posts/2020-08-22-nfl-pass-location-visualization/",
    "title": "NFL Pass Location Visualization",
    "description": "Methods for visualizing NFL passing location data.",
    "author": [
      {
        "name": "Ethan Douglas",
        "url": "https://twitter.com/ChiefsAnalytics"
      }
    ],
    "date": "2020-08-21",
    "categories": [
      "nflfastR",
      "python"
    ],
    "contents": "\nTable of Contents\nVisualizing NFL Pass Location\nSet-up\nData Exploration\n\nVisualizing NFL Pass Location\nSo this is a post all about how my life got flipped-turned upside down…\nKidding, but kind of. About a year ago I became very interested in NFL analytics, and managed to stumble across the incredible work of Sarah Mallepalle, Ron Yurko, Konstantinos Pelechrinis, and Sam Ventura. Their paper (https://arxiv.org/abs/1906.03339) unveiled the creation of a scraper which could take static Next Gen Stats passing charts and translate them into x and y coordinate data. This tool was as impressive as it was interesting, and so it should be no surprise that Sarah (the lead author on the paper) was scooped up to work as an analyst for the Baltimore Ravens, terminating the public code for this work. Thankfully, Ron Yurko was happy to revive her github when I reached out. I have had a ton of fun exploring this dataset, and even adapted the code a bit to scrape routes and carries as well.\nThat said, sometimes location data can be a bit overwhelming so I thought I’d do a quick walkthrough of a few things you can do with it. Consider this post Part One, where I’ll walk through some simple two-dimensional visualizations. In later posts I’ll do my best to go over the expected completion surfaces Sarah et al. debuted in the paper, and other ways to visualize the data.\nSet-up\nFirst we need to import the libraries we’ll be using\n\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nNext we’ll read in our data from the repo I maintain\n\n\ndf = pd.read_csv('https://raw.githubusercontent.com/ArrowheadAnalytics/next-gen-scrapy-2.0/master/pass_and_game_data.csv', low_memory=False)\n#There's an additional index row we don't need, so I am getting rid of it here\ndf = df.iloc[0:,1:]\n\nData Exploration\n\n\n#Drop the games that weren't able to be scraped\ndf.dropna(inplace=True)\n#Explore the data a bit\ndf.head(3)\n\n      game_id           name  ... home_score away_score\n0  2017091004  Carson Palmer  ...         35         23\n1  2017091004  Carson Palmer  ...         35         23\n2  2017091004  Carson Palmer  ...         35         23\n\n[3 rows x 14 columns]\n\n\n\n#Get a summary of the numerical data in our dataframe\ndf.describe()\n\n            game_id       x_coord  ...    home_score    away_score\ncount  4.383900e+04  43839.000000  ...  43839.000000  43839.000000\nmean   2.018206e+09      0.401437  ...     23.654600     22.306827\nstd    8.202842e+05     15.229154  ...     10.318057     10.243724\nmin    2.017091e+09    -28.300000  ...      0.000000      0.000000\n25%    2.017122e+09    -12.300000  ...     17.000000     16.000000\n50%    2.018112e+09      0.900000  ...     24.000000     23.000000\n75%    2.019101e+09     12.900000  ...     31.000000     28.000000\nmax    2.019123e+09     28.300000  ...     57.000000     59.000000\n\n[8 rows x 6 columns]\n\nLooks like we’ve got almost 43k passes, from 2017-2019, ranging from -28.3 to 28.3 in the x coordinate and -10.4 to 62.1 in the y coordinate. We know a field is only 53 and 1/3 yards wide, so tthere are some passes being identified that are technically out of bounds.\nOne common thing to do with new data is to explore the distributions of the numerical variables we are interested in. This is typically done with histograms.\n\n\n#Let's visualize the distribution of the coordinates\nfig = sns.distplot(df.x_coord, kde=False)\nplt.show()\n\n\nNothing fancy here but we can already see some trends in where players pass the ball. Let’s look at the y direction\n\n\n#Now the x coordinate\nfig = sns.distplot(df.y_coord, kde=False)\nplt.show()\n\n\nSo we’ve plotted both the horizontal (x) and vertical (y) field components, but nothing too interesting here still. What if we wanted to visualize these distributions as 2-dimensional?\nWe have a few different ways to go about this. One, a simple jointplot.\n\n\nfig = sns.jointplot(x='x_coord', y='y_coord', data=df)\nplt.show()\n\n\nThis is neat! We’ve now visualized the data by both the x and y coordinate, and we can see the distributions for both on the side of the graph\nBut, with just one small tweak we can make this even more clear.\n\n\nfig = sns.jointplot(x='x_coord', y='y_coord', data=df, kind='hex', color='red')\nplt.show()\n\n\nWhat we’ve done here is bin each of the passes into a hexagons by where they were thrown on the field. The darker hexagons here represent areas of the field where a higher density of passes were targeted.\nIf this kind of discrete binning isn’t your thing, how about a smoother 2-dimensional kernel density estimate? Again, with just a very small tweak to the above code we can plot a different way of visualizing the location of these passes\n\n\nfig = sns.jointplot(x='x_coord', y='y_coord', data=df, kind='kde', cmap='Reds', color='red')\nplt.show()\n\n\nNow, what if we want to directly compare two distributions? Looking at the entire dataset has been interesting, but how does someone like Derek Carr differ in his target locations compared to someone like Patrick Mahomes?\n\n\n#prepare our two plots - no reason to separate out our axes here as we want to compare these two players on an equal scale so we set sharex and sharey to true\nfig, ax =plt.subplots(1,2, sharex=True, sharey=True)\n\n#First, we'll partition the data to the player we are looking for\nqb_name = 'Mahomes' #I'm being intentional vague here and not listing the full player name, because many player names listed on Next Gen Stats are different than you're likely accustomed to. So, I've found this method easire to ensure I get the player data\nqb = df.loc[(df['name'].str.contains(qb_name))]\n#Assign this plot to the first (0-index) axis. Not going to use the univariate distributions on the axes for these plots as they don't work well as subplots\nsns.kdeplot(qb.x_coord, qb.y_coord, ax=ax[0], cmap='Reds')\n\n#Let's get player 2\nqb_name = 'Carr'\nqb = df.loc[(df['name'].str.contains(qb_name))]\n#Second plot\nsns.kdeplot(qb.x_coord, qb.y_coord, ax=ax[1], cmap='Reds')\nplt.show()\n\n\nThis isn’ too bad! With just a few lines of code we can see that Carr’s passes are more dense around the line of scrimmage, while Mahomes spreads the ball around more.\nLet’s spice these plots up a bit\n\n\n#Layering two plotting styles here to get a big, clean, but dark background look\nplt.style.use('seaborn-poster')\nplt.style.use('dark_background')\n\n#Set up our subplots\nfig, (ax1, ax2) =plt.subplots(1,2)\n\n\nqb_name = 'Mahomes'\nqb = df.loc[(df['name'].str.contains(qb_name))]\n\n#What we've added here is shading for the densities, but leaving the lowest density area unshaded.\n#I've also added the *n_level* parameter, which allows us to choose how many levels we want to have in our contour. The higher the number here, the smoother the plot will look.\nsns.kdeplot(qb.x_coord, qb.y_coord, ax=ax1, cmap='gist_heat', shade=True, shade_lowest=False, n_levels=10)\n\n#Set title, remove ticks and labels\nax1.set_title(qb_name)\nax1.set_xlabel('')\nax1.set_xticks([])\n\nax1.set_yticks([])\n\nax1.set_ylabel('')\n\n#Remove any part of the plot that is out of bounds\nax1.set_xlim(-53.3333/2, 53.3333/2)\n\nax1.set_ylim(-15,60)\n#This makes our scales (x and y) equal (1 pixel in the x direction is the same 'distance' in coordinates as 1 pixel in the y direction)\n\n\n\n\n#Plot all of the field markings (line of scrimmage, hash marks, etc.)\n\nfor j in range(-15,60-1,1):\n    ax1.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax1.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax1.axhline(i,c='w',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax1.axhline(i,c='w',ls='-',alpha=1, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax1.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax1.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\n\n#Now we just repeat for player 2.\nqb_name = 'Carr'\nqb = df.loc[(df['name'].str.contains(qb_name))]\n#Second plot\nsns.kdeplot(qb.x_coord, qb.y_coord, ax=ax2, cmap='gist_heat', shade=True, shade_lowest=False, n_levels=10)\n\nax2.set_title(qb_name)\nax2.set_xlabel('')\nax2.set_ylabel('')\nax2.set_xticks([])\n\nax2.set_yticks([])\n\nax2.set_xlim(-53.3333/2, 53.3333/2)\n\nax2.set_ylim(-15,60)\n\n\nfor j in range(-15,60,1):\n    ax2.annotate('--', (-3.1,j-0.5),\n                 ha='center',fontsize =10)\n    ax2.annotate('--', (3.1,j-0.5),\n                 ha='center',fontsize =10)\n    \nfor i in range(-10,60,5):\n    ax2.axhline(i,c='w',ls='-',alpha=0.7, lw=1.5)\n    \nfor i in range(-10,60,10):\n    ax2.axhline(i,c='w',ls='-',alpha=1, lw=1.5)\n    \nfor i in range(10,60-1,10):\n    ax2.annotate(str(i), (-12.88,i-1.15),\n            ha='center',fontsize =15,\n                rotation=270)\n    \n    ax2.annotate(str(i), (12.88,i-0.65),\n            ha='center',fontsize =15,\n                rotation=90)\n\nplt.show()\n\n\nNow the nice thing about these seaborn plots is they do all the heavy lifting for us - but, they don’t let us see the underlying numbers. We can’t see at the coordinate (-20, 10) exactly what density of passes each player has.\nThat concludes this post. In my next post I’ll walk through performing a kernel density estimate ourselves, which lets us overlay QB densities and do some other fun things. Huge thanks to Sarah Mallepalle and her coauthors for publishing this awesome data and scraper, thanks to Ronald Yurko, Samuel Ventura, and Maksim Horowitz for the creation of the nflscrapR tool which spurred my and so many other’s interest in NFL analytics, and to Ben Baldwin and Sebastian Carl for continuing to advance the public work in this space.\n\n\n",
    "preview": "posts/2020-08-22-nfl-pass-location-visualization/nfl-pass-location-visualization_files/figure-html5/subplots2-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 7680,
    "preview_height": 5280
  },
  {
    "path": "posts/2020-08-22-rodgers-efficiency-decline/",
    "title": "Rodgers Efficiency Decline",
    "description": "A look into Rodgers Efficiency Decline. Also some functions for plotting EPA/CPOE moving averages.",
    "author": [
      {
        "name": "Austin Ryan",
        "url": "https://twitter.com/packeRanalytics"
      }
    ],
    "date": "2020-08-21",
    "categories": [
      "Figures",
      "nflfastR",
      "CPOE / EPA functions",
      "Packers"
    ],
    "contents": "\nTable of Contents\nEfficiency Metrics\nRodgers career as seen by EPA, CPOE, and QBR\nDigging Deeper\nSupporting Cast\n2008 - 2014\n2015-2016\n2017\n2018\n2019\n\nWhat’s next?\nFor the first time in the seven-year span of Mike Sando’s quarterback tiers survey Aaron Rodgers was not ranked as the NFL’s top quarterback. However, the fifty NFL coaches and evaluators who participated in the survey have not observed much of a drop off. In the 2020 survey Rodgers received a tier 1 vote from 46 respondents and finished 3rd overall in the ranking.\nFellow NFL players recently gave Rodgers his lowest ever ranking in the ten-year history of the annual NFL Top 100 poll following the 2019 season. Similar to the respondents of Sando’s survey, players still view Rodgers as a near top tier player. He earned the 16th overall ranking and was the 6th highest ranked quarterback in the 2020 poll.\nPFF grades tell a similar but not quite as rosy of a story. From 2008, Rodgers first season as a starter, to 2014 his PFF grade was never worse than 6th best in the league. In 2015 his PFF grade fell out of the top 10 but quickly bounced back to top 3 in 2016. After an injury shortened 2017 season PFF graded Rodgers as their 5th and 7th best in 2018 and 2019 respectively.\nIf people in the NFL still consider Rodgers a near top talent and PFF grade has him slightly outside the top 5 why are the efficiency stats so much more sour on him recently? Most of the advanced efficiency metrics rank the future Hall of Famer closer to league average than top 5. Expected Points Added (EPA), ESPN’s Quarterback Rating (QBR), Football Outsiders Defense-adjusted Value Over Average (DVOA), and Completion Percentage Over Expectation (CPOE) suggest Rodgers hasn’t been a tier 1 quarterback since 2016.\nThe Computer Cowboy’s EPA + CPOE composite metric on rbsdm.com ranked Rodgers as the 16th best since 2016, one spot below Ryan Fitzpatrick and three spots below Kirk Cousins.\n“Efficiency”Football Outsiders DVOA placed Rodgers outside the top 10 in each of the last three seasons. QBR lists Rodgers one spot below Andy Dalton in 2018 (16th) and one spot ahead of Jacoby Brissett in 2019 (20th).\nEfficiency Metrics\nIf you are not familiar with EPA, QBR, DVOA, or CPOE please check out the below.\nEPA\nEPA\nQBR\nDVOA\nnflfastR’s CPOE (Completion Percentage Over Expectation) forms an expectation of how likely a pass is to be completed based on factors we can observe like how far down the field the pass was thrown, whether the pass was thrown to the left, middle, or right, or whether or not it was thrown in a dome, and compares it to what the passer’s actual completion percentage.\nRodgers career as seen by EPA, CPOE, and QBR\n\n\n\n\n\n\nThe below graph is plotting the average expected points added per dropback from Rodgers most recent 300 dropbacks over the course of his career. When the white line and yellow fill are near .20 on the Y axis it means Rodgers last 300 dropbacks produced an EPA average similar to one that the 5th best in the game would have earned. The further above or below the .20 mark it goes it means he was that much better or worse than the 5th best.\n\n\n\nThis tells us Rodgers EPA was tier one level for over 71% of his dropbacks through 2014. We can see the highs were are really high as Rodgers doubled the EPA average of the 5th best at times. Remarkably, Rodgers was never outside top 5 play in his 2011 and 2014 MVP seasons. The lows were never very low either. Rodgers EPA average only fell below the .10 mark for less than 4% of his 4,600 plus dropbacks prior to 2015. The only similarly sustained levels of dominance I can see in the nflfastR library are a couple different stretches from Peyton Manning and the Patrick Mahomes Experience.\n\n\n\nAfter 2014 we see Rodgers rolling average collapse to an alarming low in 2015, skyrocket in 2016 during the “run-the-table” stretch, and then remain below top 5 play outside of a couple week stretch in 2019.\nCPOE mostly matches what we saw in the EPA per dropback plot. Rodgers CPOE was generally among the game’s elites through 2014 but has been rather mediocre other than the 2016 run since.\n\n\n\n\n\n\nESPN’s QBR is scaled to equate a 50 rating with average play and Pro Bowl-level play at 75. Again we see Rodgers hasn’t been viewed as one of the games best by QBR in recent seasons. His 2011 season ranks as the 3rd best QBR season of all time above 2019 Lamar Jackson and 2018 Patrick Mahomes. However, unlike respondents of Sando’s survey and the Top 100 poll, QBR downgraded Rodgers to marks well below that the past two seasons.\n\n\n\n\n\n\nWe are assigning EPA and CPOE to Rodgers in the above plots. However, a more apt description would be EPA or CPOE of the Packers passing offense in plays where Aaron Rodgers was the quarterback. Coaching, playcalling, and supporting cast are responsible for some portion of the output but we do know the quarterback plays a very large role.\nDigging Deeper\nWhat else can the publicly available data tell us about what part of Rodgers game is declining in the eyes of advanced efficiency metrics? One feature in the data we can use to look at EPA and CPOE at a more granular level is air yards. The air yards a pass gets assigned is the distance the ball traveled in the air past the line of scrimmage (not the distance traveled from the QBs release to the receiver).\n\n\n\nIf we bin air yards into into deep, intermediate, short, and behind the line of scrimmage passes we find CPOE has regressed at every level of the field in the past five years.\n\n\n\n\n\n\nThe 2008 to 2014 figures were absurdly good so some regression would be expected. However, over the last five years passes in the 10-19 and 20+ yard ranges are being completed at rates lower than we would expect while the 0-9 yard range is barely positive. In fact, below we see that Rodgers worst four seasons in the 0-9 yard range and 10-19 yard range (the two most frequently targeted regions) have come in the last five years.\n\n\n\nCompleting less passes than one would expect at every level obviously translates to lower EPA at every level of the field. Since 2015 deep passing EPA per dropback has decreased by 27% (.74 to .54), intermediate passing has decreased 25% (.61 to .45), and short passing efficiency has decreased a whopping 40% (.30 to .18).\n\n\n\nSupporting Cast\nSando’s article and his recent appearance on the Bill Barnwell Show suggest the NFL community hasn’t seen a significant deterioration of Rodgers skills and are assigning most of the statistical drop off to the supporting cast. It’s very hard to disentangle the strength of the scheme and the supporting cast from EPA and CPOE numbers. Nevertheless, let’s see what we can glean from the available data.\nThe below plot looks at EPA per target and CPOE by targeted receiver for passes that went past the line of scrimmage. Targets from 2008 to 2014 show as white dots and targets from 2015 to 2019 are in yellow. Players with fewer than 50 targets have been removed and the size of the dot corresponds to the number of targets the player received.\n\n\n\n\n\n\n\n\n\nWe see targeted receivers from ’08 to ’14 are largely clustered in the upper right meaning those targets were completed more frequently than we would expect and they were very efficient in terms of EPA. The only targeted receivers in that quadrant from ’15 to ’19 are Jordy Nelson, Davante Adams, and Allen Lazards 50 targets. The dispersion of yellow and white dots shouldn’t be a surprise given what we know about Rodgers numbers as a whole over those time frames. To give this some more context let’s think about the situation Rodgers came into in 2008.\n2008 - 2014\nRodgers spent the 2005 through 2007 seasons on the bench behind Favre learning Mike McCarthy’s offense. Donald Driver was entering his age 33 season coming of back-to-back Pro Bowl selections in 2006 and 2007. Late 2006 2nd round selection Greg Jennings was entering his 3rd season after emerging as a blossoming star while posting an impressive 11 yard per target 12 touchdown season in ’07. Late 2007 3rd round pick James Jones was entering his second season. Green Bay general manager Ted Thompson also invested an early 2nd round pick and late 3rd on pass catchers in the 2008 draft by selecting Jordy Nelson and Jermichael Finley.\nThese five receivers accounted for nearly 80% of Rodgers targets in his first three seasons en route to the Super Bowl XLV victory in February 2011. A few months later in 2011 Thompson & Co spent another 2nd round pick on a wide receiver (Randall Cobb). The continuity from the aforementioned five pass catchers and the addition of Cobb helped spur Rodgers to one of the best statistical seasons ever in 2011. In a 2011 ESPN The Magazine article Rodgers spoke to what helped contribute to his phenomal accuracy. He was quoted saying the following:\n“Learning to time up my drop with each route has been a big thing for me. It allows me to throw the ball in rhythm and hit the same release point with every throw, meaning that no matter what else is happening, the ball comes out on a similar plane. That’s when accuracy comes.”\nHe went on to say the fundamentals come first, “…then you have to become an expert in your own offense. Then you can get to a point where you’re attacking instead of reacting. Rich Gannon told me this back in 2006: You’ll know you’re at a good level by the things you’re thinking about when you break the huddle. If you’re thinking about your own guys – what routes they have, who has what – you’re not thinking about the right things. When I break the huddle now, I know what my guys are doing. I know the areas they’re going to be in.”\nRodgers efficiency was largely maintained over the next three seasons despite losing Driver and Jennings after the 2012 season along with Jones and Finley after the 2013 season. Rodgers added his 2nd MVP award in 2014 when Nelson earned a career high number of targets and another 2nd rount talent, Davante Adams, was added to the mix.\n2015-2016\nGreen Bay’s passing efficiency dropped to abysmal levels in 2015 after Nelson tore his ACL in the preseason. The efficiency didn’t immediately return to elite when Nelson came back in 2016 as the Packers sputtered out to a 4-6 start. Doug Farrar wrote about Green Bay’s passing game regression in October 2016.\n“Over time, Rodgers has overcompensated for the things that offense doesn’t provide to the point where it’s broken him as a mechanically consistent player.”\n“Rodgers appears to be operating under the belief that he must transcend a faulty offense with his own impressive physical attributes.”\nGreg Cosell also commented “What continues to stand out is that his accuracy is not what it once was. Precise ball placement is what made him special, and that attribute has been very erratic going back to last season. And now, it’s getting worse. Now, he’s missing wide-open receivers.”\nIf you refer back to Rodgers moving average EPA and CPOE plots the passing offense was not just below top 5 but considerably lower. It is fair to say Rodgers probably not at a “good level” when Nelson exited the line up and he didn’t trust what his guys were doing or where they were going to be. However, after Rodgers “run-the-table” remarks something changed and the offense turned in one of the better stretches of Rodgers career in terms of efficiency over the next nine games.\n2017\nIn 2017 Rodgers EPA and CPOE metrics were okay before the collarbone injury in week 6 (.16 EPA/dropback and CPOE +2.2%). He did return several weeks later in an effort to make a run at the playoffs but had a very poor performance which. Ultimately the 301 dropbacks placed Rodgers 14 overall in the rbsdm.com EPA and CPOE composite metric.\n2018\nPassing game continuity took a hit in 2018 as Jordy Nelson moved on to the Raiders. Adams and Cobb returned but the front office hadn’t been using draft capital on pass catchers like they did earlier in Rodgers career. Since taking Adams in the 2nd round of the 2014 draft Green Bay used two 5th and two 7th round selections on pass catchers from ’15 to ’17. Jimmy Graham was signed to a big deal entering his age 32 season two years removed from a torn patellar tendon to fill some of the void. Three receivers were also added in the 2018 draft although the earliest selection was in round 4 at pick 144. To make matter worse Rodgers suffered a tibial plateau fracture and a sprained MCL in week 1. The shakeup in personnel and coaching compounded by injury surely impacted the timing and rhythm and subsequent efficiency numbers.\nAnother thing that doesn’t help efficiency stats are throwaways. The most glaring numbers from 2018 the eye popping number of throwaways. Per PFF Rodgers threw the ball away once every 26 dropbacks from 2008 to 2017 but jumped to an astonishing once every 11 dropbacks in 2018.\nPFF Steve and Sam posited on the PFF NFL Show that this is likely the reason PFF grades have been more favorable to Rodgers than offensive efficiency metrics because of how much weight is placed on avoiding turnover worthy plays. PFF grade penalizes Rodgers much less for throwaways than EPA or CPOE would.\nIf we refer back to the initial plot we see Rodgers ended 2018 with a rolling 300 play average of .13 per play. In early 2019 under LaFleur Rodgers rolling average crept back into elite territory by the end of the week 8 matchup against Kansas City. Coincidentally, Rodgers posted an astonishing 95.5 QBR, .96 EPA per dropback, and perfect 158.3 passer rating in week 7 immediatly after Ben Baldwin’s No longer elite article on Rodgers. But then the next 300 dropbacks that cover week 9 at the Chargers to mid-game week 17 at Detroit we see Rodgers rolling average collapse down to -.05 per play.\n2019\nPrior to the 2019 Season Josh Hermsmeyer posed the question Are We Sure Aaron Rodgers Is Still An Elite Quarterback? at fivethirtyeight. Hermsmeyer noted CPOE decline starting in 2015 and speculated that the Packer’s underperformance could be due to a subpar play-action passing game. From 2015 to 2018 Hermsmeyer found Rodgers QBR was 32nd out of 41 in Raw QBR and could possibly be bolstered by a better play-action game with LaFleur in 2019.\nI charted play-action and personnel info in 2019 to pair this with the EPA and CPOE play by play data from nflscrapR to see how this played out. Rodgers CPOE on play-action was +5.6% as opposed to 0.2% for non play-action passes, however, EPA per play was the same at .17. This would suggest Rodgers wasn’t hitting the explosive downfield shots any more frequently on play-action. One glimmer of hope for Green Bay fans entering 2020 is that EPA per play on play-action plays with 2 or fewer wide receivers on the field was .24 per play and .08 per play in 11 personnel.\nWhat’s next?\nThe numbers tell us Green Bay has not completed passes and moved the chains at the rate an elite offense would lately. In 2019 we saw Rodgers flirt with elite level efficiency numbers through the first 8 weeks. But that all came crashing down and we rapidly approached the abysmal 2015 levels in later weeks.\nMaybe Rodgers will master the offense and get back to attacking instead of reaction and 2020 brings a big year two leap. Maybe without several elite weapons and an intimate comfortability with the scheme Rodgers just doesn’t see things well and we shouldn’t expect any bounce in year two. Maybe the confluence of factors that contributed to the previous elite play will never come back and we will see flashes of brilliance littered with throwaways.\nI don’t have the data or the knowledge to say whether or not Rodgers isn’t pulling the trigger when he should be. Alex Rollins of SB Nation took a film based approach to answering this question and noted Rodgers is not consistently operating within the structure of the offense and is sacrificing efficiency to extend plays but isn’t hitting them. Maybe the drop in receiver talent is driving Rodgers inconsistent play.\nWe surely can’t say the supporting cast is trash when it includes Davante Adams. Hermsmeyer used tracking data to measure how good NFL receivers are at creating separation and ranked Adams’ 2018 and 2019 seasons as the first and third best at generating separation on intermediate routes. Maybe we’ve reached the point where the lack of aggression and propensity to avoid the turnover worthy play is holding back the offense. NFL Next Gen Stats has created an aggressiveness metric that tracks the amount of passing attempts a quarterback makes that are into tight coverage, where there is a defender within 1 yard or less of the receiver at the time of completion or incompletion. The aggressiveness metric only goes back to 2016 but Rodgers has consistently been among the lowest in the league as he is not afraid to throw the ball away if he doesn’t see anything he likes.\nDesirability bias badly wants me to handwave the poor numbers over the past five years and blame McCarthy, injury, or lack of receiving weapons. I don’t know how much to weigh those factors but they are at least partially contributing to Rodgers decline in efficiency stats. I have a few more weeks to hope the offense takes a significant step foward like the 2016 Falcons offense did under Shanahan and LaFleur. But you wont’t see me making any bullish bets on this offense anytime soon.\n\n\n",
    "preview": "posts/2020-08-22-rodgers-efficiency-decline/rodgers-efficiency-decline_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-08-19-visualizing-the-runpass-efficiency-gap/",
    "title": "Visualizing the Run/Pass Efficiency Gap",
    "description": "Using nflfastR data to show how much more efficient passing is than rushing at the team level",
    "author": [
      {
        "name": "Anthony Reinhard",
        "url": "https://twitter.com/reinhurdler"
      }
    ],
    "date": "2020-08-20",
    "categories": [
      "Figures",
      "nflfastR"
    ],
    "contents": "\nTable of Contents\nIntro\nThe Data\nPlot Extras\nBasic Plot\nFancier Plot\nIntro\nThe football philosophers of Twitter know that the passing game reigns supreme over the rushing game. As easy as it is to just say this, we need to be able to prove it. We also need to acknowledge that there could come a time where a team is so good at running the ball or so bad at passing that they should be running the ball more often. I thought an easy way to communicate this would be show the difference between a team’s dropback EPA/play and their designed rush EPA/play while also showing how often they attempt to pass.\nThe Data\nAfter we’ve pulled in the 2019 play-by-play data from the nflfastR data repository, we need to start thinking about what cuts of data we should exclude from our sample. I’ve chosen to include only 1st and 2nd down here, as teams are not burdened on these plays with having to advance the ball to the first down marker to keep their drive alive. I’m also going to only include plays where the game was reasonably competitive, which I’m defining here as win probability between 20% and 80%. Lastly, I’m going to remove plays that begin inside the final two minutes of the half, where teams are more likely to be passing (unless they are salting away a lead, of course). Keeping only the plays described above ensures that the offense is not in an obvious passing or running situation.\n\n\nlibrary(tidyverse)\n\npbp_df <- readRDS(url(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_2019.rds\"))\n\nprem_epa_df <- pbp_df %>%\n  filter(down <= 2 & wp <= .8 & wp >= .2 & half_seconds_remaining >= 120) %>%\n  group_by(posteam) %>%\n  summarize(\n    pass_freq = mean(pass),\n    pass_epa = mean(ifelse(pass == 1, epa, NA), na.rm = T),\n    run_epa = mean(ifelse(rush == 1, epa, NA), na.rm = T),\n    pass_epa_prem = pass_epa - run_epa\n  )\n\nPlot Extras\nIt doesn’t hurt to label the four corners of the plot so it can be understood more easily. We also want to include some gridlines that show the league averages.\n\n\nlabel_df <- data.frame(\n  pass_freq = c(.675, .425, .675, .425),\n  pass_epa_prem = c(.42, .42, -.02, -.02),\n  label = c(\n    \"Better at Passing\\nPass A Lot\",\n    \"Better at Passing\\nRun A Lot\",\n    \"Better at Running\\nPass A Lot\",\n    \"Better at Running\\nRun A Lot\"\n  )\n)\n\npbp_df %>%\n  filter(down <= 2 & wp <= .8 & wp >= .2 & half_seconds_remaining >= 120) %>%\n  group_by(season) %>%\n  summarize(\n    pass_freq = mean(pass),\n    pass_epa = mean(ifelse(pass == 1, epa, NA), na.rm = T),\n    run_epa = mean(ifelse(rush == 1, epa, NA), na.rm = T),\n    pass_epa_prem = pass_epa - run_epa\n  )\n\n# A tibble: 1 x 5\n  season pass_freq pass_epa run_epa pass_epa_prem\n   <int>     <dbl>    <dbl>   <dbl>         <dbl>\n1   2019     0.524   0.0894  -0.117         0.206\n\nWe can use this function to turn team abbreviations into the URL needed get high quality logos from ESPN.\n\n\nESPN_logo_url <- function(x) paste0(\"https://a.espncdn.com/i/teamlogos/nfl/500/\", x, \".png\")\n\nBasic Plot\nI typically make some cosmetic changes to my graphs to make them more visually appealing. If that isn’t your thing, this is the bare bones version.\n\n\nlibrary(ggimage) # I'm going to use ggimage's geom_image to add team logos here\nlibrary(scales) # percent_format will convert the y-axis to percentages\n\nggplot(data = prem_epa_df, aes(x = pass_epa_prem, y = pass_freq)) +\n  geom_hline(yintercept = 0.525, color = \"red\", linetype = \"52\", size = 0.2) +\n  geom_vline(xintercept = 0.198, color = \"red\", linetype = \"52\", size = 0.2) +\n  geom_text(data = label_df, aes(label = label), color = \"red\", size = 1.5) +\n  geom_image(aes(image = ESPN_logo_url(posteam)), size = 0.04) +\n  scale_y_continuous(labels = percent_format(accuracy = 1)) +\n  labs(\n    title = \"Dropback Premium vs Dropback Frequency\",\n    subtitle = \"Early Downs with Win Probability Between 20% and 80% Outside of\\nLast Two Minutes of The Half\",\n    y = \"Dropback Freq\",\n    x = \"(EPA/Play on Dropbacks) - (EPA/Play on Designed Runs)\"\n  ) +\n  theme_minimal()\n\n\nFancier Plot\nAdding a bunch of junk to this plot will require loading some additional packages…\n\n\nlibrary(scales) # fixes labels\nlibrary(shadowtext) # geom_shadowtext adds a thin border around your text, making it more readable\nlibrary(ggpmisc) # geom_grob is my preferred function for placing images on plots\nlibrary(grid) # convert an image to a raster object\nlibrary(magick) # read our image in so we can make manipulations, if needed\n\nI like to use +/- signs for positive & negative sometimes. This function usually gets the job done and can be applied directly to label argument of the axis layer.\n\n\nplus_lab <- function(x, accuracy = NULL, suffix = \"\") paste0(ifelse(x > 0, \"+\", \"\"), number(x, accuracy = accuracy, suffix = suffix, scale = ifelse(suffix == \"%\", 100, 1)))\nplus_lab_format <- function(accuracy = NULL, suffix = \"\") function(x) plus_lab(x, accuracy = accuracy, suffix = suffix)\n\nThis function will take a URL and return a grob with a modified color or transparency. It will produce an image when set as the label aesthetic for geom_grob. Processing graphs takes longer with geom_grob than it does with geom_image, but aspect ratios are always perfect for each image.\n\n\ngrob_img_adj <- function(img_url, alpha = 1, whitewash = 0) {\n  return(lapply(img_url, function(x) {\n    if (is.na(x)) {\n      return(NULL)\n    } else {\n      img <- magick::image_read(x)[[1]]\n      img[1, , ] <- as.raw(255 - (255 - as.integer(img[1, , ])) * (1 - whitewash))\n      img[2, , ] <- as.raw(255 - (255 - as.integer(img[2, , ])) * (1 - whitewash))\n      img[3, , ] <- as.raw(255 - (255 - as.integer(img[3, , ])) * (1 - whitewash))\n      img[4, , ] <- as.raw(as.integer(img[4, , ]) * alpha)\n      return(grid::rasterGrob(image = magick::image_read(img)))\n    }\n  }))\n}\n\nThis is my own personal theme that I use for all my graphs. I typically use a different font, but I’ll use the default here. I’m also going to leave out my personal branding for this one.\n\n\ntheme_SB <- theme(\n  line = element_line(lineend = \"round\", color = \"darkblue\"),\n  text = element_text(color = \"darkblue\"),\n  plot.background = element_rect(fill = \"grey95\", color = \"transparent\"),\n  panel.border = element_rect(color = \"darkblue\", fill = NA),\n  panel.background = element_rect(fill = \"white\", color = \"transparent\"),\n  axis.ticks = element_line(color = \"darkblue\", size = 0.5),\n  axis.ticks.length = unit(2.75, \"pt\"),\n  axis.title = element_text(size = 8),\n  axis.text = element_text(size = 7, color = \"darkblue\"),\n  plot.title = element_text(size = 14),\n  plot.subtitle = element_text(size = 8),\n  plot.caption = element_text(size = 5),\n  legend.background = element_rect(fill = \"grey90\", color = \"darkblue\"),\n  legend.key = element_blank(),\n  panel.grid.minor = element_blank(),\n  panel.grid.major = element_line(color = \"grey85\", size = 0.3),\n  axis.title.y = element_text(angle = 0, vjust = 0.5),\n  strip.background = element_blank(),\n  strip.text = element_text(size = 6, color = \"darkblue\"),\n  legend.position = \"bottom\"\n)\n\nAnd here is the final graph!\n\n\nggplot(data = prem_epa_df, aes(x = pass_epa_prem, y = pass_freq)) +\n  geom_hline(yintercept = 0.525, color = \"red\", linetype = \"52\", size = 0.2) +\n  geom_vline(xintercept = 0.198, color = \"red\", linetype = \"52\", size = 0.2) +\n  geom_shadowtext(data = label_df, aes(label = label), color = \"red\", size = 1.5, bg.color = \"white\", bg.r = 0.2) +\n  geom_grob(aes(x = pass_epa_prem, y = pass_freq, label = grob_img_adj(ESPN_logo_url(posteam), alpha = 0.7), vp.height = 0.06)) +\n  scale_x_continuous(labels = plus_lab_format(accuracy = .01), breaks = seq(-1, 1, .1), limits = c(-0.1, 0.5), expand = expansion(mult = c(0.02, 0.02))) +\n  scale_y_continuous(labels = percent_format(accuracy = 1), breaks = seq(0, 1, .05), limits = c(.4, .7), expand = expansion(mult = c(0.03, 0.03))) +\n  labs(\n    title = \"Dropback Premium vs Dropback Frequency\",\n    subtitle = \"Early Downs with Win Probability Between 20% and 80% Outside of Last Two\\nMinutes of The Half\",\n    y = \"Dropback\\nFreq\",\n    x = \"(EPA/Play on Dropbacks) - (EPA/Play on Designed Runs)\"\n  ) +\n  theme_SB\n\n\n\n\n",
    "preview": "posts/2020-08-19-visualizing-the-runpass-efficiency-gap/visualizing-the-runpass-efficiency-gap_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3300
  },
  {
    "path": "posts/2020-08-20-adjusting-epa-for-strenght-of-opponent/",
    "title": "Adjusting EPA for Strength of Opponent",
    "description": "This article shows how to adjust a team's EPA per play for the strength of their opponent. The benefits of adjusted EPA will be demonstrated as well!",
    "author": [
      {
        "name": "Jonathan Goldberg",
        "url": "https://twitter.com/gberg1303"
      }
    ],
    "date": "2020-08-20",
    "categories": [
      "Opponent adjusted EPA",
      "Figures",
      "nflfastR"
    ],
    "contents": "\nHere we are going to take a look at how to adjust a team’s epa per play to the strength of their opponent. This technique will use weekly epa/play metrics, which can ultimately summarize a team’s season-long performance. It is also possible to adjust the epa of individual plays with this process if you are so inclined to do so.\nQuick note: the adjustments were inspired by the work done in this paper. It’s a bit technical but a good additional read!\nAlright, let’s get into it by first loading up our data!\n\n\nNFL_PBP <- purrr::map_df(2009:2019, function(x) {\n  readr::read_csv(\n    glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.csv.gz\")\n  )\n})\n\nWith the data loaded, we can finally get down to business by summarizing each team’s weekly epa/play.\n\n\nlibrary(tidyverse)\nepa_data <- NFL_PBP %>%\n  dplyr::filter(!is.na(epa), !is.na(ep), !is.na(posteam), play_type == \"pass\" | play_type == \"run\") %>%\n  dplyr::group_by(game_id, season, week, posteam, home_team) %>%\n  dplyr::summarise(\n    off_epa = mean(epa),\n  ) %>%\n  dplyr::left_join(NFL_PBP %>%\n    filter(!is.na(epa), !is.na(ep), !is.na(posteam), play_type == \"pass\" | play_type == \"run\") %>%\n    dplyr::group_by(game_id, season, week, defteam, away_team) %>%\n    dplyr::summarise(def_epa = mean(epa)),\n  by = c(\"game_id\", \"posteam\" = \"defteam\", \"season\", \"week\"),\n  all.x = TRUE\n  ) %>%\n  dplyr::mutate(opponent = ifelse(posteam == home_team, away_team, home_team)) %>%\n  dplyr::select(game_id, season, week, home_team, away_team, posteam, opponent, off_epa, def_epa)\n\nNow we can get into the fun part: adjusting a team’s epa/play based on the strength of the opponent they are up against.\nWe are going to reframe each team’s epa/play as a team’s weekly opponent.\nWe are going to convert each statistic into a moving average of the last ten games — this decision was based on this research and this model — and lag that statistic by one week. The lag is important because we need to be comparing a team’s weekly performance against their opponent’s average performance up to that point in the season.\nWe are going to join the data back to the epa_dataset.\n\n\n# Construct opponent dataset and lag the moving average of their last ten games.\nopponent_data <- epa_data %>%\n  dplyr::select(-opponent) %>%\n  dplyr::rename(\n    opp_off_epa = off_epa,\n    opp_def_epa = def_epa\n  ) %>%\n  dplyr::group_by(posteam) %>%\n  dplyr::arrange(season, week) %>%\n  dplyr::mutate(\n    opp_def_epa = pracma::movavg(opp_def_epa, n = 10, type = \"s\"),\n    opp_def_epa = dplyr::lag(opp_def_epa),\n    opp_off_epa = pracma::movavg(opp_off_epa, n = 10, type = \"s\"),\n    opp_off_epa = dplyr::lag(opp_off_epa)\n  )\n\n# Merge opponent data back in with the weekly epa data\nepa_data <- epa_data %>%\n  left_join(\n    opponent_data,\n    by = c(\"game_id\", \"season\", \"week\", \"home_team\", \"away_team\", \"opponent\" = \"posteam\"),\n    all.x = TRUE\n  )\n\nDon’t fret that the opponent’s epa columns will have NAs in the first week. You simply can’t lag from the first observation.\nThe final piece of the equation needed to make the adjustments is the league mean for epa/play on offense and defense. We need to know how strong the opponent is relative to the average team in the league.\n\n\nepa_data <- epa_data %>%\n  dplyr::left_join(epa_data %>%\n    dplyr::filter(posteam == home_team) %>%\n    dplyr::group_by(season, week) %>%\n    dplyr::summarise(\n      league_mean = mean(off_epa + def_epa)\n    ) %>%\n    dplyr::ungroup() %>%\n    dplyr::group_by(season) %>%\n    dplyr::mutate(\n      league_mean = lag(pracma::movavg(league_mean, n = 10, type = \"s\"), ) # We lag because we need to know the league mean up to that point in the season\n    ),\n  by = c(\"season\", \"week\"),\n  all.x = TRUE\n  )\n\nFinally, we can get to adjusting a team’s epa/play. We’ll create an adjustment measure by subtracting the opponent’s epa/play metrics from the league mean. Then we add the adjustment measure to each team’s weekly performance.\n\n\n# Adjust EPA\nepa_data <- epa_data %>%\n  dplyr::mutate(\n    off_adjustment_factor = ifelse(!is.na(league_mean), league_mean - opp_def_epa, 0),\n    def_adjustment_factor = ifelse(!is.na(league_mean), league_mean - opp_off_epa, 0),\n    adjusted_off_epa = off_epa + off_adjustment_factor,\n    adjusted_def_epa = def_epa + def_adjustment_factor,\n  )\n\nWe’re done! You can now view each team’s epa/play adjusted for their strength of schedule. Let’s check out how different the league looks by comparing unadjusted epa to adjusted epa stats.\n\n\n\nAbove, you can see that some teams are revealed to be stronger after adjusting their epa/play while other teams appear to be weaker. We can use these adjustments to make more accurate predictions of individual NFL games.\nHere, each metrics are used in separate glm models to predict the outcome of games from the past two seasons. Their accuracy is below.\n\n\n[1] \"Adjusted EPA Accuracy\"\n\n[1] 0.6404494\n\n\n\n[1] \"Normal EPA Accuracy\"\n\n[1] 0.6348315\n\nThere is a slight edge to the adjusted EPA model. Its a solid start but there is more work to be done in finding the best version on epa/play.\nThere is good work being done on properly weighting epa on a given type of play. For instance, DVOA is a does well in predicting future team performance because the downweight the impact of interceptions in their metric. More work can be done to properly weight epa based on its play type!\nIt is possible to make these adjustments at the individual play level and with more specificity. For instance, you could adjust run plays based on the team’s run defense rather than adjusting the entire offense to the team’s entire defense. I think more work should be done to determine if these more detailed techniques can improve the predictiveness of the stat.\nThere may be other ways to construct epa/play that improve its strength as a predictor The paper that inspired this article uses the solution of an optimization problem to construct a team’s true offensive epa/play and defensive epa/play. Perhaps a moving averaged should be eschewed in favor of a technique that more properly accounts for common regression to the mean over the offseason.\nThanks to Sebastian Carl and Ben Baldwin for setting this forum up! I can’t wait to see others’ works and improvements to my own make its way on here.\n\n\n",
    "preview": "posts/2020-08-20-adjusting-epa-for-strenght-of-opponent/adjusting-epa-for-strength-of-opponent_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3300
  },
  {
    "path": "posts/2020-08-20-python-contributing-example/",
    "title": "Python contributing example",
    "description": "Showing how to contribute using Python code",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin"
      }
    ],
    "date": "2020-08-20",
    "categories": [
      "nflfastR",
      "python"
    ],
    "contents": "\n\nContents\nContributing in python\nThere is some stuff to install and setup:\nOnce setup is done:\n\n\nContributing in python\nThere is some stuff to install and setup:\nR and RStudio\nGet github in RStudio setup (see here)\nIn R, run install.packages(c(\"distill\", \"rmarkdown\")) to get the needed packages for the website installed, and install.packages(\"reticulate\") to install reticulate, which allows python to be run from within R\nYou’ll need to somehow point RStudio to your python installation. I ran py_install(\"pandas\") and the below worked\nOnce setup is done:\nStart working on your document. You can put in python code chunks as in the examples below (see here for the source code).\nOnce you have everything ready in your document, press the “Knit” button on the top and it will create the document.\nThen you can committ all the files using the Git tab\nPlease see the step-by-step instructions here\nOnce everything is set up, you can just use python code as normal. Here is a quick example borrowing some code from from Deryck’s nflfastR python guide.\n\nimport pandas\ndata = pandas.read_csv(\"https://github.com/guga31bb/nflfastR-data/blob/master/data/play_by_play_2019.csv.gz?raw=True\", compression='gzip', low_memory=False)\ndata = data.loc[(data.play_type.isin(['no_play','pass','run'])) & (data.epa.isna()==False)]\ndata.groupby('posteam')[['epa']].mean()\n              epa\nposteam          \nARI      0.002421\nATL      0.009436\nBAL      0.165036\nBUF     -0.030460\nCAR     -0.091361\nCHI     -0.082597\nCIN     -0.095607\nCLE     -0.039001\nDAL      0.104154\nDEN     -0.057336\nDET     -0.046365\nGB       0.054262\nHOU      0.026844\nIND      0.002937\nJAX     -0.060641\nKC       0.167632\nLA      -0.007031\nLAC      0.027416\nLV       0.024514\nMIA     -0.082475\nMIN      0.024179\nNE      -0.004101\nNO       0.060163\nNYG     -0.062409\nNYJ     -0.147206\nPHI     -0.002494\nPIT     -0.146697\nSEA      0.024912\nSF       0.066445\nTB      -0.017379\nTEN      0.058257\nWAS     -0.133023\n\nGrouping by QBs:\n\nqbs = data.groupby(['passer','posteam'], as_index=False).agg({'epa':'mean',\n                                                              'cpoe':'mean',\n                                                              'play_id':'count'})\n# at least 200 plays\nqbs = qbs.loc[qbs.play_id>199]\n# sort by EPA\nqbs.sort_values('epa', ascending=False, inplace=True)\n\n#Round to two decimal places where appropriate\nqbs = qbs.round(2)\n\n#Rename columns\nqbs.columns = ['Player','Team','EPA per Dropback','CPOE','Dropbacks']\n\nqbs\n            Player Team  EPA per Dropback   CPOE  Dropbacks\n86       P.Mahomes   KC              0.32   2.26        721\n71       L.Jackson  BAL              0.29   2.76        554\n81      M.Stafford  DET              0.22   2.09        330\n39      D.Prescott  DAL              0.19   0.97        675\n95     R.Tannehill  TEN              0.19   6.36        421\n28         D.Brees   NO              0.18   6.20        447\n52     J.Garoppolo   SF              0.17   1.81        629\n96        R.Wilson  SEA              0.16   7.10        732\n64       K.Cousins  MIN              0.16   3.77        585\n40        D.Watson  HOU              0.15   2.12        715\n6        A.Rodgers   GB              0.15   1.72        755\n29          D.Carr   LV              0.14   5.69        574\n87        P.Rivers  LAC              0.11   3.72        676\n105  T.Bridgewater   NO              0.09   0.41        235\n53          J.Goff   LA              0.09  -1.79        703\n61       J.Winston   TB              0.08   0.67        743\n92   R.Fitzpatrick  MIA              0.07  -0.48        612\n25         C.Wentz  PHI              0.07  -0.77        719\n79          M.Ryan  ATL              0.06   1.80        731\n104        T.Brady   NE              0.06  -2.53        724\n46      J.Brissett  IND              0.06  -3.26        540\n45         J.Allen  BUF              0.03  -2.29        618\n14      B.Mayfield  CLE              0.02  -2.96        641\n67        K.Murray  ARI              0.02  -1.52        654\n18        C.Keenum  WAS              0.01  -1.09        285\n44    G.Minshew II  JAX              0.01  -4.28        592\n75       M.Mariota  TEN              0.01  -3.21        209\n1         A.Dalton  CIN             -0.00  -3.27        601\n97       S.Darnold  NYJ             -0.01   1.04        516\n62         K.Allen  CAR             -0.03  -0.95        574\n36         D.Jones  NYG             -0.03  -1.94        560\n82      M.Trubisky  CHI             -0.03  -2.33        605\n78       M.Rudolph  PIT             -0.03  -1.23        331\n51        J.Flacco  DEN             -0.06  -0.21        313\n32       D.Haskins  WAS             -0.18  -4.65        255\n27        D.Blough  DET             -0.20 -13.59        209\n\nHopefully the process is painless once all the setup is done.\n\nView source code on GitHub \n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-19-matching-players-without-id-keys/",
    "title": "Matching players without ID keys",
    "description": "Rebuilding player graphs when ID keys go missing or are corrupted.",
    "author": [
      {
        "name": "Analytics Darkweb",
        "url": "https://twitter.com/footballdaRkweb"
      }
    ],
    "date": "2020-08-19",
    "categories": [
      "Figures",
      "Roster",
      "nflfastR"
    ],
    "contents": "\nSometimes when we go to remake graphs from other people resources that used to work no longer do. This example shows how to work through that problem.\nFirst let’s load the data and define which season we care about.\n\n\nlibrary(tidyverse)\nlibrary(nflfastR)\nseasons <- 2019\npbp <- purrr::map_df(seasons, function(x) {\n  readr::read_csv(\n    glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.csv.gz\")\n  )\n})\n\nroster <-\n  readRDS(url(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/roster-data/roster.rds\")) %>%\n  filter(teamPlayers.position == \"QB\", team.season == 2019)\n\ncpoe <-\n  pbp %>%\n  filter(!is.na(cpoe)) %>%\n  group_by(passer_player_id, air_yards) %>%\n  summarise(\n    count = n(),\n    cpoe = mean(cpoe)\n  )\n\nseason <- 2019\n\nNext, we have a couple of players that are in our Top 30 from last year that have changed teams from where our roster has a different nfl ID structure than what we get from nflfastR.\nSo instead we can match on first initial, last name, and jersey number. I’ve renamed posteam to team.abbr below to change as little of Seb’s code as possible. Also, we have to update the roster file to indicate that the Raiders have moved to Las Vegas.\n\n\nsummary <-\n  pbp %>%\n  separate(passer_player_name, c(\"firstName\", \"lastName\"), sep = \"\\\\.\") %>%\n  filter(!is.na(cpoe)) %>%\n  group_by(lastName, posteam, firstName, passer_player_id, jersey_number) %>%\n  summarise(plays = n()) %>%\n  ungroup() %>%\n  arrange(desc(plays)) %>%\n  head(30) %>%\n  mutate(\n     lastName = ifelse(lastName == \"Minshew II\", \"Minshew\", lastName)\n   ) %>%\n  left_join(\n    roster %>% \n      filter(team.season == seasons) %>% \n      mutate(firstInit = str_extract(teamPlayers.firstName, \"\\\\w\"), team.abbr = ifelse(team.abbr == \"OAK\", \"LV\", team.abbr)) %>% \n      select(name = teamPlayers.displayName, firstInit, teamPlayers.lastName, team.abbr, teamPlayers.headshot_url, teamPlayers.jerseyNumber),\n      by = c(\"lastName\" = \"teamPlayers.lastName\", \"firstName\" = \"firstInit\", \"jersey_number\" = \"teamPlayers.jerseyNumber\", \"posteam\" = \"team.abbr\")\n  ) %>%\n  mutate(# some headshot urls are broken. They are checked here and set to a default \n    teamPlayers.headshot_url = dplyr::if_else(\n      RCurl::url.exists(as.character(teamPlayers.headshot_url)),\n      as.character(teamPlayers.headshot_url),\n      \"http://static.nfl.com/static/content/public/image/fantasy/transparent/200x200/default.png\",\n    )\n  ) %>%\n  left_join(cpoe, by = \"passer_player_id\") %>%\n  left_join(\n    teams_colors_logos %>% select(team_abbr, team_color, team_logo_espn),\n    by = c(\"posteam\" = \"team_abbr\")\n  ) %>%\n  rename(team.abbr = posteam)\n\ncolors_raw <-\n  summary %>%\n  group_by(passer_player_id) %>%\n  summarise(team = first(team.abbr), name = first(name)) %>%\n  left_join(\n    teams_colors_logos %>% select(team_abbr, team_color),\n    by = c(\"team\" = \"team_abbr\")\n  ) %>%\n  arrange(name)\n\nn_eval <- 80\ncolors <-\n  as.data.frame(lapply(colors_raw, rep, n_eval)) %>%\n  arrange(name)\n\n\nmean <-\n  summary %>%\n  group_by(air_yards) %>%\n  summarise(league = mean(cpoe), league_count = n())\n\nNext, we need to change how Sebastian calls for colors in the geom_smooth call due to a package update. We can make a named vector to match the color hex numbers to the data frame.\n\n\nasp <- 1.2\ncols <- c()\nfor(i in 1:length(unique(summary$team_color))) {\n  cols <- append(cols, unique(summary$team_color)[i])\n}\ncolor_names <- as.vector(unique(summary$team_color))\ncols <- set_names(cols, color_names)\n\nplot <-\n  summary %>%\n  ggplot(aes(x = air_yards, y = cpoe)) +\n  geom_smooth(\n    data = mean, aes(x = air_yards, y = league, weight = league_count), n = n_eval,\n    color = \"red\", alpha = 0.7, se = FALSE, size = 0.5, linetype = \"dashed\"\n  ) +\n  geom_smooth(\n    se = FALSE, alpha = 0.7, aes(weight = count, color = team_color), size = 0.65,\n    n = n_eval\n  ) +\n  scale_color_manual(values = cols) + \n  geom_point(color = summary$team_color, size = summary$count / 15, alpha = 0.4) +\n  ggimage::geom_image(aes(x = 27.5, y = -20, image = team_logo_espn),\n                      size = .15, by = \"width\", asp = asp\n  ) +\n  ggimage::geom_image(aes(x = -2.5, y = -20, image = teamPlayers.headshot_url),\n                      size = .15, by = \"width\", asp = asp\n  ) +\n  xlim(-10, 40) + # makes sure the smoothing algorithm is evaluated between -10 and 40\n  coord_cartesian(xlim = c(-5, 30), ylim = c(-25, 25)) + # 'zoom in'\n  labs(\n    x = \"Target Depth In Yards Thrown Beyond The Line Of Scrimmage (DOT)\",\n    y = \"Completion Percentage Over Expectation (CPOE in percentage points)\",\n    caption = \"Figure: @mrcaseb | Data: @nflfastR | Update: AnalyticsDarkweb\",\n    title = glue::glue(\"Passing Efficiency {season}\"),\n    subtitle = \"CPOE function of depth. Dotsize equivalent to num targets. Red Line = League Average.\"\n  ) +\n  theme_bw() +\n  theme(\n    axis.title = element_text(size = 10),\n    axis.text = element_text(size = 6),\n    plot.title = element_text(size = 12, hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(size = 10, hjust = 0.5),\n    plot.caption = element_text(size = 8),\n    legend.title = element_text(size = 8),\n    legend.text = element_text(size = 6),\n    strip.text = element_text(size = 6, hjust = 0.5, face = \"bold\"),\n    aspect.ratio = 1 / asp,\n    legend.position = \"none\"\n  ) +\n  facet_wrap(vars(name), ncol = 6, scales = \"free\")\n\nLastly, we can kick out a save of our image file as before.\n\n\nplot\n\n\nggsave(glue::glue(\"cpoe_vs_dot_{season}.png\"), dpi = 600, width = 24, height = 21, units = \"cm\")\n\n\n\n",
    "preview": "posts/2020-08-19-matching-players-without-id-keys/matching-players-without-id-keys_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 4800
  },
  {
    "path": "posts/2020-08-19-neural-nets-using-r/",
    "title": "Neural Nets using R",
    "description": "Using Keras in R to build neural networks.",
    "author": [
      {
        "name": "Analytics Darkweb",
        "url": "https://twitter.com/footballdaRkweb"
      }
    ],
    "date": "2020-08-19",
    "categories": [
      "Keras",
      "Tensorflow",
      "nflfastR"
    ],
    "contents": "\n\n\nlibrary(reticulate)\nlibrary(tidyverse)\n\nuse_condaenv(\"r-tf-gpu\", required = TRUE)\n\n\nlibrary(keras)\n\nBefore you proceed you will need to install Keras for R. In order to do that, I followed this guide. https://github.com/antoniosehk/keras-tensorflow-windows-installation\nThe following guide is heavily borrowed from the following Rstudio guide! https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_regression/\nUse the following lines to download the data if you need to.\n\n\nseasons <- 2010:2019\npbp <- purrr::map_df(seasons, function(x) {\n  readr::read_csv(\n    glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{x}.csv.gz\")\n  )\n})\n\nInstead of throwing the kitchen sink at a problem, let’s choose some variables we think would influence yards after catch.\n\n\ndf <-\n  pbp %>%\n  filter(pass == 1) %>%\n  mutate(\n    pass_location = as.numeric(ifelse(pass_location == \"middle\", 1, 0)),\n    roof = as.numeric(as.factor(roof))\n    ) %>%\n  select(yardline_100, down, ydstogo, shotgun, air_yards, yards_after_catch, qb_hit, pass_location, roof) %>%\n  na.omit()\n\nIn this step you are converting your data frame into something Keras can injest.\n\n\nset.seed(7)\nsample <- sample.int(n = nrow(df), size = floor(.9*nrow(df)), replace = F)\ntrain_df <- df[sample, ]\ntest_df  <- df[-sample, ]\n\ntrain_labels <- train_df$yards_after_catch\ntest_labels <- test_df$yards_after_catch\n\ntrain_df <- train_df %>% select(-yards_after_catch)\ntest_df <- test_df %>% select(-yards_after_catch)\n\ncolumn_names <- colnames(train_df)\n\ntrain_df <- train_df %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  setNames(column_names) %>% \n  mutate(label = train_labels)\n\ntest_df <- test_df %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  setNames(column_names) %>% \n  mutate(label = test_labels)\n\nNext we’ll use a little helper function to create the model, here we’re just doing a little toy model. No convolutions or anything too fancy. Just a little good ole fashioned brute force! Mostly because you should go read about different network types before you use them. :)\n\n\nlibrary(tfdatasets)\n\nspec <- feature_spec(train_df, label ~ . ) %>% \n  step_numeric_column(all_numeric(), normalizer_fn = scaler_standard()) %>% \n  fit()\n\nspec\n\n-- Feature Spec --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \nA feature_spec with 8 steps.\nFitted: TRUE \n-- Steps ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \nThe feature_spec has 1 dense features.\nStepNumericColumn: yardline_100, down, ydstogo, shotgun, air_yards, qb_hit, pass_location, roof \n-- Dense features ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- \n\nlayer <- layer_dense_features(\n  feature_columns = dense_features(spec), \n  dtype = tf$float32\n)\n\nbuild_model <- function() {\n  input <- layer_input_from_dataset(train_df %>% select(-label))\n  \n  output <- input %>% \n    layer_dense_features(dense_features(spec)) %>% \n    layer_dense(units = 64, activation = \"relu\") %>%\n    layer_dropout(.25) %>%\n    layer_dense(units = 64, activation = \"relu\") %>%\n    layer_dropout(.25) %>%\n    layer_dense(units = 64, activation = \"relu\") %>%\n    layer_dropout(.25) %>%\n    layer_dense(units = 1) \n  \n  model <- keras_model(input, output)\n  \n  model %>% \n    compile(\n      loss = \"mse\",\n      optimizer = \"adam\",\n      metrics = list(\"mean_absolute_error\")\n    )\n  \n  model\n}\n\nearly_stop <- callback_early_stopping(monitor = \"val_loss\", patience = 20)\n\nprint_dot_callback <- callback_lambda(\n  on_epoch_end = function(epoch, logs) {\n    if (epoch %% 20 == 0) cat(\"\\n\")\n    cat(\".\")\n  }\n)  \n\nmodel <- build_model()\n\nsummary(model)\n\nModel: \"model\"\n______________________________________________________________________\nLayer (type)           Output Shape   Param # Connected to            \n======================================================================\nair_yards (InputLayer) [(None,)]      0                               \n______________________________________________________________________\ndown (InputLayer)      [(None,)]      0                               \n______________________________________________________________________\npass_location (InputLa [(None,)]      0                               \n______________________________________________________________________\nqb_hit (InputLayer)    [(None,)]      0                               \n______________________________________________________________________\nroof (InputLayer)      [(None,)]      0                               \n______________________________________________________________________\nshotgun (InputLayer)   [(None,)]      0                               \n______________________________________________________________________\nyardline_100 (InputLay [(None,)]      0                               \n______________________________________________________________________\nydstogo (InputLayer)   [(None,)]      0                               \n______________________________________________________________________\ndense_features_1 (Dens (None, 8)      0       air_yards[0][0]         \n                                              down[0][0]              \n                                              pass_location[0][0]     \n                                              qb_hit[0][0]            \n                                              roof[0][0]              \n                                              shotgun[0][0]           \n                                              yardline_100[0][0]      \n                                              ydstogo[0][0]           \n______________________________________________________________________\ndense (Dense)          (None, 64)     576     dense_features_1[0][0]  \n______________________________________________________________________\ndropout (Dropout)      (None, 64)     0       dense[0][0]             \n______________________________________________________________________\ndense_1 (Dense)        (None, 64)     4160    dropout[0][0]           \n______________________________________________________________________\ndropout_1 (Dropout)    (None, 64)     0       dense_1[0][0]           \n______________________________________________________________________\ndense_2 (Dense)        (None, 64)     4160    dropout_1[0][0]         \n______________________________________________________________________\ndropout_2 (Dropout)    (None, 64)     0       dense_2[0][0]           \n______________________________________________________________________\ndense_3 (Dense)        (None, 1)      65      dropout_2[0][0]         \n======================================================================\nTotal params: 8,961\nTrainable params: 8,961\nNon-trainable params: 0\n______________________________________________________________________\n\nNext let’s run the model and see how it does!\n\n\nhistory <- model %>% fit(\n  x = train_df %>% select(-label),\n  y = train_df$label,\n  epochs = 500,\n  batchsize = 64,\n  validation_split = 0.2,\n  verbose = 0,\n  callbacks = list(print_dot_callback, early_stop)\n)\n\n....................\n.................\n\nNow to check the results!\nHere we visualize how our nnet trained over our epochs. We define epochs here since we had some early stopping.\n\n\nlibrary(ggplot2)\nhistory$params$epochs <- length(history$metrics$loss)\n\nplot(history)\n\n\ntest_predictions <- model %>% predict(test_df %>% select(-label))\n\nNext we can take a look at the mean absolute error and loss from our model on the test set.\n\n\nc(loss, mae) %<-% (model %>% evaluate(test_df %>% select(-label), test_df$label, verbose = 0))\n\nloss\n\n[1] 46.37026\n\nmae\n\n[1] 4.239087\n\ntest_predictions <- test_predictions %>% as.data.frame()\n\ntest_predictions %>% ggplot(aes(V1)) + geom_density()\n\n\nLastly, let’s visualize our trained model versus both actual YAC yardage and nflfastR’s XYAC mean yards model.\n\n\ndf <-\n  pbp %>%\n  filter(pass == 1) %>%\n  mutate(\n    pass_location = as.numeric(ifelse(pass_location == \"middle\", 1, 0)),\n    roof = as.numeric(as.factor(roof))\n  ) %>%\n  select(yardline_100, down, ydstogo, shotgun, air_yards, yards_after_catch, qb_hit, pass_location, roof, xyac_mean_yardage) %>%\n  na.omit()\n\ntrain_df <- df %>% \n  as_tibble(.name_repair = \"minimal\") %>% \n  setNames(colnames(df)) %>% \n  mutate(label = yards_after_catch)\n\ntest_predictions <- model %>% predict(train_df %>% select(-label))\n\ntest_predictions <- test_predictions %>% as.data.frame()\n\ndf <- \n  cbind(df, test_predictions)\n\ndf %>%\n  ggplot() + \n  geom_density(aes(V1), color = \"blue\") + \n  geom_density(aes(yards_after_catch)) + \n  geom_density(aes(xyac_mean_yardage), color = \"red\") +\n  xlim(c(-10, 20)) + \n  theme_minimal() + \n  labs(\n    title = \"Expected YAC yardage\",\n    x = \"YAC Yardage\",\n    y = \"Density\",\n    subtitle = \"Actual: Black, NNet: Blue, XYAC_Mean: Red\"\n    \n  )\n\n\nThere you have it, your own little nnet done completely in R using the Keras/tensorflow backend.\n\n\n",
    "preview": "posts/2020-08-19-neural-nets-using-r/neural-nets-using-r_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 2400
  },
  {
    "path": "posts/2020-08-19-the-accumulation-of-qb-hits-vs-passing-efficiency/",
    "title": "The accumulation of QB hits vs passing efficiency",
    "description": "Do quarterbacks who get hit see their performance decline throughout the game?",
    "author": [
      {
        "name": "Ben Baldwin",
        "url": "https://twitter.com/benbbaldwin"
      }
    ],
    "date": "2020-08-19",
    "categories": [
      "Figures",
      "nflfastR"
    ],
    "contents": "\nTable of Contents\nGet the data\nCalculate total hits and cumulative hits\nMake sure the data are sound\nSome final cleaning up\nMake the graph\nWrapping up\nIn a follow-up to his excellent piece on the value of the run game in The Athletic (great website, highly recommended), Ted Nguyen shared the following:\n“In-house NFL analytics crews track QB hits and the results of the accumulation of hits and how it affects offensive performance over the course of a game.”\nDoes the accumulation of hits affect offensive performance over the game? Is this finally a feather in the cap for the run game defenders?\nBecause QB hits are tracked by the NFL, we can investigate this ourselves. Let’s dive in.\nGet the data\n\n\nlibrary(tidyverse)\n\npbp <- map_df(2015 : 2019, ~{\n  readRDS(\n    url(\n      glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{.}.rds\")\n    )\n  ) %>%\n    filter(pass == 1, !is.na(epa))\n})\n\nAs a starting point, I’m using the saved dataset of pass plays from nflfastR.\nLet’s make sure the qb_hit variable includes penalty plays, because presumably a quarterback feels the effects of a hit even if the play didn’t count.\n\n\npbp %>% \n  filter(qb_hit==1, play_type == \"no_play\") %>%\n  select(desc, qb_hit)\n\n# A tibble: 0 x 2\n# ... with 2 variables: desc <chr>, qb_hit <dbl>\n\nWomp womp. Let’s see if we can just create hits by searching for the bracket [ since that’s what NFL uses to denote hits.\n\n\npbp %>% \n  filter(play_type != \"no_play\") %>%\n  mutate(\n    hit = if_else(stringr::str_detect(desc, \"\\\\[\") | sack == 1, 1, 0)\n  ) %>%\n  group_by(hit, qb_hit) %>%\n  summarize(n = n())\n\n# A tibble: 4 x 3\n# Groups:   hit [2]\n    hit qb_hit     n\n  <dbl>  <dbl> <int>\n1     0      0 88971\n2     0      1     3\n3     1      0   373\n4     1      1 14515\n\nJust counting sacks and hits works pretty well for the non-penalty plays; there’s high agreement between the official NFL stats (qb_hit) and the variable we created (hit). Let’s see which plays drive the difference:\n\n\npbp %>% \n  filter(play_type != \"no_play\") %>%\n  mutate(\n    hit = if_else(stringr::str_detect(desc, \"\\\\[\") | sack == 1, 1, 0)\n  ) %>%\n  filter(hit == 0 & qb_hit == 1) %>%\n  select(desc)\n\n# A tibble: 3 x 1\n  desc                                                                \n  <chr>                                                               \n1 (6:33) (Shotgun) 5-J.Flacco to DEN 15 for -5 yards. FUMBLES, and re~\n2 (8:37) (Shotgun) 11-C.Wentz Aborted. 62-J.Kelce FUMBLES at NYJ 28, ~\n3 (:44) (Shotgun) 6-D.Hodges FUMBLES (Aborted) at NYJ 49, and recover~\n\nI guess these are data errors or something? I don’t know.\n\n\npbp %>% \n  filter(play_type != \"no_play\") %>%\n  mutate(\n    hit = if_else(stringr::str_detect(desc, \"\\\\[\") | sack == 1, 1, 0)\n  ) %>%\n  filter(hit == 1 & qb_hit == 0) %>%\n  select(desc)\n\n# A tibble: 373 x 1\n   desc                                                               \n   <chr>                                                              \n 1 (7:04) (Shotgun) 5-T.Taylor sacked ob at BUF 23 for -6 yards (50-R~\n 2 (4:41) (Shotgun) 12-T.Brady sacked at BUF 41 for -6 yards (55-J.Hu~\n 3 (14:21) 3-R.Wilson sacked ob at SEA 43 for -1 yards (56-J.Peppers).\n 4 (13:08) (Shotgun) 5-N.Foles sacked at STL 31 for -6 yards. FUMBLES~\n 5 (4:58) (Shotgun) 9-D.Brees sacked at NO 15 for -8 yards (56-J.Smit~\n 6 (1:37) (No Huddle, Shotgun) 8-M.Mariota sacked at CLE 43 for -8 ya~\n 7 (1:17) (No Huddle, Shotgun) 17-R.Tannehill sacked at MIA 39 for -7~\n 8 (14:21) (Shotgun) 12-A.Luck sacked at IND 18 for -5 yards (97-K.Kl~\n 9 (11:33) (Shotgun) 5-B.Bortles sacked at NE 31 for -6 yards (91-J.C~\n10 (14:13) (Shotgun) 2-M.Vick sacked ob at PIT 27 for 0 yards (52-A.O~\n# ... with 363 more rows\n\nSeems like these are sacks out of bounds or fumbles without getting hit. Okay whatever, close enough. Let’s go with the official qb_hit on normal plays and the created version for no_play.\n\n\nhits_data <- pbp %>% \n  mutate(\n    hit = case_when(\n      play_type != \"no_play\" & qb_hit == 1 ~ 1,\n      play_type == \"no_play\" & (stringr::str_detect(desc, \"\\\\[\") | sack == 1) ~ 1,\n      TRUE ~ 0\n    )\n  )\n\nCalculate total hits and cumulative hits\nNow we need to create two variables: (1) qb hits taken up to the current point in the game and (2) total qb hits taken in the game. I’ll also filter out run plays.\n\n\nhits_data <- hits_data %>% \n  group_by(posteam, game_id) %>%\n  mutate(\n    cum_hits=cumsum(qb_hit),\n    total_hits=sum(qb_hit)\n  ) %>%\n  ungroup()\n\nI’m grouping by team (posteam), which isn’t quite perfect. If a team has to switch quarterbacks mid-game, then the count of hits won’t be accurate for the second quarterback. But because these situations are so rare, it shouldn’t matter in the aggregate.\nThe variable cum_hits is created using cumsum, which totals up how many QB hits a team has suffered to that point in the game. And total_hits just sums up the total number of hits over the whole game. I’m kind of amazed at how easy this is to do in R.\nNow let’s see how total_hits affects EPA per dropback at the game level:\n\n\nhits_data %>% \n  group_by(total_hits) %>%\n  summarize(\n    mean_epa = mean(epa),\n    games=n_distinct(game_id, posteam)\n    )\n\n# A tibble: 17 x 3\n   total_hits mean_epa games\n        <dbl>    <dbl> <int>\n 1          0   0.256     35\n 2          1   0.232    119\n 3          2   0.195    238\n 4          3   0.167    307\n 5          4   0.123    374\n 6          5   0.0557   407\n 7          6   0.0339   331\n 8          7   0.0171   285\n 9          8  -0.0162   200\n10          9  -0.0511   146\n11         10  -0.0343    90\n12         11  -0.112     59\n13         12  -0.0274    36\n14         13  -0.0337    24\n15         14   0.0182     7\n16         15  -0.249      7\n17         16  -0.245      5\n\nWow, the most efficient games are most decidedly the ones in which a QB isn’t hit often!\nMake sure the data are sound\nI was surprised that there have been so many games where a QB was never hit (35, the first row above). Initially I thought I did something wrong, but it checks out. Let’s make sure we can replicate the official NFL data. I’m going to look at the later stage of Cleveland’s 2018 season because I know that’s where some of the 0-hit games come from.\n\n\nhits_data %>% \n  filter(posteam == \"CLE\" & season == 2018 & week >= 10) %>%\n  group_by(week) %>%\n  summarize(hits = mean(total_hits), mean_epa = mean(epa))\n\n# A tibble: 7 x 3\n   week  hits mean_epa\n  <int> <dbl>    <dbl>\n1    10     0   0.614 \n2    12     1   0.837 \n3    13     1  -0.145 \n4    14     1  -0.112 \n5    15     3  -0.0154\n6    16     0   0.458 \n7    17     2   0.211 \n\nNow compare to the official stats (with thanks to SportRadar):\nHitsBoom! A perfect match!\nSome final cleaning up\nReturning to the relationship between hits and EPA per dropback, case closed, right? Games with fewer hits have higher EPA per dropback. Well, not so fast. This is picking up, in part, a game script effect, where overmatched teams fall behind early and are forced to pass a lot, resulting in their QB being hit more often.\nSo we want to create a level playing field. To do this, let’s take teams with a given number of hits and see how the number of accumulated hits affects passing efficiency, holding the total number of hits received in the game constant. There are a number of other ways we could have approached this – looking at plays within some range of win probability or score differential, for example – but I think this is a nice illustration.\n\n\nhits_data <- hits_data %>%\n  mutate(\n    hit_in_game=\n      case_when(total_hits==0 | total_hits==1~\"0-1\",\n                 total_hits==2 | total_hits==3~\"2-3\", \n                 total_hits==4 | total_hits==5~\"4-5\", \n                 total_hits==6 | total_hits==7~\"6-7\", \n                 total_hits==8|total_hits==9~\"8-9\", \n                 total_hits>9~\"10+\") %>% \n                  factor(levels = c(\"0\", \"2-3\", \"4-5\", \"6-7\", \"8-9\", \"10+\"))\n    )\n\nAbove, we’ve created some BINS based on how often a quarterback is hit in a game (the factor(levels… part at the end isn’t strictly necessary, but allows the legend to display in the right order later on).\nNow we can group by our bins, along with how many hits a QB has taken up to that point in a given game.\n\n\nchart <- hits_data %>% \n  group_by(hit_in_game,cum_hits) %>%\n  summarize(avg_epa = mean(epa), plays = n())\n\nMake the graph\nNow all that’s left to do is plot the data (with a huge thanks to R genius Josh Hornsby for helping make this look better)\n\n\nchart %>% \n  filter(cum_hits > 0 & cum_hits <=12 & !is.na(hit_in_game)) %>%\n  ggplot(aes(x = cum_hits, y = avg_epa, color = hit_in_game, shape = hit_in_game)) +\n    geom_jitter(aes(x = cum_hits, y = avg_epa, fill = hit_in_game), shape = 21, stroke = 1.25, size = 4, width = 0.1, show.legend=FALSE)+\n   geom_smooth(method = lm, se = FALSE) +\n   theme_minimal() +\n   theme(\n    legend.position = c(0.99, 0.99), \n    legend.justification = c(1, 1) ,\n    plot.title = element_text(size = 16, hjust = 0.5),\n    panel.grid.minor = element_blank())+ \n  ggsci::scale_color_locuszoom(name = \"Total Hits\\nIn-Game\") +\n  scale_y_continuous(name = \"EPA Per Dropback\", breaks = scales::pretty_breaks(n = 5))+\n  scale_x_continuous(breaks = 0:50, name = \"Cumulative QB Hits Suffered In Game So Far\")+\n  labs(title=\"QB hits versus QB efficiency\", caption = \"Data from nflfastR\")\n\n\nWell then. The negative relationship between QB hits and efficiency is because the group of teams that get hit often are the only ones to make it to the high numbers of hits. Stated this way, it sounds obvious, but it’s important. These teams aren’t necessarily inefficient because their QBs are getting hit a lot; but rather, their QBs are getting hit a lot because they’re bad teams to begin with.\nSide note: I’m not showing 0 hits because there’s a mechanical relationship between QB hits and efficiency. It is the one x-axis point that contains 0 hits, by definition, so of course EPA per play is higher: it’s a comparison of a set of plays with no QB hits to other sets of plays with QB hits. I also truncated the x-axis at 12 hits because anything higher is extremely rare.\nWrapping up\nLetting your QB get hit is bad. Obviously. Teams that allow more hits are less likely to have efficient offenses. But for a given level of hits, there is no evidence that the accumulation of hits makes any difference throughout the course of a game. The evidence suggests that we’ve found a variation of Brian Burke’s “passing paradox”:\nBurkeAs with the Rule of 53, the NFL has appeared to draw the wrong conclusions from a correlation driven by game state.\n\n\n",
    "preview": "posts/2020-08-19-the-accumulation-of-qb-hits-vs-passing-efficiency/the-accumulation-of-qb-hits-vs-passing-efficiency_files/figure-html5/unnamed-chunk-12-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3300
  },
  {
    "path": "posts/2020-08-19-wins-above-expectation/",
    "title": "Wins Above Expectation",
    "description": "This article looks at the percentage of snaps with win probability over an \narbitralily chosen critical value and compares it with the true win percentage.",
    "author": [
      {
        "name": "Sebastian Carl",
        "url": "https://twitter.com/mrcaseb"
      }
    ],
    "date": "2020-08-19",
    "categories": [
      "Figures",
      "nflfastR"
    ],
    "contents": "\n\nContents\nPreface\nLoad nflfastR Play by Play and compute some helper columns\nCreate the plots\n\nPreface\nIn the NFL, practically everyone can beat anyone. So it often happens that games are tight until the very end and the winner is likely to have had some luck. Every year there are teams where you subjectively feel that they have lost or won particularly many of the aforementioned games.\nIn this post I will show off a very simple way to illustrate that by looking at how many snaps a team played with the nflfastR win probability (model with Vegas line) above a critical value (50%) more or less arbitrarily chosen by me and compare this value with the true win percentage.\nLoad nflfastR Play by Play and compute some helper columns\nSince we want to compute true win percentage from nflfastR play-by-play data we have to do a little data wrangling before we can create the plot.\n\n\nlibrary(tidyverse)\n\n# Parameter --------------------------------------------------------------------\n\nseason <- 2019\nwp_limit <- 0.5\n\n# Load the data ----------------------------------------------------------------\n\npbp <- readRDS(url(\n  glue::glue(\"https://raw.githubusercontent.com/guga31bb/nflfastR-data/master/data/play_by_play_{season}.rds\")\n)) %>%\n  filter(pass == 1 | rush == 1)\n\n# Compute outcomes and win percentage ------------------------------------------\n\noutcomes <- pbp %>%\n  group_by(season, game_id, home_team) %>%\n  summarise(\n    home_win = if_else(result > 0, 1, 0),\n    home_tie = if_else(result == 0, 1, 0)\n  ) %>%\n  group_by(season, home_team) %>%\n  summarise(\n    home_games = n(),\n    home_wins = sum(home_win),\n    home_ties = sum(home_tie)\n  ) %>%\n  ungroup() %>%\n  left_join(\n    # away games\n    pbp %>%\n      group_by(season, game_id, away_team) %>%\n      summarise(\n        away_win = if_else(result < 0, 1, 0),\n        away_tie = if_else(result == 0, 1, 0)\n      ) %>%\n      group_by(season, away_team) %>%\n      summarise(\n        away_games = n(),\n        away_wins = sum(away_win),\n        away_ties = sum(away_tie)\n      ) %>%\n      ungroup(),\n    by = c(\"season\", \"home_team\" = \"away_team\")\n  ) %>%\n  rename(team = \"home_team\") %>%\n  mutate(\n    games = home_games + away_games,\n    wins = home_wins + away_wins,\n    losses = games - wins,\n    ties = home_ties + away_ties,\n    win_percentage = (wins + 0.5 * ties) / games\n  ) %>%\n  select(\n    season, team, games, wins, losses, ties, win_percentage\n  )\n\n# Compute percentage of plays with wp > wp_lim ---------------------------------\n\nwp_combined <- pbp %>%\n  filter(!is.na(vegas_wp) & !is.na(posteam)) %>%\n  group_by(season, posteam) %>%\n  summarise(\n    pos_plays = n(),\n    pos_wp_lim_plays = sum(vegas_wp > wp_limit)\n  ) %>%\n  ungroup() %>%\n  left_join(\n    pbp %>%\n      filter(!is.na(vegas_wp) & !is.na(posteam)) %>%\n      group_by(season, defteam) %>%\n      summarise(\n        def_plays = n(),\n        def_wp_lim_plays = sum(vegas_wp < wp_limit)\n      ) %>%\n      ungroup(),\n    by = c(\"season\", \"posteam\" = \"defteam\")\n  ) %>%\n  rename(team = \"posteam\") %>%\n  mutate(\n    wp_lim_percentage = as.numeric(pos_wp_lim_plays + def_wp_lim_plays) / as.numeric(pos_plays + def_plays)\n  ) %>%\n  select(season, team, wp_lim_percentage)\n\n# Combine data and add colors and logos ----------------------------------------\n\nchart <- outcomes %>%\n  left_join(wp_combined, by = c(\"season\", \"team\")) %>%\n  filter(!is.na(wp_lim_percentage)) %>%\n  mutate(diff = 100 * (win_percentage - wp_lim_percentage)) %>%\n  group_by(team) %>%\n  summarise_all(mean) %>%\n  ungroup() %>%\n  inner_join(\n    nflfastR::teams_colors_logos %>% select(team_abbr, team_color, team_logo_espn, team_logo_wikipedia),\n    by = c(\"team\" = \"team_abbr\")\n  ) %>%\n  mutate(\n    grob = map(seq_along(team_logo_espn), function(x) {\n      grid::rasterGrob(magick::image_read(team_logo_espn[[x]]))\n    })\n  ) %>%\n  select(team, win_percentage, wp_lim_percentage, diff, team_color, grob) %>%\n  arrange(desc(diff))\n\n\n\nCreate the plots\nWe will create two separate plots. A scatterplot comparing true win percentage with the percentage of plays with win probability > 50% and a barplot showing the difference between the above variables.\n\n\n# Create scatterplot -----------------------------------------------------------\nchart %>%\n  ggplot(aes(x = wp_lim_percentage, y = win_percentage)) +\n  geom_abline(intercept = 0, slope = 1) +\n  geom_hline(aes(yintercept = mean(win_percentage)), color = \"red\", linetype = \"dashed\") +\n  geom_vline(aes(xintercept = mean(wp_lim_percentage)), color = \"red\", linetype = \"dashed\") +\n  ggpmisc::geom_grob(aes(x = wp_lim_percentage, y = win_percentage, label = grob), vp.width = 0.05) +\n  labs(\n    x = glue::glue(\"Percentage of snaps with win probability (vegas_wp) over {100 * wp_limit}%\"),\n    y = \"True win percentage (including ties as half a win)\",\n    title = \"NFL Team Efficiency\",\n    caption = \"Figure: @mrcaseb | Data: @nflfastR\"\n  ) +\n  ggthemes::theme_stata(scheme = \"sj\", base_size = 8) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.caption = element_text(hjust = 1),\n    axis.text.y = element_text(angle = 0, vjust = 0.5),\n    legend.title = element_text(size = 8, hjust = 0, vjust = 0.5, face = \"bold\"),\n    legend.position = \"top\",\n    aspect.ratio = 1 / 1.618\n  ) +\n  NULL\n\n\n\n\n\n\n# Create bar plot  -------------------------------------------------------------\nchart %>%\n  ggplot(aes(x = seq_along(diff), y = diff)) +\n  geom_hline(aes(yintercept = mean(diff)), color = \"red\", linetype = \"dashed\") +\n  geom_col(width = 0.5, colour = chart$team_color, fill = chart$team_color, alpha = 0.5) +\n  ggpmisc::geom_grob(aes(x = seq_along(diff), y = diff, label = grob), vp.width = 0.035) +\n  # scale_x_continuous(expand = c(0,0)) +\n  labs(\n    x = \"Rank\",\n    y = \"Win Percentage Over Expectation\",\n    title = \"NFL Team Efficiency\",\n    subtitle = \"How Lucky are the Teams?\",\n    caption = \"Figure: @mrcaseb | Data: @nflfastR\"\n  ) +\n  ggthemes::theme_stata(scheme = \"sj\", base_size = 8) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.caption = element_text(hjust = 1),\n    axis.text.y = element_text(angle = 0, vjust = 0.5),\n    legend.title = element_text(size = 8, hjust = 0, vjust = 0.5, face = \"bold\"),\n    legend.position = \"top\",\n    aspect.ratio = 1 / 1.618\n  ) +\n  NULL\n\n\n\n\n\nView source code on GitHub \n\n\n\n\n",
    "preview": "posts/2020-08-19-wins-above-expectation/wins-above-expectation_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3000
  },
  {
    "path": "posts/2020-08-18-pfrs-bad-throw-percentage-for-quarterbacks/",
    "title": "PFR's Bad Throw Percentage for Quarterbacks",
    "description": "This article shows how to scrape football data from Pro Football Reference and\nhow to plot the bad throw percentage data for quarterbacks.",
    "author": [
      {
        "name": "Sebastian Carl",
        "url": "https://twitter.com/mrcaseb"
      }
    ],
    "date": "2020-08-18",
    "categories": [
      "Scraping",
      "PFR",
      "Figures",
      "nflfastR"
    ],
    "contents": "\n\nContents\nPreface\nGet the data and save it locally\nCreate the plot\n\nPreface\nOne of the most important sources for football related data is Pro Football Reference (short PFR). There are a ton of useful stats and I would like to look into their “Bad Throw Percentage” defined as\n\nPercentage of poor throws per pass attempt, excluding spikes and throwaways\n\nPFR provides it’s data on static html websites which makes it easy to scrape them. This will be demonstrated below.\nGet the data and save it locally\nScraping data means traffic for the provider. We don’t want to annoy them so we scrape them once and save them locally. This can be done with the following code (please note it saves only the for this example relevant variables and you might want to change that):\n\n\nlibrary(tidyverse)\nlibrary(rvest)\n\n# scrape data from PFR----------------------------------------------------------\nurl <- \"https://www.pro-football-reference.com/years/2019/passing_advanced.htm\"\npfr_raw <- url %>%\n  read_html() %>%\n  html_table() %>%\n  as.data.frame()\n\n# clean the scraped data--------------------------------------------------------\n\n# rename the columns as the actual column names are saved in the first row now\ncolnames(pfr_raw) <- make.names(pfr_raw[1, ], unique = TRUE, allow_ = TRUE)\n\n# drop the first row and select the columns we are interested in\npfr <- pfr_raw %>%\n  slice(-1) %>%\n  select(Player, Tm, IAY.PA, Bad., Att) %>%\n  rename(team = Tm) %>%\n  mutate(\n    # pfr uses different team abbreviations than nflfastR, fix them\n    team = case_when(\n      team == \"GNB\" ~ \"GB\",\n      team == \"KAN\" ~ \"KC\",\n      team == \"NOR\" ~ \"NO\",\n      team == \"NWE\" ~ \"NE\",\n      team == \"SFO\" ~ \"SF\",\n      team == \"TAM\" ~ \"TB\",\n      TRUE ~ team\n    ),\n    # repair player names\n    Player = str_replace(Player, \"\\\\*\", \"\"),\n    Player = str_replace(Player, \"\\\\+\", \"\"),\n\n    # make interesting columns numeric\n    IAY.PA = as.numeric(IAY.PA),\n    Bad. = as.numeric(str_replace(Bad., \"%\", \"\")),\n    Passattempts = as.numeric(Att)\n  ) %>%\n  # join colors and logos from nflfastR\n  left_join(nflfastR::teams_colors_logos, by = c(\"team\" = \"team_abbr\"))\n\n# save to disk------------------------------------------------------------------\n# binary\nsaveRDS(pfr, file = \"pfr_bad_throws.rds\")\n\n# ASCII\nwrite_csv(pfr, \"pfr_bad_throws.csv\")\n\n\n\nCreate the plot\nThe data we want to have a look at is now saved locally and can be used to create the plot:\n\n\nlibrary(tidyverse)\nchart_data <- readRDS(\"pfr_bad_throws.rds\") %>% filter(Passattempts > 180)\n\nchart_data %>%\n  ggplot(aes(x = IAY.PA, y = Bad. / 100)) +\n  geom_hline(aes(yintercept = mean(Bad. / 100)), color = \"red\", linetype = \"dotted\") +\n  geom_vline(aes(xintercept = mean(IAY.PA)), color = \"red\", linetype = \"dotted\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\", size = 0.3) +\n  geom_point(color = chart_data$team_color, aes(cex = Passattempts), alpha = 1 / 4) +\n  ggrepel::geom_text_repel(aes(label = Player), force = 1, point.padding = 0, segment.size = 0.1, size = 3) +\n  scale_y_continuous(labels = scales::percent) +\n  scale_size_area(max_size = 6) +\n  labs(\n    x = \"Average Depth of Target in Yards\",\n    y = \"Bad Throw Percentage\",\n    caption = \"Bad Throw Percentage = Percentage of throws that weren't catchable with normal effort, excluding spikes and throwaways\\nFigure: @mrcaseb | Data: @pfref\",\n    title = \"QB Passing Performance 2019\",\n    subtitle = \"We may see regression hitting Tannehill and Prescott in 2020\"\n  ) +\n  ggthemes::theme_stata(scheme = \"sj\", base_size = 8) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    plot.caption = element_text(hjust = 1),\n    axis.text.y = element_text(angle = 0, vjust = 0.5),\n    legend.title = element_text(size = 8, hjust = 0, vjust = 0.5, face = \"bold\"),\n    legend.position = \"top\",\n    aspect.ratio = 1 / 1.618\n  ) +\n  NULL\n\n\n\n\n\nView source code on GitHub \n\n\n\n\n",
    "preview": "posts/2020-08-18-pfrs-bad-throw-percentage-for-quarterbacks/pfrs-bad-throw-percentage-for-quarterbacks_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2023-04-17T16:10:43+00:00",
    "input_file": {},
    "preview_width": 3900,
    "preview_height": 3300
  }
]
